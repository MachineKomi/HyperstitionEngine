{
  "id": "AI",
  "sentences": [
    "Providedproperattributionisprovided,Googleherebygrantspermissionto reproducethetablesandfiguresinthispapersolelyforuseinjournalisticor scholarlyworks.",
    "Attention Is All You Need AshishVaswani∗ NoamShazeer∗ NikiParmar∗ JakobUszkoreit∗ GoogleBrain GoogleBrain GoogleResearch GoogleResearch avaswani@google.com noam@google.com",
    "nikip@google.com",
    "usz@google.com LlionJones∗ AidanN.Gomez∗ † ŁukaszKaiser∗ GoogleResearch UniversityofToronto GoogleBrain llion@google.com",
    "aidan@cs.toronto.edu lukaszkaiser@google.com IlliaPolosukhin∗ ‡ illia.polosukhin@gmail.com",
    "Abstract Thedominantsequencetransductionmodelsarebasedoncomplexrecurrentor convolutionalneuralnetworksthatincludeanencoderandadecoder.",
    "Thebest performing models also connect the encoder and decoder through an attention mechanism.",
    "We propose a new simple network architecture, the Transformer, basedsolelyonattentionmechanisms,dispensingwithrecurrenceandconvolutions entirely.",
    "Experiments on two machine translation tasks show these models to besuperiorinqualitywhilebeingmoreparallelizableandrequiringsignificantly less time to train.",
    "Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles,byover2BLEU.OntheWMT2014English-to-Frenchtranslationtask, ourmodelestablishesanewsingle-modelstate-of-the-artBLEUscoreof41.8after trainingfor3.5daysoneightGPUs,asmallfractionofthetrainingcostsofthe bestmodelsfromtheliterature.",
    "WeshowthattheTransformergeneralizeswellto othertasksbyapplyingitsuccessfullytoEnglishconstituencyparsingbothwith largeandlimitedtrainingdata.",
    "∗Equalcontribution.",
    "Listingorderisrandom.JakobproposedreplacingRNNswithself-attentionandstarted theefforttoevaluatethisidea.",
    "Ashish,withIllia,designedandimplementedthefirstTransformermodelsand hasbeencruciallyinvolvedineveryaspectofthiswork.",
    "Noamproposedscaleddot-productattention,multi-head attentionandtheparameter-freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery detail.",
    "Nikidesigned,implemented,tunedandevaluatedcountlessmodelvariantsinouroriginalcodebaseand tensor2tensor.",
    "Llionalsoexperimentedwithnovelmodelvariants,wasresponsibleforourinitialcodebase,and efficientinferenceandvisualizations.",
    "LukaszandAidanspentcountlesslongdaysdesigningvariouspartsofand implementingtensor2tensor,replacingourearliercodebase,greatlyimprovingresultsandmassivelyaccelerating ourresearch.",
    "†WorkperformedwhileatGoogleBrain.",
    "‡WorkperformedwhileatGoogleResearch.",
    "31stConferenceonNeuralInformationProcessingSystems(NIPS2017),LongBeach,CA,USA. 3202 guA 2 ]LC.sc[ 7v26730.6071:viXra 1",
    "Introduction Recurrentneuralnetworks,longshort-termmemory[13]andgatedrecurrent[7]neuralnetworks inparticular,havebeenfirmlyestablishedasstateoftheartapproachesinsequencemodelingand transductionproblemssuchaslanguagemodelingandmachinetranslation[35,2,5].",
    "Numerous effortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder-decoder architectures[38,24,15].",
    "Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput sequences.",
    "Aligningthepositionstostepsincomputationtime,theygenerateasequenceofhidden statesh ,asafunctionoftheprevioushiddenstateh andtheinputforpositiont.",
    "Thisinherently t t−1 sequentialnatureprecludesparallelizationwithintrainingexamples,whichbecomescriticalatlonger sequencelengths,asmemoryconstraintslimitbatchingacrossexamples.",
    "Recentworkhasachieved significantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional computation[32],whilealsoimprovingmodelperformanceincaseofthelatter.",
    "Thefundamental constraintofsequentialcomputation,however,remains.",
    "Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc- tionmodelsinvarioustasks,allowingmodelingofdependencieswithoutregardtotheirdistancein theinputoroutputsequences[2,19].",
    "Inallbutafewcases[27],however,suchattentionmechanisms areusedinconjunctionwitharecurrentnetwork.",
    "InthisworkweproposetheTransformer,amodelarchitectureeschewingrecurrenceandinstead relyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput.",
    "TheTransformerallowsforsignificantlymoreparallelizationandcanreachanewstateoftheartin translationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs.",
    "2 Background ThegoalofreducingsequentialcomputationalsoformsthefoundationoftheExtendedNeuralGPU [16],ByteNet[18]andConvS2S[9],allofwhichuseconvolutionalneuralnetworksasbasicbuilding block,computinghiddenrepresentationsinparallelforallinputandoutputpositions.",
    "Inthesemodels, thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows inthedistancebetweenpositions,linearlyforConvS2SandlogarithmicallyforByteNet.",
    "Thismakes it more difficult to learn dependencies between distant positions [12].",
    "In the Transformer this is reducedtoaconstantnumberofoperations, albeitatthecostofreducedeffectiveresolutiondue to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as describedinsection3.2.",
    "Self-attention,sometimescalledintra-attentionisanattentionmechanismrelatingdifferentpositions ofasinglesequenceinordertocomputearepresentationofthesequence.",
    "Self-attentionhasbeen usedsuccessfullyinavarietyoftasksincludingreadingcomprehension,abstractivesummarization, textualentailmentandlearningtask-independentsentencerepresentations[4,27,28,22].",
    "End-to-endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence- alignedrecurrenceandhavebeenshowntoperformwellonsimple-languagequestionansweringand languagemodelingtasks[34].",
    "To the best of our knowledge, however, the Transformer is the first transduction model relying entirelyonself-attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence- alignedRNNsorconvolution.",
    "Inthefollowingsections,wewilldescribetheTransformer,motivate self-attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9].",
    "3 ModelArchitecture Mostcompetitiveneuralsequencetransductionmodelshaveanencoder-decoderstructure[5,2,35].",
    "Here, the encoder maps an input sequence of symbol representations (x ,...,x ) to a sequence 1 n of continuous representations z = (z ,...,z ).",
    "Given z, the decoder then generates an output 1 n sequence(y ,...,y )ofsymbolsoneelementatatime.",
    "Ateachstepthemodelisauto-regressive 1 m [10],consumingthepreviouslygeneratedsymbolsasadditionalinputwhengeneratingthenext. 2 Figure1: TheTransformer-modelarchitecture.",
    "TheTransformerfollowsthisoverallarchitectureusingstackedself-attentionandpoint-wise,fully connectedlayersforboththeencoderanddecoder,shownintheleftandrighthalvesofFigure1, respectively.",
    "3.1 EncoderandDecoderStacks Encoder: The encoder is composed of a stack of N = 6 identical layers.",
    "Each layer has two sub-layers.",
    "Thefirstisamulti-headself-attentionmechanism,andthesecondisasimple,position- wisefullyconnectedfeed-forwardnetwork.",
    "Weemployaresidualconnection[11]aroundeachof the two sub-layers, followed by layer normalization",
    "[1].",
    "That is, the output of each sub-layer is LayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub-layer itself.",
    "Tofacilitatetheseresidualconnections,allsub-layersinthemodel,aswellastheembedding layers,produceoutputsofdimensiond =512.",
    "model Decoder: ThedecoderisalsocomposedofastackofN =6identicallayers.",
    "Inadditiontothetwo sub-layersineachencoderlayer,thedecoderinsertsathirdsub-layer,whichperformsmulti-head attentionovertheoutputoftheencoderstack.",
    "Similartotheencoder,weemployresidualconnections aroundeachofthesub-layers,followedbylayernormalization.",
    "Wealsomodifytheself-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.",
    "This masking,combinedwithfactthattheoutputembeddingsareoffsetbyoneposition,ensuresthatthe predictionsforpositionicandependonlyontheknownoutputsatpositionslessthani.",
    "3.2 Attention Anattentionfunctioncanbedescribedasmappingaqueryandasetofkey-valuepairstoanoutput, wherethequery,keys,values,andoutputareallvectors.",
    "Theoutputiscomputedasaweightedsum 3 ScaledDot-ProductAttention Multi-HeadAttention Figure 2: (left) Scaled Dot-Product Attention.",
    "(right) Multi-Head Attention consists of several attentionlayersrunninginparallel.",
    "ofthevalues,wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe querywiththecorrespondingkey.",
    "3.2.1 ScaledDot-ProductAttention Wecallourparticularattention\"ScaledDot-ProductAttention\"(Figure2).",
    "Theinputconsistsof queriesandkeysofdimensiond k ,a√ndvaluesofdimensiond v .",
    "Wecomputethedotproductsofthe querywithallkeys,divideeachby d ,andapplyasoftmaxfunctiontoobtaintheweightsonthe k values.",
    "Inpractice,wecomputetheattentionfunctiononasetofqueriessimultaneously,packedtogether intoamatrixQ. ThekeysandvaluesarealsopackedtogetherintomatricesK andV. Wecompute thematrixofoutputsas:",
    "QKT Attention(Q,K,V)=softmax( √ )V (1) d",
    "k Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot-product(multi- plicative)attention.",
    "Dot-productattentionisidenticaltoouralgorithm,exceptforthescalingfactor of √1 .",
    "Additiveattentioncomputesthecompatibilityfunctionusingafeed-forwardnetworkwith dk asinglehiddenlayer.",
    "Whilethetwoaresimilarintheoreticalcomplexity,dot-productattentionis muchfasterandmorespace-efficientinpractice,sinceitcanbeimplementedusinghighlyoptimized matrixmultiplicationcode.",
    "Whileforsmallvaluesofd thetwomechanismsperformsimilarly,additiveattentionoutperforms k dotproductattentionwithoutscalingforlargervaluesofd",
    "[3].",
    "Wesuspectthatforlargevaluesof k d ,thedotproductsgrowlargeinmagnitude,pushingthesoftmaxfunctionintoregionswhereithas k extremelysmallgradients4.",
    "Tocounteractthiseffect,wescalethedotproductsby √1 .",
    "dk 3.2.2 Multi-HeadAttention Insteadofperformingasingleattentionfunctionwithd -dimensionalkeys,valuesandqueries, model wefounditbeneficialtolinearlyprojectthequeries,keysandvalueshtimeswithdifferent,learned linearprojectionstod ,d andd dimensions,respectively.",
    "Oneachoftheseprojectedversionsof k k v queries,keysandvalueswethenperformtheattentionfunctioninparallel,yieldingd -dimensional v 4Toillustratewhythedotproductsgetlarge,assumethatthecomponentsofqandkareindependentrandom variableswithmean0andvariance1.Thentheirdotproduct,q·k= (cid:80)dk q k ,hasmean0andvarianced .",
    "i=1",
    "i",
    "i k 4 output values.",
    "These are concatenated and once again projected, resulting in the final values, as depictedinFigure2.",
    "Multi-headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation subspacesatdifferentpositions.",
    "Withasingleattentionhead,averaginginhibitsthis.",
    "MultiHead(Q,K,V)=Concat(head ,...,head )WO 1 h wherehead =Attention(QWQ,KWK,VWV)",
    "i",
    "i i i WheretheprojectionsareparametermatricesWQ ∈Rdmodel×dk,WK ∈Rdmodel×dk,WV ∈Rdmodel×dv",
    "i i i andWO ∈Rhdv×dmodel.",
    "In this work we employ h = 8 parallel attention layers, or heads.",
    "For each of these we use d",
    "=d =d /h=64.",
    "Duetothereduceddimensionofeachhead,thetotalcomputationalcost k v model issimilartothatofsingle-headattentionwithfulldimensionality.",
    "3.2.3 ApplicationsofAttentioninourModel TheTransformerusesmulti-headattentioninthreedifferentways: • In\"encoder-decoderattention\"layers,thequeriescomefromthepreviousdecoderlayer, andthememorykeysandvaluescomefromtheoutputoftheencoder.",
    "Thisallowsevery positioninthedecodertoattendoverallpositionsintheinputsequence.",
    "Thismimicsthe typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38,2,9]. • Theencodercontainsself-attentionlayers.",
    "Inaself-attentionlayerallofthekeys,values andqueriescomefromthesameplace,inthiscase,theoutputofthepreviouslayerinthe encoder.",
    "Eachpositionintheencodercanattendtoallpositionsinthepreviouslayerofthe encoder.",
    "•",
    "Similarly,self-attentionlayersinthedecoderalloweachpositioninthedecodertoattendto allpositionsinthedecoderuptoandincludingthatposition.",
    "Weneedtopreventleftward informationflowinthedecodertopreservetheauto-regressiveproperty.",
    "Weimplementthis insideofscaleddot-productattentionbymaskingout(settingto−∞)allvaluesintheinput ofthesoftmaxwhichcorrespondtoillegalconnections.",
    "SeeFigure2.",
    "3.3 Position-wiseFeed-ForwardNetworks Inadditiontoattentionsub-layers,eachofthelayersinourencoderanddecodercontainsafully connectedfeed-forwardnetwork,whichisappliedtoeachpositionseparatelyandidentically.",
    "This consistsoftwolineartransformationswithaReLUactivationinbetween.",
    "FFN(x)=max(0,xW +b )W +b (2) 1 1 2 2 Whilethelineartransformationsarethesameacrossdifferentpositions,theyusedifferentparameters from layer to layer.",
    "Another way of describing this is as two convolutions with kernel size 1.",
    "The dimensionality of input and output is d = 512, and the inner-layer has dimensionality model d",
    "=2048.",
    "ff 3.4 EmbeddingsandSoftmax Similarlytoothersequencetransductionmodels,weuselearnedembeddingstoconverttheinput tokensandoutputtokenstovectorsofdimensiond .",
    "Wealsousetheusuallearnedlineartransfor-",
    "model mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext-tokenprobabilities.",
    "In ourmodel,wesharethesameweightmatrixbetweenthetwoembeddinglayersandthepre-√softmax lineartransformation,similarto[30].",
    "Intheembeddinglayers,wemultiplythoseweightsby d .",
    "model 5 Table1:",
    "Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations fordifferentlayertypes.",
    "nisthesequencelength,distherepresentationdimension,kisthekernel sizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself-attention.",
    "LayerType",
    "ComplexityperLayer Sequential MaximumPathLength Operations Self-Attention O(n2·d) O(1) O(1)",
    "Recurrent O(n·d2) O(n) O(n)",
    "Convolutional O(k·n·d2) O(1) O(log (n))",
    "k Self-Attention(restricted) O(r·n·d) O(1) O(n/r) 3.5 PositionalEncoding Sinceourmodelcontainsnorecurrenceandnoconvolution,inorderforthemodeltomakeuseofthe orderofthesequence,wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe tokensinthesequence.",
    "Tothisend,weadd\"positionalencodings\"totheinputembeddingsatthe bottomsoftheencoderanddecoderstacks.",
    "Thepositionalencodingshavethesamedimensiond model astheembeddings,sothatthetwocanbesummed.",
    "Therearemanychoicesofpositionalencodings, learnedandfixed[9].",
    "Inthiswork,weusesineandcosinefunctionsofdifferentfrequencies: PE =sin(pos/100002i/dmodel) (pos,2i) PE =cos(pos/100002i/dmodel) (pos,2i+1) whereposisthepositionandiisthedimension.",
    "Thatis,eachdimensionofthepositionalencoding correspondstoasinusoid.",
    "Thewavelengthsformageometricprogressionfrom2πto10000·2π.",
    "We chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby relativepositions,sinceforanyfixedoffsetk,PE canberepresentedasalinearfunctionof pos+k PE .",
    "pos Wealsoexperimentedwithusinglearnedpositionalembeddings[9]instead,andfoundthatthetwo versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion becauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered duringtraining.",
    "4 WhySelf-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tionallayerscommonlyusedformappingonevariable-lengthsequenceofsymbolrepresentations (x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z ∈ Rd, such as a hidden 1 n 1 n",
    "i i layerinatypicalsequencetransductionencoderordecoder.",
    "Motivatingouruseofself-attentionwe considerthreedesiderata.",
    "Oneisthetotalcomputationalcomplexityperlayer.",
    "Anotheristheamountofcomputationthatcan beparallelized,asmeasuredbytheminimumnumberofsequentialoperationsrequired.",
    "Thethirdisthepathlengthbetweenlong-rangedependenciesinthenetwork.",
    "Learninglong-range dependenciesisakeychallengeinmanysequencetransductiontasks.",
    "Onekeyfactoraffectingthe abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto traverseinthenetwork.",
    "Theshorterthesepathsbetweenanycombinationofpositionsintheinput andoutputsequences,theeasieritistolearnlong-rangedependencies[12].",
    "Hencewealsocompare themaximumpathlengthbetweenanytwoinputandoutputpositionsinnetworkscomposedofthe differentlayertypes.",
    "AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.",
    "In terms of computationalcomplexity,self-attentionlayersarefasterthanrecurrentlayerswhenthesequence 6 length n is smaller than the representation dimensionality d, which is most often the case with sentencerepresentationsusedbystate-of-the-artmodelsinmachinetranslations,suchasword-piece [38]andbyte-pair[31]representations.",
    "Toimprovecomputationalperformancefortasksinvolving verylongsequences,self-attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin theinputsequencecenteredaroundtherespectiveoutputposition.",
    "Thiswouldincreasethemaximum pathlengthtoO(n/r).",
    "Weplantoinvestigatethisapproachfurtherinfuturework.",
    "Asingleconvolutionallayerwithkernelwidthk <ndoesnotconnectallpairsofinputandoutput positions.",
    "DoingsorequiresastackofO(n/k)convolutionallayersinthecaseofcontiguouskernels, orO(log (n))inthecaseofdilatedconvolutions[18], increasingthelengthofthelongestpaths k betweenanytwopositionsinthenetwork.",
    "Convolutionallayersaregenerallymoreexpensivethan recurrent layers, by a factor of k. Separable convolutions",
    "[6], however, decrease the complexity considerably, toO(k·n·d+n·d2).",
    "Evenwithk = n, however, thecomplexityofaseparable convolutionisequaltothecombinationofaself-attentionlayerandapoint-wisefeed-forwardlayer, theapproachwetakeinourmodel.",
    "Assidebenefit,self-attentioncouldyieldmoreinterpretablemodels.",
    "Weinspectattentiondistributions fromourmodelsandpresentanddiscussexamplesintheappendix.",
    "Notonlydoindividualattention headsclearlylearntoperformdifferenttasks,manyappeartoexhibitbehaviorrelatedtothesyntactic andsemanticstructureofthesentences.",
    "5 Training Thissectiondescribesthetrainingregimeforourmodels.",
    "5.1 TrainingDataandBatching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentencepairs.",
    "Sentenceswereencodedusingbyte-pairencoding[3],whichhasasharedsource- targetvocabularyofabout37000tokens.",
    "ForEnglish-French,weusedthesignificantlylargerWMT 2014English-Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word-piece vocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength.",
    "Eachtraining batchcontainedasetofsentencepairscontainingapproximately25000sourcetokensand25000 targettokens.",
    "5.2 HardwareandSchedule Wetrainedourmodelsononemachinewith8NVIDIAP100GPUs.",
    "Forourbasemodelsusing thehyperparametersdescribedthroughoutthepaper,eachtrainingsteptookabout0.4seconds.",
    "We trainedthebasemodelsforatotalof100,000stepsor12hours.",
    "Forourbigmodels,(describedonthe bottomlineoftable3),steptimewas1.0seconds.",
    "Thebigmodelsweretrainedfor300,000steps (3.5days).",
    "5.3 Optimizer WeusedtheAdamoptimizer[20]withβ =0.9,β =0.98andϵ=10−9.",
    "Wevariedthelearning 1 2 rateoverthecourseoftraining,accordingtotheformula: lrate=d−0.5 ·min(step_num−0.5,step_num·warmup_steps−1.5) (3) model Thiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps, anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber.",
    "Weused warmup_steps=4000.",
    "5.4 Regularization Weemploythreetypesofregularizationduringtraining: 7 Table2: TheTransformerachievesbetterBLEUscoresthanpreviousstate-of-the-artmodelsonthe English-to-GermanandEnglish-to-Frenchnewstest2014testsatafractionofthetrainingcost.",
    "BLEU TrainingCost(FLOPs)",
    "Model EN-DE EN-FR EN-DE EN-FR ByteNet[18] 23.75 Deep-Att+PosUnk[39] 39.2 1.0·1020 GNMT+RL[38] 24.6 39.92 2.3·1019 1.4·1020 ConvS2S[9] 25.16 40.46 9.6·1018 1.5·1020 MoE[32] 26.03 40.56 2.0·1019 1.2·1020 Deep-Att+PosUnkEnsemble[39] 40.4 8.0·1020 GNMT+RLEnsemble[38] 26.30 41.16 1.8·1020 1.1·1021 ConvS2SEnsemble[9] 26.36 41.29 7.7·1019 1.2·1021 Transformer(basemodel) 27.3 38.1 3.3·1018 Transformer(big) 28.4 41.8 2.3·1019 ResidualDropout Weapplydropout[33]totheoutputofeachsub-layer,beforeitisaddedtothe sub-layerinputandnormalized.",
    "Inaddition,weapplydropouttothesumsoftheembeddingsandthe positionalencodingsinboththeencoderanddecoderstacks.",
    "Forthebasemodel,weusearateof P =0.1. drop LabelSmoothing Duringtraining,weemployedlabelsmoothingofvalueϵ = 0.1[36].",
    "This ls hurtsperplexity,asthemodellearnstobemoreunsure,butimprovesaccuracyandBLEUscore.",
    "6 Results 6.1 MachineTranslation OntheWMT2014English-to-Germantranslationtask,thebigtransformermodel(Transformer(big) inTable2)outperformsthebestpreviouslyreportedmodels(includingensembles)bymorethan2.0 BLEU,establishinganewstate-of-the-artBLEUscoreof28.4.",
    "Theconfigurationofthismodelis listedinthebottomlineofTable3. Trainingtook3.5dayson8P100GPUs.",
    "Evenourbasemodel surpassesallpreviouslypublishedmodelsandensembles,atafractionofthetrainingcostofanyof thecompetitivemodels.",
    "OntheWMT2014English-to-Frenchtranslationtask,ourbigmodelachievesaBLEUscoreof41.0, outperformingallofthepreviouslypublishedsinglemodels,atlessthan1/4thetrainingcostofthe previousstate-of-the-artmodel.",
    "TheTransformer(big)modeltrainedforEnglish-to-Frenchused dropoutrateP =0.1,insteadof0.3.",
    "drop Forthebasemodels,weusedasinglemodelobtainedbyaveragingthelast5checkpoints,which werewrittenat10-minuteintervals.",
    "Forthebigmodels,weaveragedthelast20checkpoints.",
    "We usedbeamsearchwithabeamsizeof4andlengthpenaltyα = 0.6[38].",
    "Thesehyperparameters werechosenafterexperimentationonthedevelopmentset.",
    "Wesetthemaximumoutputlengthduring inferencetoinputlength+50,butterminateearlywhenpossible[38].",
    "Table2summarizesourresultsandcomparesourtranslationqualityandtrainingcoststoothermodel architecturesfromtheliterature.",
    "Weestimatethenumberoffloatingpointoperationsusedtotraina modelbymultiplyingthetrainingtime,thenumberofGPUsused,andanestimateofthesustained single-precisionfloating-pointcapacityofeachGPU5.",
    "6.2 ModelVariations ToevaluatetheimportanceofdifferentcomponentsoftheTransformer,wevariedourbasemodel indifferentways,measuringthechangeinperformanceonEnglish-to-Germantranslationonthe 5Weusedvaluesof2.8,3.7,6.0and9.5TFLOPSforK80,K40,M40andP100,respectively.",
    "8 Table3: VariationsontheTransformerarchitecture.",
    "Unlistedvaluesareidenticaltothoseofthebase model.",
    "AllmetricsareontheEnglish-to-Germantranslationdevelopmentset,newstest2013.",
    "Listed perplexitiesareper-wordpiece,accordingtoourbyte-pairencoding,andshouldnotbecomparedto per-wordperplexities.",
    "train PPL BLEU params N d",
    "d h d d P ϵ model ff k v drop ls steps (dev) (dev) ×106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 1 512 512 5.29 24.9 4 128 128 5.00 25.5 (A) 16 32 32 4.91 25.8 32 16 16 5.01 25.4 16 5.16 25.1 58 (B) 32 5.01 25.4 60 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 (C) 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 0.0 5.77 24.6 0.2 4.95 25.5 (D) 0.0 4.67 25.3 0.2 5.47 25.7 (E) positionalembeddinginsteadofsinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 developmentset,newstest2013.",
    "Weusedbeamsearchasdescribedintheprevioussection,butno checkpointaveraging.",
    "WepresenttheseresultsinTable3.",
    "InTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions, keeping the amount of computation constant, as described in Section 3.2.2.",
    "While single-head attentionis0.9BLEUworsethanthebestsetting,qualityalsodropsoffwithtoomanyheads.",
    "InTable3rows(B),weobservethatreducingtheattentionkeysized hurtsmodelquality.",
    "This k suggests that determining compatibility is not easy and that a more sophisticated compatibility functionthandotproductmaybebeneficial.",
    "Wefurtherobserveinrows(C)and(D)that,asexpected, biggermodelsarebetter,anddropoutisveryhelpfulinavoidingover-fitting.",
    "Inrow(E)wereplaceour sinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical resultstothebasemodel.",
    "6.3 EnglishConstituencyParsing ToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish constituencyparsing.",
    "Thistaskpresentsspecificchallenges: theoutputissubjecttostrongstructural constraints and is significantly longer than the input.",
    "Furthermore, RNN sequence-to-sequence modelshavenotbeenabletoattainstate-of-the-artresultsinsmall-dataregimes[37].",
    "Wetraineda4-layertransformerwithd =1024ontheWallStreetJournal(WSJ)portionofthe model PennTreebank[25],about40Ktrainingsentences.",
    "Wealsotraineditinasemi-supervisedsetting, usingthelargerhigh-confidenceandBerkleyParsercorporafromwithapproximately17Msentences",
    "[37].",
    "Weusedavocabularyof16KtokensfortheWSJonlysettingandavocabularyof32Ktokens forthesemi-supervisedsetting.",
    "Weperformedonlyasmallnumberofexperimentstoselectthedropout,bothattentionandresidual (section5.4),learningratesandbeamsizeontheSection22developmentset,allotherparameters remained unchanged from the English-to-German base translation model.",
    "During inference, we 9 Table4: TheTransformergeneralizeswelltoEnglishconstituencyparsing(ResultsareonSection23 ofWSJ) Parser Training WSJ23F1 Vinyals&Kaiserelal.",
    "(2014)[37] WSJonly,discriminative 88.3 Petrovetal.",
    "(2006)[29] WSJonly,discriminative 90.4 Zhuetal.",
    "(2013)[40] WSJonly,discriminative 90.4 Dyeretal.",
    "(2016)[8] WSJonly,discriminative 91.7 Transformer(4layers)",
    "WSJonly,discriminative 91.3 Zhuetal.",
    "(2013)[40] semi-supervised 91.3 Huang&Harper(2009)[14] semi-supervised 91.3 McCloskyetal.",
    "(2006)[26] semi-supervised 92.1 Vinyals&Kaiserelal.",
    "(2014)[37] semi-supervised 92.1 Transformer(4layers) semi-supervised 92.7 Luongetal.",
    "(2015)[23] multi-task 93.0 Dyeretal.",
    "(2016)[8] generative 93.3 increasedthemaximumoutputlengthtoinputlength+300.",
    "Weusedabeamsizeof21andα=0.3 forbothWSJonlyandthesemi-supervisedsetting.",
    "Our results in Table 4 show that despite the lack of task-specific tuning our model performs sur- prisinglywell,yieldingbetterresultsthanallpreviouslyreportedmodelswiththeexceptionofthe RecurrentNeuralNetworkGrammar[8].",
    "IncontrasttoRNNsequence-to-sequencemodels[37],theTransformeroutperformstheBerkeley- Parser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences.",
    "7 Conclusion Inthiswork,wepresentedtheTransformer,thefirstsequencetransductionmodelbasedentirelyon attention,replacingtherecurrentlayersmostcommonlyusedinencoder-decoderarchitectureswith multi-headedself-attention.",
    "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.",
    "On both WMT 2014 English-to-German and WMT 2014 English-to-Frenchtranslationtasks,weachieveanewstateoftheart.",
    "Intheformertaskourbest modeloutperformsevenallpreviouslyreportedensembles.",
    "Weareexcitedaboutthefutureofattention-basedmodelsandplantoapplythemtoothertasks.",
    "We plantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand toinvestigatelocal,restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs suchasimages,audioandvideo.",
    "Makinggenerationlesssequentialisanotherresearchgoalsofours.",
    "The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.",
    "Acknowledgements WearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitful comments,correctionsandinspiration.",
    "References",
    "[1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton.",
    "Layernormalization.",
    "arXivpreprint arXiv:1607.06450,2016.",
    "[2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.",
    "Neuralmachinetranslationbyjointly learningtoalignandtranslate.",
    "CoRR,abs/1409.0473,2014.",
    "[3] DennyBritz,AnnaGoldie,Minh-ThangLuong,andQuocV.Le.",
    "Massiveexplorationofneural machinetranslationarchitectures.",
    "CoRR,abs/1703.03906,2017.",
    "[4] JianpengCheng,LiDong,andMirellaLapata.",
    "Longshort-termmemory-networksformachine reading.",
    "arXivpreprintarXiv:1601.06733,2016.",
    "10 [5] KyunghyunCho,BartvanMerrienboer,CaglarGulcehre,FethiBougares,HolgerSchwenk, andYoshuaBengio.",
    "Learningphraserepresentationsusingrnnencoder-decoderforstatistical machinetranslation.",
    "CoRR,abs/1406.1078,2014.",
    "[6] Francois Chollet.",
    "Xception:",
    "Deep learning with depthwise separable convolutions.",
    "arXiv",
    "preprintarXiv:1610.02357,2016.",
    "[7] JunyoungChung,ÇaglarGülçehre,KyunghyunCho,andYoshuaBengio.",
    "Empiricalevaluation ofgatedrecurrentneuralnetworksonsequencemodeling.",
    "CoRR,abs/1412.3555,2014.",
    "[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith.",
    "Recurrent neural networkgrammars.",
    "InProc.ofNAACL,2016.",
    "[9] JonasGehring,MichaelAuli,DavidGrangier,DenisYarats,andYannN.Dauphin. Convolu- tionalsequencetosequencelearning.",
    "arXivpreprintarXiv:1705.03122v2,2017.",
    "[10] Alex Graves.",
    "Generating sequences with recurrent neural networks.",
    "arXiv preprint arXiv:1308.0850,2013.",
    "[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
    "Deep residual learning for im- age recognition.",
    "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pages770–778,2016.",
    "[12] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber.",
    "Gradientflowin recurrentnets: thedifficultyoflearninglong-termdependencies,2001.",
    "[13] Sepp Hochreiter and Jürgen Schmidhuber.",
    "Long short-term memory.",
    "Neural computation, 9(8):1735–1780,1997.",
    "[14] ZhongqiangHuangandMaryHarper.",
    "Self-trainingPCFGgrammarswithlatentannotations acrosslanguages.",
    "InProceedingsofthe2009ConferenceonEmpiricalMethodsinNatural LanguageProcessing,pages832–841.ACL,August2009.",
    "[15] RafalJozefowicz,OriolVinyals,MikeSchuster,NoamShazeer,andYonghuiWu.",
    "Exploring thelimitsoflanguagemodeling.",
    "arXivpreprintarXiv:1602.02410,2016.",
    "[16] ŁukaszKaiserandSamyBengio.",
    "Canactivememoryreplaceattention? InAdvancesinNeural InformationProcessingSystems,(NIPS),2016.",
    "[17] ŁukaszKaiserandIlyaSutskever.",
    "NeuralGPUslearnalgorithms. InInternationalConference onLearningRepresentations(ICLR),2016.",
    "[18] NalKalchbrenner,LasseEspeholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKo- rayKavukcuoglu.Neuralmachinetranslationinlineartime.arXivpreprintarXiv:1610.10099v2, 2017.",
    "[19] YoonKim,CarlDenton,LuongHoang,andAlexanderM.Rush.",
    "Structuredattentionnetworks. InInternationalConferenceonLearningRepresentations,2017.",
    "[20] DiederikKingmaandJimmyBa.",
    "Adam: Amethodforstochasticoptimization.",
    "InICLR,2015.",
    "[21] OleksiiKuchaievandBorisGinsburg.",
    "FactorizationtricksforLSTMnetworks.",
    "arXivpreprint arXiv:1703.10722,2017.",
    "[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio.",
    "A structured self-attentive sentence embedding.",
    "arXiv preprint arXiv:1703.03130,2017.",
    "[23] Minh-ThangLuong,QuocV.Le,IlyaSutskever,OriolVinyals,andLukaszKaiser.",
    "Multi-task sequencetosequencelearning.",
    "arXivpreprintarXiv:1511.06114,2015.",
    "[24] Minh-ThangLuong,HieuPham,andChristopherDManning. Effectiveapproachestoattention- basedneuralmachinetranslation.",
    "arXivpreprintarXiv:1508.04025,2015.",
    "11",
    "[25] MitchellPMarcus,MaryAnnMarcinkiewicz,andBeatriceSantorini.Buildingalargeannotated corpusofenglish: Thepenntreebank.",
    "Computationallinguistics,19(2):313–330,1993.",
    "[26] DavidMcClosky,EugeneCharniak,andMarkJohnson.",
    "Effectiveself-trainingforparsing.",
    "In ProceedingsoftheHumanLanguageTechnologyConferenceoftheNAACL,MainConference, pages152–159.ACL,June2006.",
    "[27] AnkurParikh,OscarTäckström,DipanjanDas,andJakobUszkoreit. Adecomposableattention model.",
    "InEmpiricalMethodsinNaturalLanguageProcessing,2016.",
    "[28] RomainPaulus,CaimingXiong,andRichardSocher.",
    "Adeepreinforcedmodelforabstractive summarization.",
    "arXivpreprintarXiv:1705.04304,2017.",
    "[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.",
    "Learning accurate, compact, and interpretable tree annotation.",
    "In Proceedings of the 21st International Conference on ComputationalLinguisticsand44thAnnualMeetingoftheACL,pages433–440.ACL,July 2006.",
    "[30] OfirPressandLiorWolf.",
    "Usingtheoutputembeddingtoimprovelanguagemodels. arXiv",
    "preprintarXiv:1608.05859,2016.",
    "[31] RicoSennrich,BarryHaddow,andAlexandraBirch.",
    "Neuralmachinetranslationofrarewords withsubwordunits.",
    "arXivpreprintarXiv:1508.07909,2015.",
    "[32] NoamShazeer,AzaliaMirhoseini,KrzysztofMaziarz,AndyDavis,QuocLe,GeoffreyHinton, andJeffDean.",
    "Outrageouslylargeneuralnetworks: Thesparsely-gatedmixture-of-experts layer.",
    "arXivpreprintarXiv:1701.06538,2017.",
    "[33] NitishSrivastava,GeoffreyEHinton,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdi- nov. Dropout: asimplewaytopreventneuralnetworksfromoverfitting.",
    "JournalofMachine LearningResearch,15(1):1929–1958,2014.",
    "[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.",
    "End-to-end memory networks.",
    "InC.Cortes, N.D.Lawrence, D.D.Lee, M.Sugiyama, andR.Garnett, editors, AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates, Inc.,2015.",
    "[35] IlyaSutskever,OriolVinyals,andQuocVVLe.",
    "Sequencetosequencelearningwithneural networks.",
    "InAdvancesinNeuralInformationProcessingSystems,pages3104–3112,2014.",
    "[36] ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna.",
    "Rethinkingtheinceptionarchitectureforcomputervision.",
    "CoRR,abs/1512.00567,2015.",
    "[37] Vinyals&Kaiser, Koo, Petrov, Sutskever, andHinton.",
    "Grammarasaforeignlanguage.",
    "In AdvancesinNeuralInformationProcessingSystems,2015.",
    "[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,MaximKrikun,YuanCao,QinGao,KlausMacherey,etal.",
    "Google’sneuralmachine translationsystem: Bridgingthegapbetweenhumanandmachinetranslation.",
    "arXivpreprint arXiv:1609.08144,2016.",
    "[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.",
    "Deep recurrent models with fast-forwardconnectionsforneuralmachinetranslation.",
    "CoRR,abs/1606.04199,2016.",
    "[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu.",
    "Fast and accurate shift-reduceconstituentparsing.",
    "InProceedingsofthe51stAnnualMeetingoftheACL(Volume 1: LongPapers),pages434–443.ACL,August2013.",
    "12 Input-Input Layer5 AttentionVisualizations tI tI si si ni ni siht siht tirips tirips taht taht a a ytirojam ytirojam fo fo naciremA naciremA stnemnrevog stnemnrevog evah evah dessap dessap wen wen swal swal ecnis ecnis 9002 9002 gnikam gnikam eht eht noitartsiger noitartsiger ro ro gnitov gnitov ssecorp ssecorp erom erom tluciffid tluciffid . .",
    ">SOE< >SOE< >dap< >dap< >dap< >dap< >dap< >dap< >dap< >dap< >dap< >dap< >dap< >dap< Figure 3: An example of the attention mechanism following long-distance dependencies in the encoderself-attentioninlayer5of6.",
    "Manyoftheattentionheadsattendtoadistantdependencyof theverb‘making’,completingthephrase‘making...moredifficult’.",
    "Attentionshereshownonlyfor theword‘making’.",
    "Differentcolorsrepresentdifferentheads.",
    "Bestviewedincolor.",
    "13 Input-Input Layer5 ehT ehT waL waL lliw lliw reven reven eb eb tcefrep tcefrep , , tub tub sti sti noitacilppa noitacilppa dluohs dluohs eb eb tsuj tsuj - - siht siht si si tahw tahw ew ew era era gnissim gnissim , , ni ni ym ym noinipo noinipo . .",
    ">SOE< >SOE< >dap< >dap< Input-Input Layer5 ehT ehT waL waL lliw lliw reven reven eb eb tcefrep tcefrep , , tub tub sti sti noitacilppa noitacilppa dluohs dluohs eb eb tsuj tsuj - - siht siht si si tahw tahw ew ew era era gnissim gnissim , , ni ni ym ym noinipo noinipo . .",
    ">SOE< >SOE< >dap< >dap< Figure4: Twoattentionheads,alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution.",
    "Top: Fullattentionsforhead5.",
    "Bottom: Isolatedattentionsfromjusttheword‘its’forattentionheads5 and6.",
    "Notethattheattentionsareverysharpforthisword.",
    "14",
    "Input-Input Layer5 ehT ehT waL waL lliw lliw reven reven eb eb tcefrep tcefrep , , tub tub sti sti noitacilppa noitacilppa dluohs dluohs eb eb tsuj tsuj - - siht siht si si tahw tahw ew ew era era gnissim gnissim , , ni ni ym ym noinipo noinipo . .",
    ">SOE< >SOE< >dap< >dap< Input-Input Layer5 ehT ehT waL waL lliw lliw reven reven eb eb tcefrep tcefrep , , tub tub sti sti noitacilppa noitacilppa dluohs dluohs eb eb tsuj tsuj - - siht siht si si tahw tahw ew ew era era gnissim gnissim , , ni ni ym ym noinipo noinipo . .",
    ">SOE< >SOE< >dap< >dap< Figure5: Manyoftheattentionheadsexhibitbehaviourthatseemsrelatedtothestructureofthe sentence.",
    "Wegivetwosuchexamplesabove,fromtwodifferentheadsfromtheencoderself-attention atlayer5of6.",
    "Theheadsclearlylearnedtoperformdifferenttasks.",
    "15 Ep.",
    "50 - Awakening from the Meaning Crisis - Tillich and Barfield Welcome back to Awakening from the Meaning Crisis.",
    "So last time we looked in depth at Corbin and Jung.",
    "And I tried to draw out very deeply the notion of the relationship to the sacred second self.",
    "I launched into sort of mutual criticism between Corbin and Jung and brought in some Buber along the way.",
    "And then I pointed to somebody whose work, also deriving from Heidegger integrates aspects of all of these together in kind of a profound way.",
    "Tillich is deeply influenced and aware of what he calls depth psychology, the kind of psychology in Jung.",
    "He, of course, is deeply aware of Heidegger.",
    "I don't know, I don't think that Tillich was aware of Corbin, but he is deeply aware of the symbol in an imaginal rather than in a merely imaginary way.",
    "Tillich takes the meaning crisis seriously.",
    "He writes, perhaps, his most well-known, and I think it's a masterpiece book, The Courage To Be as a response to the meaning crisis.",
    "Like Jung and Corbin, and for very related reasons, he's deeply critical of literalism and fundamentalism throughout.",
    "But he takes it deeper, as I mentioned, he really deepens it in terms of Heidegger's critique of ontotheology.",
    "And he becomes critical of literalism and fundamentalisms as forms of idolatry in which we are attempting to have rather than become.",
    "So there are some excellent books on the relationship between Jung and Tillich.",
    "This is a series of ongoing work by John Dourley.",
    "I recommend two books to you, The Psyche as Sacrament, which I have tweeted about in my book recommendations.",
    "I would also recommend his later book, Paul Tillich, Carl Jung and the Recovery of Religion.",
    "But make no mistake.",
    "Dourley is not talking about a recovery in a nostalgic sense.",
    "He writes another book called, Strategy For A Loss Of Faith, where he is trying to get beyond classical theism.",
    "And so I recommend Dourley's work as a comprehensive way of bringing about a deep dialogue and a kind of integration between Jung and Tillich.",
    "En-couragement",
    "Okay.",
    "So Tillich says the main response to the meaning crisis and here's how Tillich is not just theorizing.",
    "He is trying to give us guidance on how to live.",
    "And let's remember that this really matters because of, you know, the way Tillich resisted the Nazis.",
    "Because what Tillich talks about in The Courage To Be is courage.",
    "Now he's careful to note that this is a kind of existential courage that ultimately allows us to confront and overcome meaninglessness in its depth.",
    "But also, of course, to more practically respond to, you know, perverted responses to the meaning crisis itself like Nazi-ism and its Gnostic nightmare.",
    "This process of encouragement.",
    "Now he is like Aristotle.",
    "He's not talking about something as simple as just bravery facing danger or fortitude, the ability to endure.",
    "No.",
    "For Tillich, courage is a virtue.",
    "There's something of wisdom in courage.",
    "Courage involves within it that central feature of wisdom, which is seeing through illusion into reality.",
    "The brave person faces danger, but that's all we can say about them.",
    "The person with fortitude endures difficulty, but that's all we can say of them.",
    "The courageous person sees through the illusion and the distortion of fear or distress to what is truly good and acts accordingly.",
    "get the notes!",
    "Faith",
    "As Ultimate Concern So what is this seeing through and how does it help us confront the meaning crisis?",
    "So this notion of seeing through, seeing to the depths, as Tillich often says, is related to Tillich's notion of faith (writes Faith).",
    "And here, notice how we're circling back around.",
    "And this isn't really a circle because Tillich's notion of faith is not the assertion of propositions to believe.",
    "He is circling back to the ancient Israelite Hebrew notion of the Da'at.",
    "And now we can add that that participatory knowing, in a course of being, is an aspirational process.",
    "Tillich understands faith as ultimate concern (Fig. 1a) (writes Ultimate Concern).",
    "That which concerns us ultimately.",
    "His notion of idolatry is to treat something that could be a symbolic icon through which you articulate and develop your ultimate concern.",
    "You transform that into idol, an object to have and possess to control and manipulate.",
    "And you, thereby, are using the machinery that it's appropriate for ultimate concern for something that is not ultimate.",
    "Fig.",
    "1a What is ultimate concern?",
    "What is concern?",
    "Well, when you're concerned about something, you care about it, but you're also coping with it.",
    "You're committed to it.",
    "You're involved in it.",
    "It encompasses you, even though you are being involved in and through it.",
    "It is deeply perspectival and participatory.",
    "That's what he's trying to get at.",
    "And it is aspirational and it is open ended.",
    "It points towards the inexhaustibleness of the ground of being.",
    "So, of course, Tillich's notion is directed at Heidegger's notion of dasein, right?",
    "This concern that dasein is the being whose being is in question and therefore by our perspectival participatory knowing of our being, we come into deeper contact.",
    "But remember for Heidegger that requires a deep remembering, an overcoming of our forgetfulness, an aletheia.",
    "We have to have the ontological wonder towards the ground of meaning.",
    "The ground of being.",
    "This leads into Tillich's notion of God (writes God), which, I'm going to try and propose to you, is transgressive of classical theism in important ways, without it being identifiable with atheism in important ways.",
    "So Tillich understands God as an icon, as opposed to an idol.",
    "As an imaginal symbol for the ground of being.",
    "God symbolizes the ground of being and therefore God is no kind of being.",
    "There is a no-thingness (Fig. 1b)",
    "(writes No-thingness) to God, God is no kind of thing.",
    "And any attempt to reify to think of God as a thing is, for Tillich, a form of idolatry.",
    "Fig.",
    "1b",
    "So here's what I'm trying to get you to see what Tillich means.",
    "Here's meaning (Fig. 2) (writes Meaning).",
    "And here's reality (writes Reality).",
    "And here's the relationship between them (draws a line between Meaning and Reality).",
    "God is the simultaneous grounding of all of these (writes God below and draws three arrows to Meaning, the line, and Reality).",
    "Fig.",
    "2",
    "You can see the influence of Heidegger here, right?",
    "God is the ground of the meaning making, of reality and of the relationship between them.",
    "And any attempt to limit God to any one of these three components, just to the meaning, just to the reality, just to the relationship between them is, for Tillich, a profound kind of idolatry.",
    "This is why literalism and fundamentalism are so pernicious to him.",
    "So ultimate concern, if we allow ourselves to truly come into question and quest in a wondering way.",
    "If we participate in an aspirational trajectory motivated by ultimate concern, this puts us into a resonant relationship.",
    "This gets to what is known as Tillich's method of correlation (writes Method of correlation).",
    "This is how he saw himself doing theology.",
    "I know it's kind of odd for a, you know, naturalistic, cognitive scientist to be talking about theology again.",
    "But I think we've gotten to a place where that seems to you as a viable and valuable thing to do.",
    "Method Of Correlation",
    "What's the method of correlation?",
    "That there is always this ongoing tonos (Fig. 3) (writes Tonos)—there's polar tension, which we talked about earlier—between existential questioning (writes Existential questioning) , understood as existential questing, and what Tillich calls revelation the way the depths of reality reveal themselves (draws a double-headed arrow and writes Revelation).",
    "Fig.",
    "3",
    "These are always resonating with each other.",
    "The revelation has to fit the existential questioning, but the existential questioning has to fit itself.",
    "So there's a mutual dynamic fittedness going on—no, that's not, I want to get a verbal rather than an adjectival.",
    "Yeah—there is an ongoing resonant fitting, mutual fitting togetherness of the essential questioning and the revelation.",
    "You can see how this is very similar to anagoge.",
    "I think one of the problems that I see in a lot of interpreters of Tillich—and I've read quite a few—is that this method has been, this correlational method has been misunderstood as just propositional theology.",
    "That this is all about, you know, propositional proposals—listen to the word proposition.",
    "Propositional proposals.",
    "And then what we get are we get other propositions from the sacred texts, the Bible, and what we're doing is we're putting the propositions of theology into concordance with the propositions of the Bible.",
    "And I think that's to fundamentally trivialize Tillich to not get at the depths, the existential depths of the method of correlation.",
    "I want to propose a different way of understanding that that picks up on the tonos and takes us towards God as the ground of being as I've represented it to you.",
    "(erases the board)",
    "And this is the language Tillich uses.",
    "Tillich talks about the depths of reason (writes Depth of reason).",
    "This is a Platonic notion.",
    "That which makes reasoning possible.",
    "It's all the relevance realization machinery, I would argue.",
    "The recursive machinery of rationality, the aspirational rationality, that Callard talks about.",
    "All of those things, the depths of reason.",
    "And Tillich talks about that we have an ekstatic relationship (Fig. 4a) (writes Ekstatic below Depth of reason) ecstasy, right?",
    "The depths of our reason we're standing beyond ourselves.",
    "It's the depths of the psyche, but not just in the psychological way that Jung means, but also the depths of the grounding depths of our rationality.",
    "Fig.",
    "4a",
    "And then what stands between is a symbol (Fig. 4b) (writes Symbol beside Depth of reason) in Corbin's imaginal sense.",
    "And Tillich is so clear about that.",
    "The symbol to the depths of reality (writes Depths of reality beside Symbol).",
    "So in the psyche, the depths of reason are experienced as ekstasis, self-transcendence moving beyond myself.",
    "So crucial to aspiration, genuine transcendence in genuine self-transcendence.",
    "Fig.",
    "4b The depths of reality Tillich talks about, he uses two words here.",
    "He uses, sometimes, he uses the word miracle (Fig. 4c) (writes Miracle below Depths of reality) and this was probably like, I know for me, that's like, Oh no, right?",
    "And that's like angel, because this looks like magic in the pejorative sense of the word.",
    "He also talks about it as mystery (writes Mystery below Miracle).",
    "I think there's two ways using some Heidegger, which is fair because of the Heideggerian influence on Tillich.",
    "We can think of miracle as that aspect of being that we've talked about as the shining (writes Shining beside Miracle).",
    "And we can talk about the mystery as that aspect we've talked about as like the withdrawal into the moreness (writes Withdrawal and Moreness beside Mystery).",
    "Fig.",
    "4c The combinatorially explosive depths of reality.",
    "And, of course, I tried to argue through Heidegger and through Corbin, especially through Heidegger how these two (indicates Shining and Withdrawing) are interaffording in our sense of realness.",
    "And then the idea, the method of correlation is basically, as I suggested to you, the anagoge (Fig. 4d) (draws a double headed arrow between Ekstasis and Miracle & Mystery and writes Anagoge) between the ekstasis, as we resonate with the depths, the grounding and formative depths of reason are resonating with the grounding informative depths of realness",
    "and they are anagogically cycling together.",
    "Fig.",
    "4d So you've heard all of this before, but let's quickly repeat it for Tillich.",
    "The symbol is much more than a sign.",
    "Chris Mastropietro in his first discussion with me brought this out excellently.",
    "And that is clearly the case in Tillich.",
    "Symbols are participatory.",
    "The symbol opens up levels of reality otherwise closed to us.",
    "The symbol opens up levels of ourselves, otherwise closed to us, and it does this in a mutually affording resonant fashion.",
    "Symbols are not made by us.",
    "And this, of course, is something we've talked about before and we'll see it come out when we get to Barfield.",
    "They are self-organizing and they grow out of the unconscious within us and the unconscious without us.",
    "Symbols have a life.",
    "They can die, they can be born, they can live, they can die.",
    "Tillich worries that many of the symbols in Christianity are dying.",
    "And that fundamentalism and literalism are an inappropriate way of trying to hold on to them and keep them alive rather than affording the rebirth—sorry, that's the wrong word—affording the new birth of a new symbol that brings back the relationship, the resonant relationship that the old symbol possessed.",
    "Perhaps, this is what Jonathan Pageau means when he feels that Christianity in the meaning crisis is going through a profound death and rebirth of its symbolic structure.",
    "get the notes!",
    "For Tillich symbols have a surplus of meaning.",
    "There is a moreness to them.",
    "If they're not resonating with moreness, they're not symbols.",
    "They have a numinous character grounded in the resonant depth of mind and reality, and therefore symbols are deeply transformative.",
    "They're deeply transjective, their deeply translucent, et cetera, et cetera.",
    "So this is why correlation is not just propositional theology.",
    "If you're not undergoing a profound transformation, you're not doing Tillich's correlational method.",
    "So [-] how is it realized symbolically?",
    "It's realized symbolically—and you're going to see—okay, how is all of this, how is it realized by you?",
    "(erases the board)",
    "Both senses of the word realized, taken into your frame, but also actualized in reality.",
    "How is this transformative power of the symbol realized?",
    "It's realized in the relationship between the existential self (Fig.",
    "5a) (writes Existential self) and the essential self (writes The essential self beside Existential self).",
    "And do you see what's coming here?",
    "Fig.",
    "5a",
    "This is the relationship to—the relationship of the current self (indicates Existential self), the self in existence to the sacred second self (indicates The essential self).",
    "The essential self is the self in the fullness of being.",
    "Remember Plato's anagoge.",
    "The self in its fullness of being that is capable of recognizing through conformity, the fullness of being in the world.",
    "This relationship that Tillich is pointing to use, we can use language we've carefully worked out together.",
    "This relationship between the existential self and the essential self is aspirational.",
    "Tillich, like, he repeatedly talks about how the essential self is ahead, of course, not causally ahead.",
    "Normatively ahead.",
    "The essential self is ahead, not causally, but normatively.",
    "It's ahead of the existential self.",
    "The essential self beckons the existential self towards fulfillment.",
    "It's constantly tempting, if you'll allow me this language, the existential self to a better way of being.",
    "It's trying to tempt it out of the world in which it might be existentially trapped.",
    "This is the gnosis within Tillich.",
    "So the sacred second self.",
    "And for Tillich, this is bound up with like, you know, when Saint Paul was talking about, you know, remember when we talked about agape?",
    "I used to be this way, I acted like a kid.",
    "Now here's the new way.",
    "I act like a man.",
    "This is the way of agape.",
    "So my salience landscape is sophrosyne, it naturally self-organizes to constantly tempt me towards the good.",
    "And Tillich wrote a very excellent little book on agape.",
    "I highly recommend it to you.",
    "So this aspirational, transformative journey of encouragement, literally embodying courage.",
    "Encouragement (Fig. 5b) (writes En-couragement), right?",
    "Encouragement gets us to confront seriously meaninglessness.",
    "Now In The Courage To Be, Tillich goes through various historical developments.",
    "In the ancient world, the meaningless is confronted in our being as the finitude of our being.",
    "And he talks about how the Stoics responded to that.",
    "And then he talks about during the Christian period and especially during the Protestant reformation.",
    "Because Tillich is deeply influenced by Lutheranism in a very critical way though, of course.",
    "And the meaningless is confronted within our self-knowing as guilt.",
    "And he talks about the Protestant reformation as an attempt to seriously respond to the issue of guilt.",
    "And then now in our current period, right, we are experiencing meaninglessness in our self as despair, and he represents that.",
    "He says that is being represented by the existentialists.",
    "Fig.",
    "5b Of course, Heidegger—sorry, Tillich is writing in the period of the fifties and sixties when existentialism was still more prevalent.",
    "We, of course, can talk about things following on existentialism if we have more time, like postmodernism and other things that are represent—not representatives [-]—discussing and articulating a deeper way in which we are embodying the meaning crisis.",
    "But nevertheless, you can follow this trajectory through The Courage To Be.",
    "You can go through the Stoics.",
    "You can go through the Protestant Christians.",
    "You can go into the existentialist.",
    "The No-Thingness Of God And this trajectory leads us to a position beyond all three.",
    "The position that Tillich is arguing for which he calls the response to faith.",
    "Remember faith is Da'at for Tillich.",
    "It is not the willful assertion of belief.",
    "And here's the thing.",
    "And this is why idolatry is so pernicious for Tillich.",
    "The no-thingness of God coming to really encounter the no-thingness of God is central to this notion of faith.",
    "The no-thingness of God takes into itself the no-thingness that's—sorry, no,, I misspoke.",
    "The no-thingness—let me change how I'm pronouncing these terms because it will help.",
    "The no-thingness of God takes into itself the nothingness of meaninglessness and it overcomes it.",
    "The no-thingness of God has a transformative power over the nothingness of despair.",
    "So this is the notion of a fundamental aspect, identity shift.",
    "And here's where if I had time and I'll bring it out when I get into this other series.",
    "I would talk about Nishitani because his book of Religion and Nothingness is an extended, philosophical, profound examination of this fundamental aspect shift, identity shift.",
    "Remember what an aspect shift is.",
    "Remember the Necker cube, right?",
    "(Fig. 6) (draws a 3D outline of a cube)",
    "You're looking at something and the thing doesn't change, but the aspect by which you're seeing it changes.",
    "It flips.",
    "And what's salient?",
    "What's foreground and background?",
    "But this isn't just a shift of aspect, that's why I'm creating this neologism.",
    "Right?",
    "It's an aspect identity shift.",
    "Fig.",
    "6",
    "What does it mean?",
    "You come to see the no-thingness of God.",
    "You come to experience it as the inexhaustible creation of meaning.",
    "It is an inexhaustible fount of meaning cultivation.",
    "It is the ground of meaning intelligibility, the relationship between them, et cetera, for Tillich.",
    "Nishitani thinks the same thing can be found within Buddhism.",
    "That when we deeply realize the no-thingness of shunyata, when we participate it, when we identify with it, we gain the competence, the ability to aspect shift the nothingness of meaninglessness so that we come to see it instead as pointing to its ground, which is an inexhaustible source of meaning cultivation that cannot be drained dry by our despair.",
    "There is a fecundity at the level of fundamental framing and the way it's coupled to being that cannot be drained dry by despair.",
    "When we stop trying to push away the nothingness, but have instead an imaginal relationship to it and move through it anagogically in an imaginal fashion with the nothingness of God, then we overcome meaninglessness.",
    "We overcome meaninglessness.",
    "Nietzsche bumped up against this, right?",
    "He got close to it.",
    "If you stare long enough into the abyss, it begins to stare long enough into you.",
    "(Text overlay appears saying, 'John means \"If you stare long enough into the abyss it begins to stare back into you.",
    "\"').",
    "But you know what Nietzsche didn't do.",
    "He didn't stare long enough.",
    "He didn't look deeply enough.",
    "That's Nishitani's critique of Nietzschean nihilism.",
    "This fundamental aspects shift in which the nothingness of despair is transformed into the revelation of no-thingness as inexhaustible being meaning.",
    "This takes—and Tillich talks about the mystical tradition.",
    "There's a term from Gregory of Nyssa (writes Gregory of Nyssa) in the Eastern Orthodox Neoplatonic tradition.",
    "And you see it also in John Scotus Eriugena picks it up.",
    "This notion of epek-tasis (writes Epek-tasis).",
    "Epek-tasis.",
    "Which is such a cool sounding word.",
    "It sounds so cool anyways.",
    "So this is the idea.",
    "So there's a sort of standard, I suppose you might call it a teleological model of sort of salvation, right?",
    "Where the point is, you know, I'm moving towards a final destination, the promised land in which I will see God and I will come to rest in the promised land.",
    "So, you know, the whole point of a purpose is that it it self-dissolves.",
    "When I've achieved my purpose, I've realized my goal, then the pursuit has ended.",
    "get the notes!",
    "But Gregory of Nyssa and Eriugena have this different notion of epek-tasis.",
    "So here you're not trying to rest in God.",
    "God is not ultimately had even in resting in him.",
    "Instead, what the human being is engaging in—now for them, there's a mythos of this continues on after your death, in a life after death.",
    "And I'll put that aside.",
    "But nevertheless, the notion here of infinite self-transcendence in the infinity of God, right?",
    "There is no resting.",
    "There is only the constant disclosure of the inexhaustibleness of the ground of being.",
    "The transjectivity and the transframing never stopped.",
    "They never stopped.",
    "So the method of correlation and the encouragement are therefore, are emphasized as transjective in nature.",
    "Tillich explicitly and repeatedly argues that the symbol joins together and grounds—it joins together, but also grounds the subjective and the objective.",
    "He talks about how even Nietzsche uses this in the notion of will to power.",
    "Nietzsche in his notion of will to power is trying to use something that has a subjective meaning, will, but also, like, the will to power, power in existence, the way everything is sort of like Spinoza's conatus.",
    "Like, pushing itself and maintaining itself in existence.",
    "He's trying to get something that bridges between the subjective and the objective.",
    "And this is one of the ways in which therefore Tillich is different than Jung.",
    "(erases the board).",
    "And there's criticism in Tillich of Jung.",
    "Also appreciation of Jung.",
    "Tillich sees, of course, the process of individuation (Fig. 7a) (writes Individuation), very similar to the way Jung does.",
    "And this is perhaps the more subjective side of this symbolic imaginal relationship.",
    "But Tillich always, and this is the tonos (writes Tonos above Individuation) again, always puts that into creative tension with participation (draws a double-headed arrow and writes Participation), not just in groups, although he's not excluding that, but this is your participation in being.",
    "Fig.",
    "7a",
    "And he relates that to—and this is really interesting—neither the autonomy of being—sorry, that's wrong, wrong word—autonomy of reason (Fig. 7b) (writes Neither the autonomy of reason below Individuation) emphasized in the enlightenment nor what he calls a heteronomous (writes Heteronomous below Participation) or sometimes he even uses the more religious term, the demonic imposition of authority without—from without (writes Demonic imposition of authority from without).",
    "Fig.",
    "7b",
    "And see how this just so beautifully lines up with the problem of aspiration?",
    "Remember it can't be something that just slams you from the outside that you passively receive.",
    "It can't be something that you just autonomously make, or you don't actually get genuine self-creation or what I would call self-transcendence.",
    "Tillich sees this overcome in what he calls theonomous (Fig. 7c) (writes Theonomous).",
    "Which literally means God-ordered, God-governed, but, of course, God here means the ground of being, the ongoing Epek-tasis of the inexhaustible, the affordance of ongoing transframing.",
    "Fig.",
    "7c",
    "So, what we see here is transjectivity, the sacred second self.",
    "We see the anagogic ascent joining reason and revelation together, and the fundamental aspect shift, the deep criticism of fundamentalism and literalism.",
    "And there is, of course, therefore, something that is deeply about gnosis.",
    "And the connection to gnosticism and the transgression of theory of theism is explicit in Tillich.",
    "Tillich talks about this whole process, and this is why I qualified so much the use of the word theonomous.",
    "He qualifies this whole process as the process in which we are responding deeply to the meaning crisis.",
    "He calls this as a realization of the God beyond the God of theism.",
    "The God beyond the God of theism, which is a deeply transgressive statement.",
    "It's very Gnostic in the sense of seeing God as that, which is the demiurge entrapping us within existential entrapment.",
    "The existential entrapment of the meaning crisis.",
    "This whole process is so transjective and so transformative in nature and it is so deeply resistant to literalism, fundamentalism, and idolatry that it is going to take us to the God beyond the God of theism.",
    "Non-Theism Of Tillich This is the non-theism of Tillich (writes Non-theism).",
    "Non-theism is a position that tries to transcend theism and atheism.",
    "Many people are talking about it now.",
    "There's related ideas like anatheism (writes Anatheism), which is sort of the kind of theism that you get after going through atheisms.",
    "I think this is a better way of talking about it non-theism.",
    "Non-theism is the correct and appropriate way, as I've already mentioned, of talking about religions like Buddhism and Taoism.",
    "Non-theism is the rejection of the presuppositions that are shared by both theism and atheism.",
    "I will go into this in much greater detail in another series.",
    "Let me just give you—and this is not an exhaustive list.",
    "It is just a preliminary list, but nevertheless, I think a good starting point.",
    "What are four shared presupposition between the classical theist and the atheist?",
    "Number one, God is the Supreme being.",
    "The theist accepts that.",
    "Gives a yes to that.",
    "And the atheist gives a no to it, but they both accept that proposition as the one they are debating about.",
    "The non-theist rejects that.",
    "Number two, God is accessed primarily or even solely through belief.",
    "The theist and the atheist agree to this.",
    "They just disagree about whether or not there's really any access to be found.",
    "The non-theist rejects both of these.",
    "Number three, theology or anti-theology, which is what atheism often engages in.",
    "Although I'm not equating atheism to anti-theology.",
    "But theology and anti-theology do not require transformative anagoge.",
    "All you need to do is have possession of the propositions and be able to infer the correct implications.",
    "Thereby losing everything that we've been talking about in these last four episodes.",
    "The theist and the atheist agree with that proposition, the non theist rejects it.",
    "Number four, sacredness is personal or impersonal.",
    "The theist and the atheist disagree about which one of those to pick.",
    "The theist says it's personal.",
    "The atheist says it's impersonal.",
    "I agree that trying to say that the atheist has nothing that functions like sacredness in their life—this just does not sit with their performative existence.",
    "Where—but I do agree when the atheist says that they do not share the theist notion of sacredness as something fundamentally personal.",
    "The non-theist rejects that.",
    "The non-theist rejects that sacredness is personal or impersonal.",
    "Rather because the non-theist rejects the Cartesian grammar that drives it.",
    "The non-theist argues that sacredness is transjective participatory.",
    "It is aspirational.",
    "This is what Tillich was going on about.",
    "My main criticism of Tillich is, although in one way, he's way more practical than Heidegger.",
    "He's giving us guidance on how to live, how to cultivate courage and faith.",
    "He does not offer practices of transformation.",
    "See, Jung actually created a practice, intra-psychic though it might be, he created a practice for enacting and cultivating the imaginal.",
    "He created active imagination, which is not just to, in an imaginary sense, call up images.",
    "It's not just to conceptually think about things it's to allow images to self-organize in an autopoietic fashion, such that the depths of the psyche are revealed so that the self and the ego can talk to each other.",
    "Jung creates a practice of active imagination.",
    "He creates a practice of dream interpretation, and that is sadly missing Tillich.",
    "Tillich is better than Heidegger.",
    "I would argue.",
    "In that Tillich gives us a way to live, courage and faith deeply reinterpret it, but he does not give us the processes that Jung gives us.",
    "Owen Barfield",
    "This notion of deep symbolic participation that is translated into practices, I think goes to the heart of Owen Barfield's work.",
    "So Barfield is one of the inklings.",
    "He is part of the ongoing discussion and fellowship between Tolkien and C.S. Lewis of course, Barfield, Charles Williams.",
    "Those are the core four, and then a bunch of other people.",
    "I would recommend three books to try and get a better sense of Barfield.",
    "Lost Knowledge of the Imagination by Lachman, which I've already recommended to you.",
    "The Fellowship: The Literary Lives of the Inklings—and there's that word inkling again.",
    "The Literary Lives of the Inklings by Philip Zaleski and Carol Zaleski.",
    "There's the anyways, that's a very good book.",
    "And then the book I would most heartily recommend, it's a book that I'm finding brilliant.",
    "It's the book, Owen Barfield: Philosophy, Poetry, and Theology by (writes Di Fuccia)—all of this will come up, of course, on the panels.",
    "So a couple of things.",
    "Barfield is definitely influenced by Gnosticism.",
    "Gnosticism.",
    "He's influenced by Rudolf Steiner.",
    "And I won't go into it.",
    "You can read the book in depth.",
    "Steiner has like—I'll either say too little or too much about Steiner.",
    "It's a mystery to many people, myself included, why Barfield was so taken with Steiner and thought so highly of him.",
    "But if you reinterpret Steiner as basically a modern Gnostic, who's generating a gnostic mythology, the two worlds and the divine spark and all this kind of stuff, then you understand perhaps why Barfield was so enamored with Steiner.",
    "I think Steiner was the vehicle whereby Gnosticism comes into Barfield's thinking.",
    "So Barfield is therefore influenced by gnosis.",
    "He's influenced by Neoplatonism through Coleridge, the Romantic poet, who also engages in some important philosophy.",
    "Di Fuccia makes very clear.",
    "This is one of the most profound parts of the book, the deep indebtedness that Coleridge had to the early Romantics or the post-Kantians people that followed Kant and go on a different path from the later Romantics and Hegel.",
    "So a prototypical figure here from these early Romantics or post-Kantians is somebody like Schlegel and what these early Romantics emphasized is they emphasized the infinity.",
    "(erases the board)",
    "The infinity of reality (writes In-finity)",
    "The Infinity Of Reality This is really interesting.",
    "I find it so.",
    "When I'm trying to get the non-finiteness, the lack of being bound, being fully frameable, rather than just being uncountable, although I'm alluding, of course, again, to combinatorial explosion.",
    "So they emphasize the infinity of reality.",
    "And this term is used by them and by Di Fuccia in his examination of Owen Barfield.",
    "This is the inexhaustibleness, the inexhaustible moreness.",
    "And the idea is that the inexhaustible moreness is that which continually draws us, constantly draws us and affords us into self-transcendence, that inexhaustible moreness.",
    "So Schlegel had a way of putting this.",
    "The finite longing for the infinite.",
    "The finite longing for the infinite.",
    "It is this eduction (Fig.",
    "8) (writes Eduction)—I'm using this word to draw out, which, of course, became our word education (writes Education below Eduction), which if we think of education aspirationally is fine.",
    "The way Callard does.",
    "It is this eduction that discloses or reveals the sacredness.",
    "Fig. 8",
    "So our transjectivity, our finite, [-] our epek-static (writes Epek static) trajectory are finite—are always finite, are always framed, longing for the transframing that discloses, but never completely discloses the combinatorially explosive inexhaustible moreness of reality, and simultaneously discloses the ongoing capacity of relevance realization to adapt to that in a coupled manner.",
    "We experience and we participate.",
    "I'm using this as a transitive verb.",
    "We experience and participate this in creativity, not creativity, just in the sense of making, but the creativity that you experienced in the flow state.",
    "See how this is different from the later Romantics?",
    "This is not to find the contact in realness in some irrational locus in the psyche or in Hegel as the dialectic of a system, a propositional system.",
    "But instead it's to find sacredness in the flow of self-transcendence within creativity.",
    "And this is what is meant by poesis (writes Poesis).",
    "What we translate as poetry.",
    "get the notes!",
    "Poesis As Ekstasis In Creativity Barfield picks up on this poesis as ekstasis in creativity, the way we stand beyond ourselves in creativity.",
    "And he's very clear about how this is a transformative experience.",
    "There's a felt change in consciousness.",
    "The self after is both continuous and discontinuous from the self before the transformative experience.",
    "All that stuff we've been talking about with the sacred second self and aspiration.",
    "There's an ekstasis within the creative—there's an ekstasis in creativity.",
    "There's an ekstasis in creativity found within poetry and the poetical aspects of everyday language that can reawaken us to this kind of connectedness.",
    "To the inexhaustibleness.",
    "A connectedness that experiences as sacredness.",
    "So Barfield looks at words, the etymology, the history of words.",
    "And I touched on this briefly earlier when I was talking about symbols and I made some, sort of, criticisms of Barfield, which I noted at the time were preliminary and promissory.",
    "And I promised to come back to them in more depth as a way of trying to both defend my criticism, but also to defend a deeper reading of Barfield.",
    "So you remember (erases the board), the idea here is we have a word like pneuma (writes Pneuma), the Greek for spiritus (writes Spiritus beside Pneuma).",
    "And Barfield knows if you go—and for us—so for the Greeks, for the Latins, for the Romans, for spiritus, it can mean both wind (Fig. 9) (writes Wind) or what we now think of the word spirit sort of the self-moving aspects of the psyche (writes Self-moving aspects of the psyche).",
    "And we divide it into spirit is this (indicates Fig. 9).",
    "We really can't see it that way, but there's a division here and for Barfield, this division replicates this sort of Cartesian division between the objective world of like wind and the subjective world of what's going on in the psyche in itself movement or self-contact.",
    "But what Barfield says is when you go back, there isn't this (draws a line between Wind and Self-moving aspects of the psyche).",
    "These terms are used and they're treated as if they have a kind of identity, what I would argue as a non-logical identity, [-] they interpenetrate, they inter-afford each other, these meanings that we see as so antithetical, so disjunctive with respect to each other.",
    "Fig.",
    "9",
    "For Barfield, people who used the word that way were engaged in a form of participation.",
    "This is a way of being before the division, before the Cartesian disjunction, I would argue that what Barfield is pointing to is that these people had a more transjective anagogic resonance with reality so that the wind is imaginal for them in that it discloses the self-moving aspects of reality and themselves in a highly resonant fashion.",
    "The reason I want to say that is because I'm not quite sure about his evolutionary hypothesis.",
    "I pointed to the work of Lakoff and Johnson, that show we, unlike the people of the ancient path, we use language in this fashion.",
    "A way that is pervasive through all of our cognition and speech.",
    "Words have these dual meanings and inner and outer meaning.",
    "I pointed to work that I've published on and work that they published on (erases the board).",
    "How we use the word attack (Fig. 10) (writes Attack).",
    "There's many examples.",
    "This is only one.",
    "That's the point.",
    "There's many, many of these examples.",
    "We use the word attack and we mean physical destruction (writes Physical distraction below Attack).",
    "Like I attack the castle, at least the intent to physically destroy it.",
    "But we also mean critical argumentation (writes Critical argumentation).",
    "Like he attacked that point that I just made and we don't feel they're not ideologically identical, but we don't feel them as, you know, radically disjunctive from each other.",
    "Fig.",
    "10 Notice we do it with this (writes See)—sorry (erases See).",
    "Yes.",
    "I want to do it that way.",
    "There's something here (Fig. 11) (draws a circle and draws a line below) and we'll use the word see (writes See).",
    "And we can use that to mean either visual experience (writes Visual experience below See) or we can use see to mean to understand (writes Understand below See).",
    "And then that converges back, we can use understand or originally unterstand, but we changed it to understand (writes Understand below the circle), and that can mean to stand under (writes Stand under below Understand), but it can also mean conceptual understanding (writes Conceptual understanding).",
    "And so there's a weird synonymy between understand and see.",
    "I'm trying to point out to you how complex this is.",
    "Fig.",
    "11",
    "Now what's interesting about Lakoff and Johnson is they don't claim this as an evolutionary thing, pointing to ancient ways of being conscious.",
    "This is something pervasive in our cognition and our culture right now.",
    "And it doesn't point to the evolution across generations.",
    "It points to psychological development within individuals.",
    "Psychological development within individuals.",
    "According to Lakoff and Johnson, we start out in sensory motor seeing and then that gets taken up into this conceptual sense of seeing.",
    "We start with a sensory motor way of understanding that gets taken up.",
    "We start as a sensory motor way of attacking.",
    "And that gets taken up.",
    "It's a psychological process of development and it is ongoing right now",
    "and it is pervasive.",
    "There might be something sort of fundamentally wrong therefore with Barfield's evolutionary analysis.",
    "Now Vervaeke and Kennedy argue that the psychological development is deeper than as being represented by Lakoff and Johnson.",
    "We argued that the model is too simplistic.",
    "As I mentioned, we argued that there should be a top-down aspect to this psychological development, not just bottom-up.",
    "So there's a sense in which we agree with Lakoff and Johnson that there's stuff coming up from the sensory (Fig. 12) (writes Sensory and draws an upward arrow above it), but we think—and here's the influence of Corbin, right?",
    "Especially on me.",
    "I don't know if there's an influence on John Kennedy, but definitely on me.",
    "Here's the abstract intellectual (writes Intellectual above Sensory and draws a downward arrow below) and here's the concrete sensory, and then they meet together in the imaginal (writes Imaginal between Intellectual and Sensory).",
    "That when I say, I see what you're saying, this is an imaginal way that's getting a bottom-up from seeing as a sensory motor thing, and then bringing into imaginal expression this abstract not yet speakable sense of understanding.",
    "Fig.",
    "12 That helps to explain why these two (indicates See and Understand) very different sensory motor things, verbal experience, standing under—or a totally different one: grasping—or yet another one: getting.",
    "Why do all of these converge?",
    "Because there is something like an intellectual form that they converge upon, but it is also expressed, developed through these different imaginal renderings that connect back to concrete instances of the sensory motor.",
    "I argued that we can link that to Michael Anderson's notion of their massive redeployment hypothesis.",
    "The circuit we use, cognitive exaptation.",
    "I went through this idea of how the symbolic—now you can understand that's the imaginal, is this process of re-exaptation (writes Re-exaptation).",
    "How I can invoke balance.",
    "I can evoke balance to talk about justice.",
    "And then I have the image, not an imaginary, the imaginal statue of lady justice, as a way of using, re-exapting the physical balance machinery and using that machinery to give a structural functional organization to this hard to articulate ineffable sense of justice.",
    "Recycling that whole process and inducing new functions.",
    "It's an enacted metaphor.",
    "It's an enacted symbol.",
    "This is poiesis, I think in its deepest sense.",
    "This helps to explain the translucency of the symbol, why we can see through it.",
    "See it and see through it.",
    "I can look at it as physical balance, but I can see through it into justice.",
    "And justice and balance are not logically identical, but they're not separate from each other.",
    "It also explains our temptation to literalism and idolatry.",
    "We can forget, we can forget justice and focus just on having balance.",
    "We can lose the iconic seeing through and only look at the concrete.",
    "get the notes!",
    "Final Participation Now back to Barfield's evolutionary schema, he talks about that we had original participation and then there's the division, which is the meaning crisis.",
    "And he's really explicit about the meaning crisis.",
    "Read the opening essay in The Rediscovery of Meaning.",
    "He's really clear about that.",
    "And then we have the two worlds of mythology and everything is being broken up.",
    "The inner and the outer are being separated.",
    "The subjective, the objective, blah, blah, blah.",
    "You know, all of this, the idea is what we need to do is to move to what he calls final participation as a response to the meaning crisis.",
    "Final participation is a recovery of participation integrated within the gains of the rational sciences.",
    "Now he says that",
    "and that's explicitly and importantly what he means.",
    "And I take him at his word.",
    "Part of what that means, and this is what he emphasizes, is the recovery of the perspectival and the participatory.",
    "And I think that is deeply right and deeply consonant, but here's where I'm critical of Barfield.",
    "But I think it also means, and this is where Barfield does not do, I think, good work.",
    "It also means a science of meaning cultivation.",
    "How does that participatory and perspectival participation fit into our scientific processes, our scientific way of being?",
    "If you're going to integrate in final participation, participation with the scientific, rational mind, both sides have to be involved in this marriage, or it will fail.",
    "That, of course, is what I've tried to do with relevance realization theory, and then put it into discourse with spirituality, symbolism, sacredness, and these great prophets of the meaning crisis.",
    "Now here's more of a criticism of Barfield's followers.",
    "I think there needs to be more understanding of how much Barfield is indebted to Coleridge and Schlegel and understanding sacredness as a poiesis participation of the inexhaustible within transformative creativity.",
    "You can't simply import Barfield into classical theism and say, Oh, he's just talking about the things we've always been talking about.",
    "How is that going to bring about final participation?",
    "That is not fair to Barfield's argument or his ideas.",
    "This leads to the point that Di Fuccia argues that this is what makes Barfield different to Heidegger.",
    "Heidegger, as we saw, took from Eckhart, this notion of relating to this, letting the rose be.",
    "And he takes up this notion from Eckhart.",
    "Gelassenheit (writes Gelassenheit) .",
    "John Caputo talks a lot about this, the letting be.",
    "And he tends to emphasize a deep—so you can see what Heidegger's doing.",
    "He's trying to respond, but like Nietzsche, he's overcompensating Descartes' notion of the complete activity of mind.",
    "So Heidegger responds by a complete passivity, Gelassenheit.",
    "Letting be, letting be.",
    "It's a deep passivity.",
    "It's so bloody Lutheran.",
    "And there's something deeply right about that aspect of Eckhart, but Heidegger forgets the other important term in Eckhart, Durchbrock, Durchbrock.",
    "Breakthrough.",
    "Breakthrough (writes Breakthrough).",
    "You know what breakthrough is all about?",
    "It's about attentional scaling (draws an arrow pointing to Break), breaking the inappropriate frame (draws an arrow pointing to Through) moving through and making the new frame.",
    "Durchbrock is just as important as gelassenheit .",
    "And this is something Barfield picks up on.",
    "That his notion of creativity as participatory is not to be just passively receptive.",
    "Of course, it's not what Heidegger criticizes either.",
    "The Cartesian technological imposition of our will on the world.",
    "That's not what's meant by poiesis either.",
    "Poiesis is synergistic.",
    "God—because I think Barfield is ultimately non-theistic in some very important ways—God plays the leading role, but we contribute.",
    "And this was the original Hebrew insight of the da'at.",
    "We're not just passive recipients of history, nor are we it's complete authors.",
    "We participate history.",
    "We participate in history and we are synergistically working with God in its making.",
    "Is Barfield a non-theist?",
    "I don't know.",
    "I can't make that argument as clearly as I can make it for Heidegger, for Jung, for Corbin, for Tillich.",
    "I suspect though, if Barfield were to talk to these other prophets of the meaning crisis, he would also be led into a kind of non-theism.",
    "That is clearly the case with people like Schlegel, who so deeply influenced him.",
    "What have I tried to show you?",
    "I've tried to show you that the language—not the language, the vocabulary, the grammar, the framework of relevance realization and how it can be developed to talk about spirituality and sacredness can be put into deep dialogue with Heidegger.",
    "Deep dialogue with Corbin.",
    "Deep dialogue, with Jung.",
    "Deep dialogue, with Tillich.",
    "Deep dialogue with Barfield.",
    "And also afford deep dialogue, critical but creative dialogue, between them and afford a potential synoptic integration.",
    "All of this is what I've meant by and what I mean by awakening from the meaning crisis.",
    "Thank you so very much for this long journey, we have traveled together.",
    "I've often taxed your attention, your patience, your understanding, your good spirits.",
    "I thank you for the ongoing support and appreciation and encouragement many of you have given me.",
    "And I look forward to an ongoing dialogue in the next series that I will create.",
    "Thank you very much.",
    "And I want to thank deeply the crew, constant here.",
    "My brothers in this project who continually afforded it and made it possible to be presented to you at the time exemplary level of excellent quality.",
    "Thank you very much one and all.",
    "- END - Episode 50 Notes To keep this site running, we are an Amazon Associate where we earn from qualifying purchases Paul Tillich Paul Johannes Tillich was a German-American Christian existentialist philosopher and Lutheran Protestant theologian who is widely regarded as one of the most influential theologians of the twentieth century.",
    "Book Mentioned: The Courage To Be – Buy Here Book Mentioned: Morality & Beyond - Buy",
    "Here John Dourley John P. Dourley was a Jungians analyst, a professor of religious studies, and a Catholic priest.",
    "He taught for many years at Carleton University in Ottawa, his doctorate being from Fordham University.",
    "Book Mentioned: The Psyche as Sacrament – Buy Here Book Mentioned: Paul Tillich, Carl Jung and the Recovery of Religion – Buy Here Book Mentioned: A Strategy for a Loss of Faith - Buy Here Da'at",
    "In the branch of Jewish mysticism known as Kabbalah, Daʻat is the location (the mystical state) where all ten sefirot in the Tree of Life are united as one.",
    "Dasein Dasein is a German word that means \"being there\" or \"presence\", and is often translated into English with the word \"existence\".",
    "Aletheia Aletheia is truth or disclosure in philosophy.",
    "Jonathan Pageau Sophrosyne Sophrosyne is an ancient Greek concept of an ideal of excellence of character and soundness of mind, which when combined in one well-balanced individual leads to other qualities, such as temperance, moderation, prudence, purity, decorum, and self-control.",
    "Keiji Nishitani Keiji Nishitani was a Japanese university professor, scholar, and Kyoto School philosopher.",
    "He was a disciple of Kitarō Nishida.",
    "Book Mentioned: Religion and Nothingness – Buy Here Necker cube",
    "The Necker cube is an optical illusion that was first published as a Rhomboid in 1832 by Swiss crystallographer Louis Albert Necker.",
    "Śūnyatā Śūnyatā – pronounced in English as /ʃuːnˈjɑː.tɑː/ (shoon-ya-ta), translated most often as emptiness, vacuity, and sometimes voidness – is a Buddhist concept which has multiple meanings depending on its doctrinal context.",
    "Friedrich Nietzsche Friedrich Wilhelm Nietzsche was a German philosopher, cultural critic, composer, poet, writer, and philologist whose work has exerted a profound influence on modern intellectual history.",
    "Gregory of Nyssa Gregory of Nyssa, also known as Gregory Nyssen, was bishop of Nyssa from 372 to 376 and from 378 until his death.",
    "John Scotus Eriugena John Scotus Eriugena or Johannes Scotus Erigena or John the Scot was an Irish Catholic Neoplatonist philosopher, theologian and poet in the Middle Ages.",
    "Will to power The will to power is a prominent concept in the philosophy of Friedrich Nietzsche.",
    "The will to power describes what Nietzsche may have believed to be the main driving force in humans.",
    "However, the concept was never systematically defined in Nietzsche's work, leaving its interpretation open to debate.",
    "Spinoza (Other names: Benedictus de Spinoza)",
    "Spinoza, Baruch, Dutch philosopher, of Portuguese-Jewish descent; also called Benedict de Spinoza.",
    "Spinoza espoused a pantheistic system, seeing ‘God or nature’ as a single infinite substance, with mind and matter being two incommensurable ways of conceiving the one reality.",
    "Richard Kearney Richard Kearney is an Irish philosopher and public intellectual specializing in contemporary continental philosophy.",
    "He is the Charles Seelig Professor in Philosophy at Boston College and has taught at University College Dublin, the Sorbonne, the University of Nice, and the Australian Catholic University.",
    "Book Mentioned: Anatheism: Returning To God After God - Buy",
    "Here Owen Barfield Arthur Owen Barfield was a British philosopher, author, poet, critic, and member of the Inklings.",
    "Book Mentioned: The Rediscovery of Meaning, and Other Essays - Buy Here Gary Lachman Gary Joseph Lachman, also known as Gary Valentine, is an American writer and musician.",
    "Book Mentioned:",
    "Lost Knowledge of the Imagination - Buy Here Philip Zaleski Philip Zaleski is the author and editor of several books on religion and spirituality, including The Recollected Heart, The Benedictines of Petersham, and Gifts of the Spirit.",
    "Book Mentioned: The Fellowship: The Literary Lives of the Inklings - Buy Here Michael Vincent Di Fuccia Book Mentioned: Owen Barfield: Philosophy, Poetry, and Theology - Buy Here Samuel Taylor Coleridge Samuel Taylor Coleridge was an English poet, literary critic, philosopher and theologian who, with his friend William Wordsworth, was a founder of the Romantic Movement in England and a member of the Lake Poets.",
    "Georg Wilhelm Friedrich Hegel Georg Wilhelm Friedrich Hegel was a German philosopher and is considered one of the most important figures in German idealism.",
    "Friedrich Schlegel Karl Wilhelm Friedrich Schlegel was a German poet, literary critic, philosopher, philologist, and Indologist.",
    "With his older brother, August Wilhelm Schlegel, he was one of the main figures of Jena Romanticism.",
    "George Lakoff George Philip Lakoff is an American cognitive linguist and philosopher, best known for his thesis that people's lives are significantly influenced by the conceptual metaphors they use to explain complex phenomena.",
    "Mark Johnson Mark L. Johnson is Knight Professor of Liberal Arts and Sciences in the Department of Philosophy at the University of Oregon.",
    "He is known for contributions to embodied philosophy, cognitive science and cognitive linguistics, some of which he has coauthored with George Lakoff such as Metaphors We Live By.",
    "John Caputo John David Caputo is an American philosopher who is the Thomas J. Watson Professor of Religion Emeritus at Syracuse University and the David R. Cook Professor of Philosophy Emeritus at Villanova University.",
    "Book Mentioned: The Mystical Element In Heidegger’s Thought - Buy Here Gelassenheit Often translated as \"releasement,\" Heidegger's concept of Gelassenheit has been explained as \"the spirit of disponibilité",
    "[availability] before What-Is which permits us simply to let things be in whatever may be their uncertainty and their mystery.\"",
    "Meister Eckhart Eckhart von Hochheim OP, commonly known as Meister Eckhart or Eckehart, was a German theologian, philosopher and mystic, born near Gotha in the Landgraviate of Thuringia (now central Germany) in the Holy Roman Empire.",
    "Poiesis In philosophy, poiesis is \"the activity in which a person brings something into being that did not exist before.",
    "Other helpful resources about this episode: Notes on Bevry Additional Notes on Bevry Ep.",
    "49 - Awakening from the Meaning Crisis - Corbin and Jung Welcome back to Awakening from the Meaning Crisis.",
    "So last time we followed Heidegger into the depths where we encountered Eckhart and this non teleological relationship to the play of being, and that led us very directly into Corbin.",
    "And Corbin's core argument that gnosis as the way we've been using it relates centrally—the ability to engage in this serious play relates centrally to the imagination.",
    "But Corbin is making use of this term in a new way.",
    "He makes a distinction between the imaginary, which is how we typically use the word, the imagination—mental images in my head that are only subjective and have no objective reality, and the imaginal.",
    "The imaginal which mediates between the abstract intelligible world and the concrete, sensible world and transjectively mediates between the subjective and the objective.",
    "And that is not done statically.",
    "All of this mediation and mutual affordance is done in an ongoing transformative transframing, and that the symbol captures all of this.",
    "And then I wanted to bring out Corbin's core symbol.",
    "And it's a core symbol that relates directly to gnosis because in gnosis, in this transformative participatory knowing, and this goes to the core of Heidegger's notion of dasein, the being whose being is in question, we have to see self-knowledge and knowledge of the world as inextricably bound up together.",
    "In order to do that, we are pursuing Corbin's central symbol, the angel, which, of course, is immediately off-putting to many people, including myself.",
    "But I've been trying to get a way of articulating how Corbin is incorporating both Heidegger and Persian Sufism—Neoplatonic Sufism into this understanding of the symbol.",
    "And I recommended that we take a look at the work of, first of all, Stang.",
    "The historical work showing how throughout the ancient Mediterranean world and up—through the Hellenistic period and beyond, up until the period of, easily, pseudo Dionysus around, you know, the fifth century, of the common era, there's the pursuit of the divine double.",
    "And then the idea is one that is deeply transgressive of our cultural, cognitive grammar, of decadent romanticism.",
    "Where we have a—we are born with our true self that nearly needs to express itself ala Rousseau.",
    "And the core virtue is authenticity, which is being true to the true self you have, you possess.",
    "Rather than, for example, a Socratic model in which the true self is something towards which you are constantly aspiring.",
    "And then I recommended trying to make this—so, and what's the transgressive mythology?",
    "Sorry, the transgressive mythology is that the self that I have now is not my true self.",
    "My true self is my divine double.",
    "This is something that is superlative to me.",
    "It is bound to me.",
    "It is my double.",
    "It is bound to me, but it is superlative to me.",
    "It is both me and not me.",
    "It's me, as I'm meant to be, as I should be.",
    "And that the project, the existential project is not one of expressing a self that you have, but of transcending to become a self that is ecstatically ahead of you in an important way.",
    "And then I pointed out that for many of you, this would still be sort of like, okay, but—I get the transgression, but I still find this notion of a divine double unpalatable.",
    "Maybe for some of you, you don't, but nevertheless, I think there is an important way by picking up on, like, by asking the question, why did so many people for so long believe in this so deeply?",
    "Picking up on the question of what's going on there and focusing on this aspirational process.",
    "And this takes us back into work that was core to the discussion I made about gnosticism.",
    "And this had a resounding impact at various places throughout the series, which is LA Paul's work on transformative experience.",
    "And then somebody who's from the same school influenced by Paul, having a different view.",
    "Whereas Paul is more—her transformations are more like insight.",
    "Agnes Callard's notion of aspiration is much more developmental, but I argued that they can be, I think, readily reconciled together if you see development as a linked sequence of insights that bring about qualitative change in your competence.",
    "So we were zeroing in on this.",
    "I'm using LA Paul and Agnes Callard to triangulate into this relationship of aspiration.",
    "And picking up, first of all, on Callard's important point that is not addressed—and this is an important point by LA Paul—the deep connections between aspiration and rationality.",
    "That rationality is itself an aspirational process.",
    "And if we make the process by which we become rational itself, not a rational or irrational process, we will get into a position that is seriously self-undermining.",
    "Similarly, if the way in which you become wise does not involve sort of wise acts and behavior.",
    "If the process itself is not itself wise, you're going to get into all kinds of difficulties.",
    "If it's not in some sense, a rational process.",
    "Again, last time I reminded you how broadly, but I think also deeply I'm using the term rational.",
    "Or being educated.",
    "I mean, we make ourselves better, maybe even more rational or wiser by going through an education, but education, at least a liberal education is a deeply aspirational process.",
    "If that itself is not part of what makes us rational, if it's not itself a rational process, then, of course, our rationality is again being undermined in self-contradictory fashion.",
    "So the basics of this argument is if we take, if we do not understand a kind of rationality Callard calls proleptic rationality, that's the rationality of aspiration (writes Proleptic).",
    "The rationality that emerges in education, that emerges in the cultivation of rationality, that emerges in the cultivation of wisdom, then a lot of human behavior is not going to be called rational.",
    "And it's going—and that is going to render our notion of rationality, as I've said, self-contradictory and self-undermining in some very fundamental ways.",
    "And so again, we see the rejoining of love and reason that was originally talked about so deeply in Plato.",
    "So now we've come back to this problem (erases the board).",
    "I gave you the example of somebody in a liberal education.",
    "And this is Callard's example, right?",
    "Here's the self at this time (Fig. 1a) (writes S1) and the self at this time (draws a horizontal arrow and writes S2) and something, I brought out that Callard doesn't but it's important.",
    "Because it's a concise way of talking about the relationship between them and that there isn't a direct inferential relationship between these which is non-logical identity (writes Non-logical identity).",
    "Fig.",
    "1a",
    "Right?",
    "This is part and parcel.",
    "Like, think about what I said earlier.",
    "how can we broaden the notion of rationality outside of logic?",
    "If Callard is right then we have to include rationality, proleptic rationality in our model of rationality and involves its non-logical identity.",
    "Then, of course, we're stepping beyond sort of a purely logical understanding of rationality yet again for yet another reason.",
    "Okay.",
    "So what's the problem here?",
    "The other problem is the problem of non-logical identity.",
    "So I don't appreciate—always remember both meanings of that term, to deeply—and they're interwoven, the two sides of aspiration.",
    "I deeply understand it, and I'm deeply grateful for it.",
    "I value it.",
    "I don't appreciate classical music.",
    "I don't have the taste for it, and I don't get it.",
    "And I want to be somebody who appreciates classical music.",
    "Now, if I want to, if I do that because I want to satisfy a current desire I have, a current value I have.",
    "Like, I value impressing my friends or I value attracting members of the opposite sex or something like that.",
    "Then, of course, I'm not actually aspiring because this person doesn't appreciate classical music because it impresses their friends or because it helps them in their dating life or for whatever other reason.",
    "They appreciate it for a perspectival and participatory knowing that S1 doesn't have.",
    "That's the point.",
    "The appreciation (Fig. 1b) (writes Appreciation below S2) that S2 has is bound to perspectival and participatory knowing (writes Perspectival and Participatory below Appreciation) of which S1 is ignorant.",
    "And that, of course, is one of the central points who you remember of LA Paul's argument about transformative experience.",
    "So that looks like there is something, there's a fundamental discontinuity here.",
    "Fig.",
    "1b",
    "Okay.",
    "Now they bring up the problem that we need to sort of resolve and bring back and tie this back to the notion of the divine double.",
    "I want to talk about a way in which Callard shows us how this (indicates S1 and S2) is problematic as we try to talk about it.",
    "get the notes!",
    "The Paradox Of Self-Creation",
    "And she makes use of the work of Strawson (Fig. 2a) (writes Strawson).",
    "Galen Strawson and he talks about a paradox of self-creation.",
    "Now Strawson points out that for self-creation (writes Self-creation)—and doesn't this (indicates S1 and S2) look like self-creation?",
    "Here is a self creating itself.",
    "For self-creation be truly an instance of self and creation, sort of emphasizing both sides of that, double form, two things are needed.",
    "Fig.",
    "2a",
    "One requirement is a continuity requirement.",
    "There has to be something deeply continuous between S1 and S2 (Fig. 2b) (writes S1 and S2 below Strawson), because if they are not the same self, then it's not an act of self-creation.",
    "They're not the same self.",
    "It's not an act of self creation.",
    "So that's the continuity requirement (writes Continuity requirement beside S2).",
    "And so I'm going to represent it like this (Writes = between S1 and S2) S1 equals S2.",
    "So, what this means is if, like, if S1 is hit by a motorcycle and their brain is damaged and they act and behave in a different way, that's not an act of self-creation.",
    "That is not an act of self-creation.",
    "S1 has to be totally responsible for S2.",
    "Or else it's not an act of self-creation, putting an emphasis on the 'self.'",
    "Fig.",
    "2b",
    "Okay.",
    "Now let's shift to the creation side, right?",
    "Which is that there has to be real novelty between them (Fig. 2c) (writes S1 and S2 - Real novelty) or else there is no creation involved.",
    "If S1 just develops a skill or ability they already have, that is not real novelty.",
    "That is just more of the same.",
    "That's quantitative development, not qualitative development.",
    "So if all that happens is S1, you know, improve the skill, you know, deepens their capacity to acquire something that they already value et cetera, that is not real novelty (writes ≠ between S1 and S2).",
    "So real novelty means there has to be a fundamental difference between S1 and S2.",
    "Fig.",
    "2c Now what Strawson does with this, is he points out, notice how S1 and S2 for the continuity requirement have to be equal, but the real novelty means there has to be a real deep difference between S1 and S2 or it's not creation.",
    "And so what he argues is he argues that self-creation is paradoxical.",
    "In fact, the point he's trying to make is it's self-contradictory.",
    "There can be no such thing as self-creation, alright?",
    "So another way of thinking about this is if you remember, when we talked about this in connection with transformative experience, we can invoke [inaudible] (message us if you know) notion of the idea that you can't sort of create a stronger logic by logically manipulating a weaker logic.",
    "No matter how much I manipulate the machinery of predicate logic, I won't get modal logic.",
    "Because what I have to do is I have to introduce axioms that are outside—for Godelian reasons, ultimately—are outside the system of predicate logic.",
    "So putting it this way, right, in order to get the real novelty between S1 and S2, I have to introduce something that's outside the logic of S1, the logic of its values and beliefs, that will then make it into S1.",
    "But if it comes from outside of S1, it is foreign and strange and therefore it is not an act of self-creation.",
    "What that shows— [inaudible] (message us if you know) idea about you can't infer a stronger logic from a weaker logic.",
    "And that goes back to a point we've made before.",
    "There is no inferential way.",
    "There's no way you can sort of infer yourself from S1 to S2.",
    "And this, of course, is part of Kierkegaard's whole point about the leap and the leap of faith.",
    "The leap of faith is to leap into a process of development that is going to put you through this kind of qualitative change in your identity.",
    "But Strawson makes this very problematic by saying, this makes absolutely no sense (Fig. 2d)",
    "(draws a bracket connecting S1=S2 and S1≠S2).",
    "And so we're caught between two things.",
    "Either we can break this by saying there is ultimately no self.",
    "We could go rabidly empiricist.",
    "I'm just a blank slate.",
    "And all that happens is stuff from the outside changes me.",
    "And then I go for the novelty (indicates S1≠S2), but there's no underlying self.",
    "Or I can just do the continuity requirement.",
    "I can become sort of a Rousseauan romantic.",
    "My self is identical throughout.",
    "And all I'm doing is expressing what was already within myself.",
    "That's all that's happening.",
    "You see empiricism and romanticism.",
    "Choose one of the two over the other.",
    "And then what Strawson says is you have to make such a choice because self-creation is itself self-contradictory Fig.",
    "2d Participating In An Emergence Through Aspiration Callard says this is all a mistake.",
    "And I agree with her.",
    "She argues that this is both the empiricism and the romanticism, at least the Rousseauan decadent romanticism is not adequate or accurate of our experimentive developmental change.",
    "What breaks this (indicates S1≠S2), I argue, helping her, I believe, is that the relationship between S1 and S2 is one of non-logical identity.",
    "Something, of course, we practice—the narrative practice hypothesis—by engaging in narrative all the time and making ourselves into temporally extended selves that have a non-logical identity through time and through development.",
    "So I think both the romantic expressionism and the empiricist, you know, writing upon the blank slate, do not capture what's happening between S1 and [S]2.",
    "It's not that S1 is just changed randomly into S2 from the outside.",
    "Neither is it the case that S1 simply makes S2.",
    "The first self does not make it, right?",
    "It's not, it's neither pure passivity nor pure activity.",
    "This, of course, is why I've continually emphasized the notion of participation.",
    "We'll see how Barfield is trying also to step above both, you know, making and active, completely active making and completely passive reception in his notion of participation.",
    "A better way of describing the relationship is S1 does not receive nor make S2, but participates in S2's emergence.",
    "S2 emerges out of S1 to the point that S1 disappears into S2.",
    "It's an emergence.",
    "We participate in an emergence.",
    "So aspiration is Callard's name for that process by which S1 participates in the emergence of S2 out of S1 such that S1 has disappeared into S2.",
    "Self one has disappeared into, has become S2.",
    "get the notes!",
    "Reformulating The Problem Between S1 And S2 So Callard now reformulates the problem that remains.",
    "Once we acknowledge this (indicates Fig. 1b), there is a problem that remains because it, again, thwarts our usual cognitive cultural grammar.",
    "What's the problem that remains?",
    "Well, here's the problem.",
    "S1, in some important sense, causes S2 (Fig. 1c) (writes Causes between S1 and S2).",
    "My actions now are necessary and perhaps, in some important sense, sufficient for setting forth a course of development that is going to result in S2.",
    "But although S1 is therefore temporally prior (writes Temporally above Causes)",
    "it's before S2 in the arrow of causation (draws an Arrow from S1 to S2), the opposite (draws an arrow from S2 to S1) is the case normatively.",
    "Fig.",
    "1c S1 normatively depends on S2.",
    "All of S1's actions only make sense, can only be justified once S2 comes into existence.",
    "Because only S2 appreciates the music.",
    "Only S2 is rational.",
    "Only S2 understands and justifies the value of rationality, the value of the classical music.",
    "So although S1 causes S2 temporally prior, S1 is normatively dependent (Fig. 1d)",
    "(writes Normative dependent under the arrow from S2 and S1) on S2.",
    "In terms of normativity, S1 is not primary, it's secondary to S2.",
    "The first self—all, everything that the first self is doing ultimately only makes sense when the second self has come into existence.",
    "It's only after the aspirational transformation that S1's behavior can be made sense of, can be justified, can be understood.",
    "Fig.",
    "1d",
    "It's interesting because the state that justifies S1's action is the state of S1 having disappeared into and through the emergence of S2.",
    "Because only S2 understands and appreciates rationality.",
    "Understands and appreciates classical music.",
    "Understands and appreciates what it is to be a parent.",
    "Understands and appreciates what it is to be a spouse.",
    "So this goes against our normal way of doing things, right?",
    "Because we've got—this (indicates S1) is temporally prior, but this (indicates S2) is normatively primary.",
    "So this is—S1 is temporally prior (Fig. 1e)",
    "(draws an arrow from Temporally prior to S1), but S2 is normatively primary (writes Normatively primary above S2) in that it's where we find the justification, explanation, legitimation of the aspirational process, that the person that has become in S2.",
    "And that's weird for us because normally the thing that is temporally prior and causes is also the thing that is the source of justification and explanation.",
    "Fig.",
    "1e",
    "Now, the temptation here, of course, is to be teleological.",
    "To think that in some sense S2 preexists us and causes S1.",
    "And I think that's partially what's coming out in the mythos of the divine double.",
    "Trying to deal with this really difficult way of thinking.",
    "An easy way of thinking about it is, well, the divine double preexists, so already there, fully formed and they're drawing me out teleological until I eventually become S1, right?",
    "But we've already, I've already argued last time, and the time before and earlier on in the series that the teleological explanations are often thwarting us in important ways.",
    "And they are certainly thwarting what Heidegger was talking about.",
    "So let's try and do this a little bit more slowly.",
    "I want to say S1 has the causal power, but S2 has the normative authority.",
    "So S1 has the causal power, but S2 has the normative authority.",
    "So how do we relate to the self, to which we aspire?",
    "So when I am S1 and I'm aspiring to be more like Socrates, more rational, how do I now relate to this S2 that doesn't yet exist, but has authority over me?",
    "How do I do that?",
    "Well, I sort of slipped it in there, right?",
    "I sort of slipped it in there when I talked about aspiring to be like Socrates.",
    "The Aspired-To Self",
    "So let's take this step by step.",
    "I need...",
    "I'm relating to this, the aspired fore self, the self that I aspire to.",
    "There's a non-logical identity between my self now and that self then.",
    "That self that I'm aspiring to is not logically accessible to me.",
    "And those two reasons are deeply—those two points are deeply connected.",
    "I can't infer my way to it, right?",
    "And my representation of that future self, my current representation to me now has to afford me, somehow tapping into this non-logical identity, this non-logical process, and that representation has to actually afford the transformation of me into the aspired-to self.",
    "It has to actually help me become a more rational person.",
    "Now notice, of course, what this means.",
    "What kind of thing does this for me?",
    "And this is Corbin's point.",
    "It's a symbol, not in the imaginary sense, but in the imaginal sense.",
    "It's only a symbol that puts these two (indicates S1 and S2) together in the right way.",
    "It's a kind of relationship that between things that are non-logically identical, it is not something that is processed in a purely logical fashion.",
    "It is a representation that is participatory and it's supposed to help to actually afford you going through the transformative process.",
    "Now let's add a little bit more.",
    "My representation of the aspired-to self is it's a symbolic self.",
    "It's a symbolic self that I can internalize into my current self anagogically.",
    "Remember we talked about this?",
    "We become, we transcend ourselves by internalizing how other people's perspectives are being directed on us, right?",
    "So remember Spencer internalizes my perspective so that it becomes metacognitive, the stoic aspirant internalizes Socrates, so that he can self-transcend and become more Socratic.",
    "So the symbolic self has to be internalized and notice what's happening in internalization.",
    "Internalization is something other than you, yet it becomes something that is completely identified as you, not just as an idea, right?",
    "It becomes part of your metacognitive reflective rationality in the case of internalizing Socrates.",
    "It becomes part of the very guts of the machinery of yourself.",
    "Why anagogically?",
    "Because what I'm doing is I'm internalizing this symbolic self.",
    "And what it's doing is it's reordering my psyche so that I see different ways of being in the world.",
    "And as I inhabit those new ways of being in the world, they allow me to then re-internalize.",
    "Remember this?",
    "I internalize Socrates and then I indwell the world in a more Socratic fashion, which allows me to better internalize Socrates so that I indwell the world in a more Socratic fashion.",
    "Or perhaps for the Christian.",
    "Christ comes to live within them until they live more Christ-like, so that Christ comes to live within them more.",
    "So there's more internalization, more indwelling.",
    "And that anagogic process takes off of its own accord, but it's not something that's just passively happening to you.",
    "That coupled loop.",
    "It's not something you're just making happen.",
    "It's something that transcends receiving and making.",
    "It is participating.",
    "So what we're doing is that you have this symbolic self that internalizes other people's perspectives, others who live a way they make viable to you.",
    "The self you aspire to.",
    "But as you internalize them, and that self is transformed, the world is anagogically transformed also.",
    "The world is playing an important role in this.",
    "So what I'm suggesting to you is the divine double is a mythos way of trying to capture this dynamic process, which we've discussed at length in this series.",
    "And what it does is it represents this process in kind of a linear narrative, and therefore it simplifies it into a simple kind of teleology.",
    "But there's a sense in which I think that teleology is overly simplistic.",
    "It's not capturing the participatory nature.",
    "The danger with the teleology, of course, is it tends to overemphasize the passive receptivity on the part of S1 in the face of S2.",
    "get the notes!",
    "The Divine Double So the divine double.",
    "I think what people were trying to say with the mythos of the divine double.",
    "It's an imaginal symbol that affords the dynamic coupling of anagoge that allows you to participate in the act of self creation.",
    "The act, or a better way of putting it, the act of aspiration.",
    "The divine double is you, but it's not you.",
    "It's the advanced others that you've internalized into you, but eventually become you.",
    "And so you live differently in that in a new world.",
    "A way of being becomes viable to you.",
    "It is the self you will be.",
    "Not the self you are now.",
    "But if there is no inkling in your current self of, if there's no inkling of an identity possible, and already beginning to be actualized between your current self and future self, then, of course, it's not going to be part of that aspirational process.",
    "Here's you (Fig. 3a) (draws a square).",
    "You're in this frame and you're trying to move to this one (draws a larger square above the first).",
    "I'm going to separate them just so I have room to write.",
    "Normally this one (indicates the larger square) is round and encompassing.",
    "So please allow me this",
    "just so I have room to write, right?",
    "The divine double allows you to internalize from this more encompassing frame into your current frame (draws an arrow from the large square to the smaller square and writes DD) but that is simultaneously—and here's the shining in (indicates the arrow).",
    "Here's the shining in through the divine double.",
    "Angels are glorious.",
    "They shine.",
    "Here's the shining through into your frame.",
    "But that shining that internalization affords you moving towards indwelling that more expanded world (draws an arrow from the smaller square to the larger).",
    "It engenders a transframing (writes Transframing) so that you can come to indwell, this more expanded frame.",
    "Fig.",
    "3a",
    "The agent and the arena are simultaneously transformed.",
    "Here's (indicates the larger square)—so the divine double shines the greater frame into the current frame, but it also draws you out by the way it withdraws into the more encompassing frame.",
    "It gives you a sense of the closing into your relevance, but the opening into the greater self.",
    "See the gnosis?",
    "The divine double allows you to conform—conform in process to the very play of being itself.",
    "The way being is shining but also withdrawing.",
    "And how that affords your radical self-transcendence, which is always a process also of becoming a greater or better self (Fig. 3b) (writes DD along the arrow from the smaller square to the larger).",
    "So what I'm suggesting to you, right, is that the divine double is a central example of the imaginal and that that is often represented in the mythos of angels.",
    "Fig.",
    "3b",
    "So we see how the divine double is transjective, how it's transframing, how it's integrating the abstract form or a concept of the better self.",
    "I have some abstract sense of the better self.",
    "But it's integrating that with the concrete actions, of causal actions of my current self.",
    "They're being—the abstract and the concrete are being drawn together.",
    "Is the divine double subjective?",
    "No, that's not right.",
    "Is it part of just the objective part of my world?",
    "No, that's not right either.",
    "It's deeply symbolic in nature and in action.",
    "And although it is a symbol, it is not just imaginary.",
    "It is imaginal in nature.",
    "It makes, it affords the true development.",
    "It affords the core of the being mode.",
    "The being mode is not about having things.",
    "It is about becoming someone.",
    "There's a deep interconnection between the imaginal, the divine double, gnosis, and the being mode.",
    "So the angel in Corbin is a representation of the divine double.",
    "And now, the thing to note is that for Corbin, everything has an angel, right?",
    "Because it's not only the agent that is being transformed.",
    "It is also the arena.",
    "Your world is also being opened up.",
    "And aspects of being are disclosing themselves that otherwise would not disclose themselves.",
    "Every object is shining and it's also withdrawing into its mystery.",
    "Everything is a thing beyond itself.",
    "And so you are a thing beyond yourself as an agent, coupled to sets of things beyond themselves as an arena.",
    "And you are both going through this coupled process.",
    "That's what Corbin means by the angelic aspect of the angelic order of being.",
    "Now, given the way I've tried to interpret and I think explained, but not, I hope dismissively, explained away Corbin—I wouldn't want to make—I want to note, as I said, there's deep connections between gnosis and this divine double, between the being mode, between self-transcendence, between all of this.",
    "I'm a little bit unhappy with Stang's term though, the divine double, because it seems to bind us a little too much to the mythos and the teleological simple narrative structure that I think doesn't adequately capture everything that we can see in the work of LA Paul and Callard and the response to Strawson's problem.",
    "And also the notion of divine seems to bind this to theism, which is problematic given its deep connections to gnosis and the Gnostics.",
    "And also it precludes non-theistic cultures or sets of religions from having something like this.",
    "Whereas I think you can readily see the divine double in Buddhism where it's talked about the Buddha nature.",
    "And the Buddha nature is very much the aspired-self, but things have a Buddha nature.",
    "The Buddha nature is both their ultimate real nature, but not their conventional nature.",
    "Or you can say the same thing in Vedanta when there is a deep identity perhaps between the Atman and Brahman.",
    "What I'm pointing out is that this way of talking about aspiration can be seen clearly in non-theistic religions.",
    "It's clearly in gnosticism, which I think is very much, should not be interpreted theistically.",
    "I've tried to show you that.",
    "It's clearly the case in Neoplatonism and Stang makes this case for it, both in Plotinus and aspects that at least the Neoplatonic aspects of Dionysus.",
    "And that's clearly not theistic.",
    "So I am not going to use the term divine double anymore because I want to try and separate this idea from its commitment to theism.",
    "And so I'm going to call this symbolic self, I'm going to call it the sacred second self.",
    "The sacred second self.",
    "It gives me even more alliteration than the divine double, so I win.",
    "So the idea of the sacred second self.",
    "Perhaps this is a way—wow, I don't know.",
    "I don't know how, what I'm going to do right now, but I'm going to do it because I have an inkling of its value.",
    "Perhaps the notion of the sacred second self is a way of bringing back the idea of having a soul.",
    "In fact, that's even the wrong way of putting it.",
    "Perhaps that's part of what I'm trying to transgress against.",
    "Your sacred second self is the soul that you are becoming.",
    "The soul that you are aspiring through and to, and perhaps that is a way of bringing it back.",
    "Carl Gustav Jung",
    "The reason I raise this is because that will allow us to make a bridge to another one of the prophets.",
    "Carl Gustav Jung.",
    "Because this notion of a relationship to a sacred second self, that is perhaps what we were always talking about when we invoked the word soul, is central to Jung's work.",
    "One of Jung's crucial text for representing the meaning crisis and linking it to his particular particular psychology is the book, Modern Man In Search Of A Soul.",
    "So the response to the meaning crisis is that modern man has lost his soul.",
    "Now that doesn't mean that a ghost has slipped free of a person's corpus and is somehow floating around untethered.",
    "Jung is trying to talk about the—I'm going to argue—the loss of a real relationship to the sacred second self that is needed for responding to the meaning crisis.",
    "And there are deep connections, therefore between Jung and Corbin.",
    "And this is not just similarity of argumentation.",
    "Jung and Corbin had deep, had a deep interaction, a deep influence on each other.",
    "They met regularly together at the Eranos conferences and discussed.",
    "As I mentioned, I find that Corbin is more responsible to that relationship than Jung.",
    "Corbin talks more often about it explicitly.",
    "Whereas I do not see Jung giving enough credit to the influence of Corbin on his thinking.",
    "Nevertheless we can move between Corbin and Jung by picking up on this idea of your relationship to your sacred second self.",
    "And I think this is the best way to understanding the process that is central to Jung's whole notion of—it's both a notion of development and a notion of self-transformation and a notion of how to fundamentally respond to the meaning crisis.",
    "This is Jung's notion, of course, of individuation.",
    "So how do we get to this notion?",
    "Well, we're going to get to this notion—notice each thinker gets into it in a different way.",
    "And what Jung is doing, he's picking up on something that is not, it's not really present in Heidegger.",
    "It's present in Corbin, but it's present more implicitly than explicitly.",
    "And this is psychology, right?",
    "That the processes within the psyche that are conducive to responding to the meaning crisis.",
    "And by individuation, Jung, and he clearly uses this adjective to describe this as a psychological process.",
    "Now the way to get a little bit clearer about how Jung is using the notion of psychological is right to contrast him to the most important influence on him, his progenitor Freud.",
    "And I'm not going to get into a deep analysis of Freud.",
    "That would be too far afield.",
    "Freud is a Titan.",
    "Even if 95% of what Freud has said is wrong, it doesn't matter.",
    "He gets to be in the hall of the immortals because he came up with the idea of the unconscious.",
    "He comes up with the idea of it's neither nature or nurture, but the interaction between them in stages of development.",
    "These are all just, they become so deeply interwoven with our fundamental way of trying to understand and theorize about ourselves.",
    "Like I said.",
    "So Freud is a Titanic figure.",
    "However let's pick up on the difference.",
    "In what fundamental way did Jung's model of the psyche differed from Freud's?",
    "So here, I'm picking up on work done by Paul Ricoeur in his book on Freud and some work done by Storr.",
    "Anthony Storr in his work on Jung in an important contrast.",
    "So Freud ultimately has what has been called a hydraulic model of the psyche.",
    "So the psyche is basically a Newtonian machine, like a steam engine.",
    "Things are under pressure and the pressure has to be relieved and it drives and sort of pushes various processes into operation.",
    "So Freud and, of course, this makes perfect sense.",
    "Freud has a Newtonian machine, hydraulic model of the psyche.",
    "Jung ultimately rejects that, and this is more in Storr than in Ricoeur because Ricoeur's primarily concentrating on Freud.",
    "But what Storr argues is that—and this becomes clear in the language and the metaphors that Jung used.",
    "Jung replaces that hydraulic metaphor with an organic metaphor.",
    "He sees the psyche as a self-organizing, dynamical system, ultimately as an autopoietic being.",
    "So he's sees the psyche as going through a sort of a complex process of self-organization.",
    "And that you have to understand individuation as this kind of organic self-organizing—organic self-organizing process that you neither make nor receive, but you participate in.",
    "get the notes!",
    "Archetypes",
    "So this takes us to one of the quintessential notions from Jung.",
    "Jung gives a psychological analog of Plato's idea of the form, a structural functional organization.",
    "This is the archetypes, the archetypos.",
    "People should go back.",
    "'Arche' foundational, like in archeology, getting to the origins and the foundation.",
    "'Typos' the patterns.",
    "So the archetypes are the formative founding patterns of the psyche.",
    "These are the structural functional organizations by which the psyche self-organizes.",
    "The archetypes are therefore very much psychological versions of the Platonic form.",
    "And Jung is much better at acknowledging Plato's influence than Freud is, for example.",
    "So the archetypes are not images.",
    "The archetypes are not images.",
    "You have to take the images and treat them in an imaginal fashion, not as imaginary things you possess in your mind, but as imaginal things that are leading you into the aspirational process of individuation.",
    "Think of the archetypes more, the way we talked about earlier.",
    "They are systems of constraints.",
    "They are virtual engines that regulate the self-organization of what is salient to us.",
    "So if the hero archetype is active in me, it's not—it doesn't mean that I'm carrying around in my head images of the hero.",
    "It means that this is an imaginal relationship in which my salience landscaping is being transformed.",
    "So I'm anagogically interacting with the world and undergoing aspirational self-transformation so that I am becoming more and more heroic.",
    "Think of the archetypes much more adverbally.",
    "than adjectivally.",
    "And archetype is a way in which you are anagogically coming to be.",
    "Not something in you that you possess and reflect upon.",
    "So Jung argues that all of these, like the psyche as a whole, these archetypes insofar as they are virtual engines of self-organizing processes are autopoietic.",
    "They have a life to them, a life to them.",
    "These archetypes are the way.",
    "Hear this word deeply.",
    "Way as method and path of development.",
    "The archetypes are the way that psyche makes itself as a living organism.",
    "That's what I mean, think of archetypes in a deeply adverbial fashion rather than archetypal—sorry, adjectival.",
    "[-] The Sacred Second Self So where's the sacred second self?",
    "Well, let's talk about the ego (Fig. 4a) (writes Ego and draws a vertical line below) and what Jung called the Self (writes Self below Ego).",
    "And he's influenced right by Vedanta.",
    "This is the egoic self (indicates Ego), and this is Atman (indicates Self).",
    "And the notion of itself with the—see, that was such a bad choice in some ways, because unless you've done all this stuff we've just done and talked about the relation between S self one and self two and you don't—unless you've got the aspirational sense of what self is—if you come to Jung with just decadent romanticism, you're going to hear, oh, but this (indicates Self) is my inner true self that I have to be true to.",
    "Fig.",
    "4a",
    "You're going to relate to the self adjectivally from the having mode.",
    "Very great temptation to get into narcissism.",
    "I understand why Jung did this (indicates Self) because he capitalizes the S because he's trying to point towards, I would argue the sacred second self, right?",
    "So the ego is the archetype of the conscious mind.",
    "The ego is the virtual engine that regulates (Fig. 4b) (draws a circular arrow from the Ego to the Ego) the self organization of the conscious mind.",
    "What's the self?",
    "Well, it's kind of the archetype of the archetypes.",
    "It's like Plato's notion of the good, which is the form for how to be a form.",
    "The eidos of the eidos.",
    "It is the virtual engine (draws a circular arrow from the Self to the Self) regulating the self organization of the psyche as a whole.",
    "It is the principle—the self is the principle of autopoiesis itself.",
    "It's the ultimate virtual engine that constellates all the other virtual engines so that the psyche can continue its process of autopoietic self-organization.",
    "Fig.",
    "4b Remember when a system is self-organizing its function and its development are completely merged.",
    "It develops by functions and it functions by developing.",
    "So this (indicates Fig.",
    "4b) functional model is simultaneously a developmental model.",
    "That's what makes it aspirational.",
    "It is simultaneously functional and developmental, right?",
    "So one of the things you can do is you can set up an interaction with these imaginal symbolic entities, the archetypes, and that interaction can be internalized into the perspective—so I can interact with the hero archetype or the shadow archetype, and that will actually be internalized into the way the ego self-organizes.",
    "Ultimately that can become part of this (indicates the vertical arrow), the dialogue between the ego and the self.",
    "What Jung calls, the axis mundi, the axis of the world, very maybe overwrought way of putting it.",
    "But in some ways I understand what he's trying to get at.",
    "This is the process, as I dialogue through the archetypes with the self, the ego's perspectival knowing, and it's participatory being is being fundamentally altered.",
    "This is the individuation of the ego.",
    "The ego individuates through its dialogue—notice that anagogic resonant way of talking—it's dialogue with the sacred second self.",
    "And notice ultimately how that falls back to Plato and Socrates.",
    "This notion of dialogue.",
    "This, of course, is the basis of Jung—and notice the similarity here, again—of Jung's deep criticism of literalism and fundamentalism.",
    "Because, of course, the imaginal (Fig. 4c) (draws a horizontal line across the vertical), the archetype as imaginal sits right here.",
    "It mediates between these.",
    "Fig.",
    "4c Why is Jung so critical of literalism and fundamentalism?",
    "Because it is to reduce the imaginal nature of the archetypes into simply being imaginary.",
    "It is to lose the being mode and it is the simply having of subjective representations, rather than engaging in the process of individuation.",
    "It's a form of inflation in which the ego pretends that it is sufficient unto itself and tries to take on the complete role of the self, tries to just have an identity rather than continually becoming in the process of individuation.",
    "It is deeply disturbing to see someone who is, would claim to be committed to a Jungian approach being deeply enmeshed or involved with proponents of literalism or fundamentalism.",
    "This would be, I believe, a deep form of self-contradiction.",
    "get the notes!",
    "Criticism Of Jung",
    "What's my main criticism of Jung?",
    "Which will then allow me a counter criticism to Corbin.",
    "And this is a criticism that Corbin makes of Jung, but it's also independently a criticism that Buber the existentialist, the person who talked about the I-It and I-Thou and picked up on the difference between the being mode and the having mode as well.",
    "There's also convergence with the criticism that Buber made of Jung.",
    "(Text overlay appears \"For a good discussion of the Buber/Jung debate\" beside book, The Search For Roots) Jung understands all of this (indicates Fig. 4c), and that's how I've explained it to you as intra-psychically happening within the psyche.",
    "Now my friend and colleague, Anderson Todd tells me that towards the end, Jung seems to be breaking out of this purely psychological way of talking.",
    "But for most of his writing, Jung understands all of this—and this is, of course, this is problematic.",
    "And this is what Corbin was trying to get him to see.",
    "He was understanding all of this as subjectively.",
    "His Kantianism was making him see this as all happening in a very deep sense within the mind.",
    "That the archetypes are understood ultimately for a very long time in Jung as subjectively, rather than transjectively.",
    "And because of this—and then this is where Buber's criticism bites into Jung—Jung misses all of the existential modes that Buber wants to talk about.",
    "Jung can't talk about the, you know, the having and the being modes, because he doesn't have a way of representing the transjective relationship.",
    "For Corbin, Jung seems to be reducing the imaginal to the imaginary.",
    "And for Corbin, this is a mistake because the mystical, for Corbin, doesn't just disclose the depths of the psyche.",
    "The mystical also discloses the depths of the world in an integrated, coordinated fashion.",
    "That's because Corbin is ultimate Neoplatonic and not Kantian.",
    "This is why I said, if you don't understand Kant, you don't get Jung.",
    "Now, in fairness to Jung, Jung can say, but what's missing from Corbin is the psychology.",
    "What's missing from Heidegger is a psychology.",
    "How does all of this existential, ontological, Neoplatonic stuff play out within the psyche?",
    "If you're going to talk to me about internalizing, I get it.",
    "I'm answering on behalf of Jung.",
    "Jung can say, I get it.",
    "I leave off the indwelling in the world that Corbin is pointing to, and Heidegger has been pointing to.",
    "But what Jung can say is, yeah, but you haven't told me what the internalization looks like.",
    "How does the imaginal get internalized into the depths of my psyche?",
    "So what I'm suggesting to you—this is neither Corbin nor Buber, nor is it Jung, but Vervaeke is arguing to you that you can integrate the three of them together.",
    "And then you get something much better than either Jung or Corbin or Buber.",
    "I want to take a look next time at somebody who shares a lot with all three of these: Corbin, Jung, and Buber.",
    "And like them, is deeply influenced by Heidegger.",
    "And that's Paul Tillich.",
    "Thank you very much for your time and attention.",
    "- END - Episode 49 Notes To keep this site running, we are an Amazon Associate where we earn from qualifying purchases Heidegger Martin Heidegger was a German philosopher who is widely regarded as one of the most important philosophers of the 20th century.",
    "He is best known for contributions to phenomenology, hermeneutics, and existentialism.",
    "Henry Corbin Henry Corbin was a philosopher, theologian, Iranologist and professor of Islamic Studies at the École pratique des hautes études in Paris, France.",
    "Gnosis Gnosis is the common Greek noun for knowledge.",
    "The term is used in various Hellenistic religions and philosophies.",
    "It is best known from Gnosticism, where it signifies a spiritual knowledge or insight into humanity's real nature as divine, leading to the deliverance of the divine spark within humanity from the constraints of earthly existence.",
    "Active imagination Active imagination is a conscious method of experimentation.",
    "It employs creative imagination as an organ for \"perceiving outside your own mental boxes.\"",
    "Dasein Dasein is a German word that means \"being there\" or \"presence\", and is often translated into English with the word \"existence\".",
    "Charles M. Stang Jean-Jacques Rousseau Jean-Jacques Rousseau was a Genevan philosopher, writer, and composer.",
    "His political philosophy influenced the progress of the Enlightenment throughout Europe, as well as aspects of the French Revolution and the development of modern political, economic, and educational thought L.A. Paul Laurie Ann Paul is a professor of philosophy and cognitive science at Yale University.",
    "She previously taught at the University of North Carolina at Chapel Hill and the University of Arizona.",
    "She is best known for her research on the counterfactual analysis of causation and the concept of “transformative experience.”",
    "Agnes Callard Agnes Callard is associate professor of philosophy at the University of Chicago.",
    "Her primary areas of specialization are ancient philosophy and ethics.",
    "Book Mentioned: Aspiration: The Agency of Becoming – Buy Here Galen Strawson Galen John Strawson is a British analytic philosopher and literary critic who works primarily on philosophy of mind, metaphysics (including free will, panpsychism, the mind-body problem, and the self), John Locke, David Hume, Immanuel Kant and Friedrich Nietzsche.",
    "Søren Kierkegaard Søren Aabye Kierkegaard was a Danish philosopher, theologian, poet, social critic, and religious author who is widely considered to be the first existentialist philosopher.",
    "Advaita Vedanta Advaita Vedānta is a school of Hindu philosophy, and is a classic system of spiritual realization in Indian tradition.",
    "Ātman Ātman is a Sanskrit word that means inner self, spirit, or soul.",
    "In Hindu philosophy, especially in the Vedanta school of Hinduism, Ātman is the first principle: the true self of an individual beyond identification with phenomena, the essence of an individual.",
    "In order to attain Moksha (liberation), a human being must acquire self-knowledge.",
    "Brahman Brahman connotes the highest Universal Principle, the Ultimate Reality in the universe.",
    "In major schools of Hindu philosophy, it is the material, efficient, formal and final cause of all that exists.",
    "Plotinus Plotinus was a major Hellenistic philosopher who lived in Roman Egypt.",
    "In his philosophy, described in the Enneads, there are three principles: the One, the Intellect, and the Soul.",
    "Carl Jung Carl Gustav Jung, was a Swiss psychiatrist and psychoanalyst who founded analytical psychology.",
    "Jung's work has been influential in the fields of psychiatry, anthropology, archaeology, literature, philosophy, psychology and religious studies.",
    "Book Mentioned: Modern Man in Search of a Soul - Buy Here Eranos Eranos is an intellectual discussion group dedicated to humanistic and religious studies, as well as to the natural sciences which has met annually in Moscia (Lago Maggiore), the Collegio Papio and on the Monte Verità in Ascona, Switzerland since 1933.",
    "Sigmund Freud Sigmund Freud was an Austrian neurologist and the founder of psychoanalysis, a clinical method for treating psychopathology through dialogue between a patient and a psychoanalyst.",
    "Paul Ricœur Jean Paul Gustave Ricœur was a French philosopher best known for combining phenomenological description with hermeneutics.",
    "Book Mentioned: Freud and Philosophy - Buy",
    "Here Anthony Storr Anthony Storr was an English psychiatrist, psychoanalyst and author.",
    "Book Mentioned: Jung - Buy",
    "Here Jungian archetypes Jungian archetypes are defined as universal, primal symbols and images that derive from the collective unconscious, as proposed by Carl Jung.",
    "They are the psychic counterpart of instinct.",
    "Martin Buber Martin Buber was an Austrian Jewish and Israeli philosopher best known for his philosophy of dialogue, a form of existentialism centered on the distinction between the I–Thou relationship and the I–It relationship.",
    "Alfred Ribi Book Mentioned: The Search for Roots - Buy",
    "Here Other helpful resources about this episode: Notes on Bevry Additional Notes on Bevry Ep.",
    "48 - Awakening from the Meaning Crisis - Corbin and the Divine Double Welcome back to Awakening from the Meaning Crisis.",
    "So last time we were pursuing in-depth, trying to understand Heidegger's work as a prophet in the Old Testament sense of the meaning crisis.",
    "We took a look at this notion of the thing beyond itself and realness as simultaneously the shining into our framing and the withdrawing beyond our framing in a deeply inter-affording inter-penetrating manner.",
    "We took a look at this deeper notion of truth.",
    "Not truth as correctness, but truth as aletheia.",
    "That which grounds the agent arena relationship in attunement and allows us the potential to remember being by getting into an attunement with it's simultaneous disclosure and withdrawal.",
    "But we can forget that; we can get into a profound kind of modal confusion.",
    "And this is the history of metaphysics as the emergence of nihilism, we can forget the being mode.",
    "We can get trapped into the having mode in which the metaphysics is a propositional project of trying to just use truth as correctness.",
    "And we misunderstand Being as a particular being, we try to capture the unlimitedness aspect of being.",
    "But we only do it at the limit, which Heidegger's deeply critical of.",
    "And so we understand being in terms of a supreme being.",
    "A being at the limit and beyond the limit, this is onto-theology.",
    "We understand God as the supreme being.",
    "And this is deeply enmeshed for Heidegger with nihilism because this onto-theology, this theological way—at least a version of theology from, sort of, classical traditional theism—this way of understanding being gets us into the deep forgetfulness and modal confusion that is the hallmark of nihilism.",
    "But, of course, we could perhaps remember the being mode.",
    "And this is what Corbin, following Heidegger, talks about as gnosis.",
    "We can understand what this gnosis is.",
    "What does it look like?",
    "What would it be like to remember, from the being mode through aletheia, being?",
    "So I want to pick up on this idea of gnosis's serious play in a particular piece of work by Heidegger.",
    "Heidegger discusses, and this Avens discusses this in his book.",
    "And Caputo also discusses this in his excellent book, The Mystical Element In Heidegger's Thought.",
    "So both Avens and Caputo talk about this.",
    "Heidegger's commentary on the poetry of Angelus Silesius.",
    "Angelus Silesius was a poet who was basically trying to put into poetry—and we can, sort of, think of, or at least foresee some things Barfield is going to say.",
    "He's trying to put into poetry the work of Meister Eckhart, who was one of the great Neo-platonic mystics within the Rhineland mystics that I talked about so long ago.",
    "Now what's important of course for Meister Eckhart and this discussion of gnosis as the remembering through aletheia of the being mode that alleviates the forgetfulness, alleviates nihilism, is that Eckhart, of course is also experiencing this as a form of sacredness.",
    "As something that is appropriate to a religious context.",
    "So we also noted in conjunction with that, that Tillich is going to be deeply influenced by Heidegger's critique of ontotheology, but he is also going to situate it within—although he's going to radically revise what this means—a traditional religious term, which is idolatry.",
    "get the notes!",
    "The Rose And Physis Now let's think about Heidegger's commentary on this poem.",
    "So what's the poem?",
    "Here's the poem, what's in translation.",
    "So we, unfortunately, lose some of the poetry.",
    "The Rose is without why.",
    "It blooms because it blooms.",
    "It cares not for itself.",
    "Asks not if it is seen.",
    "So it's interesting that when Heidegger is doing this, he's actually talking about this word physis (writes Physis).",
    "The Greek word, which, of course, is the core of the word physics, which again, he's trying to get back to a re-experience of the physical as important way of remembering the being mode.",
    "This is again why I think many people misunderstand.",
    "And I've argued this elsewhere",
    "and I'm going to keep coming back to it.",
    "The response to the meaning crisis is somehow a rejection of physicalism and the physical.",
    "Heidegger's instead, trying to show you how it can more deeply be remembered.",
    "Now he's picking up on the Greek for this term (indicates Physis).",
    "So again, he's doing some etymological work here.",
    "Physis means, you know, blossoming forth from itself, springing forth from itself.",
    "Very much like the rose is being described.",
    "And think about what this means.",
    "This is what Heidegger says.",
    "The blossoming of the rose is grounded in itself.",
    "Has its ground in itself.",
    "The blossoming is a pure emerging out of itself.",
    "Pure shining.",
    "Now what's going on there?",
    "Now, of course, Heidegger will never talk just about the shining, even though he doesn't explicitly mention it here.",
    "It's implied and we should therefore remember it in the phrase, 'emerging out of itself,' that the shining is simultaneously withdrawing.",
    "We get a sense of the depth of the rose in its physis, because as it shines, it shines in a way that's showing that it's shining out of itself, shining out of its depth, shining out of that into which it withdraws as it presents itself to our phenomenological experience.",
    "So here Heidegger is—he's picking up on one of Eckhart's maxims.",
    "This is what Eckhart said, live without why.",
    "Or you could also translate it as live without a why.",
    "Now that sounds to some people like, what?",
    "That sounds like a meaningless existence.",
    "There's no why there's no purpose.",
    "There's no grand unifying purpose.",
    "Think about it a little bit more carefully, right?",
    "Is that quest for the grand culminating purpose?",
    "Is, is that maybe, perhaps, coming from the having mode and not from the being mode.",
    "Eckhart is not proposing meaninglessness.",
    "He's actually proposing a non-teleological way of being.",
    "A non-teleological way of being.",
    "It's to move beyond.",
    "There's no narrative to the rose.",
    "The rose is not that it's sort of lacking.",
    "It's beyond, above and beyond the narrative.",
    "So a way of thinking about this—and I promised to come back to this earlier in the series and I've come back to it now (draws an east pointing arrow).",
    "And we talked about this in a couple places.",
    "We talked about it back with the Stoics.",
    "And we talked about it when I was talking about, perhaps we can't get back to a narrative in the sense of a teleological aspect to physis, to the physical universe.",
    "But maybe the universe as a whole is like the rose.",
    "Maybe it's blooming from itself, grounded in itself, blossoming, shining from itself while always, always withdrawing.",
    "And think about how well that actually comports with, you know, the physics of an ever-expanding universe coming out of the Big Bang, but grounded in the quantum.",
    "Like, is that so foreign a way of talking about the universe, that it's very much like the rose?",
    "And we get better at being connected to its physis if we drop the axial age requirement that there be a teleological narrative to it all.",
    "Narrative And Non-Logical Identity This horizontal narrative.",
    "Look, it's important.",
    "The horiz—narrative gives us practice in something.",
    "It gives us important practice in something.",
    "We're going to come back to this because the thing about narrative is narrative gives you deep practice, cognitive existential practice in non-logical identity (Fig. 1a) (writes Narrative non-logical identity).",
    "We've talked about this and the relationship it has to the symbol.",
    "Fig.",
    "1a So let's talk about it, first of all, in the symbol.",
    "Remember, here's a framing (Fig. 2) (draws a square)",
    "and then you transframe (draws a larger square outside the small square) and then there's a non-logical identity between the world inside the frame and the world outside the frame (draws a double headed arrow between the two squares) and a non-logical identity between you here and you there (draws a double-headed arrow between the two squares).",
    "Remember this?",
    "This non-logical identity between who you are inside this frame and who you are after, right?",
    "A transframing.",
    "Remember that when we talked about aspiration?",
    "When we talked about aspiration, remember that?",
    "Fig.",
    "2",
    "Now the thing about narrative is that narrative is a way of representing through time (indicates the horizontal arrow)",
    "symbolically (Fig.",
    "1b)",
    "(writes Through below Narrative non-logical identity).",
    "We can often rep—oh, no, sometimes we're just talking about, a kind of transformation through time.",
    "But one of the things that narrative does, is through time, it represents how you have a non-logical identity to yourself.",
    "Fig.",
    "1b",
    "Look, I was born in Hamilton in 1961.",
    "I'm not in Hamilton now.",
    "I'm not, you know, nine pounds.",
    "I'm 190 pounds.",
    "That kid that was born in Hamilton, can't speak English.",
    "Couldn't walk, couldn't move around.",
    "Certainly couldn't teach this.",
    "That kid is in so many ways different non-identical from me, but in another sense, it's me.",
    "And I'm him, right?",
    "Narrative is a way of tracing out and training us in being able to work with non-logical identity, to work with this kind of fundamental transformation.",
    "But what we can do, what I think Eckhart is pointing to (erases the board except the horizontal arrow), we can exapt that ability for non-logical identity (Fig. 3) (writes Exapt).",
    "We can exapt that symbolic identity.",
    "And instead of thinking of it as unfolding narratively across time—remember how the Stoics criticized this?",
    "Stop pursuing fame and glory and wealth and power.",
    "Instead of the horizontal narrative, we can do the vertical ontology (draws a downward arrow across the horizontal arrow).",
    "We can do the vertical ontology in which we are connecting the depths of ourselves to the depths of being in a non-teleological being mode.",
    "This is, I think, is what.",
    "Heidegger is pointing to and what Eckhart is pointing to.",
    "Fig.",
    "3 Pure Shining And Pure Withdrawal",
    "So, as I mentioned, the pure shining.",
    "Let's put this on here (erases the board).",
    "Here's the pure shining (Fig. 4a) (writes Pure shining), the way the rose shines, phenomenon, experience.",
    "Right?",
    "I think that's shining.",
    "I think we can talk about it as relevance realization (writes RR below Pure shining), the salience landscaping into intelligibility (writes Salience landscaping into intelligibility).",
    "Salience landscaping into intelligibility.",
    "Fig.",
    "4a What about the pure withdrawal?",
    "(Fig. 4b) (writes Pure withdrawal beside Pure shining)",
    "This is the independent inexhaustibleness of a combinatorially explosive reality, right?",
    "Independent, because it is inexhaustible (writes Independent inexhaustible).",
    "We cannot drink it dry.",
    "The Tao Te Ching.",
    "Right?",
    "And the Dao is a way of understanding physis the way Heidegger's talking about.",
    "Look at the book, Heidegger and Asian Thought, or Heidegger's [Hidden] Sources, where it talks about the connection to Daoism and he might've been directly influenced by it.",
    "Right.",
    "And the Tao Te Ching talks about, you know, how the Dao is a well that is never used up.",
    "It is inexhaust—it is the inexhaustible mother.",
    "So the independent inexhaustibleness of a combinatorially explosive reality (writes of COE reality below Independent inexhaustibleness).",
    "The things—the thing, and the things beyond themselves (writes Things beyond themselves below Independent inexhaustibleness of COE reality).",
    "Fig.",
    "4b I think we can draw these two together (Fig. 4c) (draws a line between Salience landscaping and Independent inexhaustibleness), as I've already argued, into this.",
    "I want to say this very carefully, right?",
    "We can see this, we can experience this from within the being mode in the following way.",
    "A trajectory, a trajectory of transframing that is always closing upon the relevant, while always opening to the moreness.",
    "It's a trajectory of transframing that is always closing upon the relevant as it is simultaneously always opening to the moreness.",
    "Fig.",
    "4c When we recognize that aletheia, remember it from within the being mode so that we can accentuate it and celebrate it that's what I've argued sacredness is.",
    "And that seems to line up very well with what Eckhart is saying.",
    "And one of the things I have about Heidegger is he's reticent to talk about this in terms of sacredness.",
    "Tillich isn't.",
    "Heidegger is.",
    "And that's part of why I think he, he goes astray in certain ways.",
    "So we can think of realness as a tonos (writes Tonos).",
    "This creative tension.",
    "It's something that Barfield brings out tremendously and clearly in his work.",
    "We can think of realness as a tonos, as a creative polar tension between, you know—Look at the word confirmation, coherence, and moreness.",
    "And remember you need both.",
    "get the notes!",
    "If the virtual reality just has the confirmation and the coherence, it falls flat.",
    "If it can't provoke a sense of opening and wonder, if there's no element of surprise.",
    "If it's all assimilation and no accommodation.",
    "If it's all assimilation and no accommodation—remember accommodation is experienced as all in wonder?",
    "If it's all of that, if it's just assimilation—sorry, and not accommodation, if it's just the foreclosure and never also the opening.",
    "If it's just the homing and never the numinous.",
    "See these themes?",
    "Then it's not real.",
    "It's not experienced as real.",
    "And that being able to attune—and this is where the Dao was so, like Daoism is such a powerful symbolism, right?",
    "You have the yin, which is the confirming.",
    "Drawing down.",
    "And the yang is the opening up.",
    "And both of those interpenetrate.",
    "Think of the classic Dao symbol: within the white is the black dot, within the black is the white dot.",
    "And they're sinuous because they're interpenetrating, interleaved together.",
    "And all of that is the disclosure of the inexhaustibleness of the Dao.",
    "I'm trying to make a convergence argument here.",
    "Daoism is all about the serious play.",
    "The serious play with the serious play of being.",
    "And that's how Corbin describes it.",
    "When he's talking about gnosis he talks about the play of being.",
    "So does Avens when he's talking about Corbin.",
    "So I'd like to pass now explicitly leaving Heidegger behind now and moving into Corbin.",
    "Persian Sufism",
    "And Corbin I've already noted how deeply Corbin was influenced by Heidegger.",
    "But he's also, he's deeply influenced by Platonism.",
    "And that leads him into probably his deepest influences.",
    "So all of these things are important to Corbin: Heidegger, the Neoplatonic tradition, but most especially Neoplatonism within Persian Sufism.",
    "So Sufism (Fig. 5a) (writes Sufism) is the mystical branch of Islam and Corbin is particularly focused on Persian sufism (writes Persian above Sufism).",
    "And I think that's something important that we should just pause to note.",
    "One of the gifts of Corbin's work is to help us remember—and thereby overcome our ethnocentrism—how central—and I use that term decidedly—how central Persian philosophy is to the history of philosophy in the world.",
    "Fig.",
    "5a Persia plays a pivotal role.",
    "And I don't mean it as a neglected middle—well, it is by us, but it shouldn't be a neglected middle.",
    "Persia plays a central role between, for example, between the Arab world, the European world, and the world of India and China, the Asiatic world.",
    "And what's really important about Persian Sufism—and I can't, the history of Persia is a fraught one.",
    "We sort of think of now.",
    "And this is because of, you know, the history and since the seventies, we think of, you know, Iran is sort of rabidly Muslim and something like that.",
    "And that's played up by propaganda.",
    "It is not to deny that there is an Islamic fundamentalist totalitarian regime in control in Iran.",
    "But what it is—what I'm trying to do is challenge that as a monolithic representation of all of Persia and all of Persian culture.",
    "Instead, you have to remember that Persia was made Muslim via an Arab invasion that was nothing less than a genocide.",
    "And I know Persians.",
    "They remember this deeply to this day.",
    "So the attitude towards the Arab overlords is something that has become deeply woven into Persian culture.",
    "Why am I saying that?",
    "Because that means that the Persians were especially attracted to, at least for huge periods, Rumi and others, they are deeply attracted to Sufism.",
    "They're attracted to a mystical interpretation of Islam precisely because they are trying to find a form of liberation from an oppressive Arab empire.",
    "So that means that it's important that it is Persian Sufism, and this deeply has an impact on Corbin.",
    "He's really taken up by this and how that Persian Sufism has a much more—I'm trying not to be dismissive here—as a much more flexible relationship to Islam than you might think of when you think of Iran today in the world.",
    "And so, like, reading the poetry of ancient Persia—well, not even ancient Persia—the poetry from Persia since the Arab invasion and genocide, I think is an important thing to do too.",
    "We remember these aspects.",
    "Now Corbin did all of that.",
    "He read this stuff deeply.",
    "Profoundly, repeatedly, extensively.",
    "Now there's ways in which he suffers.",
    "There is ways in which as, you know, a French men, he will fundamentally misunderstand some of this literature.",
    "And I'm not going to say that he is a perfect interpreter, but I will say he is an insightful and important interpreter.",
    "So he draws all this in.",
    "He's drawing in the Heidegger and the Neoplatonism and espe—and this was something that you have to, and the Persian Sufist know this—the deep influence of Neoplatonism (Fig. 5b)",
    "(writes Neoplatonism above Sufism) on Sufism.",
    "Fig.",
    "5b",
    "So there's Neoplatonism then there is a mystical form of Islam, deeply influenced by Neoplatonism and then Corbin is bringing that into understanding Heidegger.",
    "And then he's bringing all of that as a way of trying to explain this gnosis and how this gnosis can ultimately be salvic and redemptive in the face of the meaning crisis.",
    "Remember, he talked about gnosis as transformative, salvic, participatory knowing?",
    "A deep at onement, attunement, at onement.",
    "See how all these things are resonating with each other?",
    "Now, what does Corbin bring to this that we don't have in Heidegger?",
    "And here's where I think you can see the influence of Sufism and the rich world of Persian poetry upon him.",
    "And I think this is an important thing.",
    "Corbin sees there, and argues for—reading Corbin is very different than reading—it's like Heidegger in the sense that it's difficult, but because he's trying again, to break out of the cognitive and cultural grammar, but it's very lyrical.",
    "It's very beautiful.",
    "But sometimes you pick up the beauty and that's again the influence of the Persian poetry on him.",
    "You pick up the beauty without, and then you, you, you should pause and say, yeah, but did I really understand what he just said?",
    "So you have to read, you almost have to recite Corbin and repeat Corbin.",
    "Now he uses that kind of argumentation to make a claim that the recovery of gnosis is bound up with imagination in an important way (writes Imagination).",
    "And you may think, \"oh no, John is just going to jump off into some decadent form of romanticism.\"",
    "No.",
    "Corbin is doing something very interesting about this.",
    "I recommend the Lachman book that I've recommended for Corbin.",
    "See also Aven's book, The New Gnosis that I've just mentioned.",
    "See all of Cheetham's work, The World Turned Inside Out, Imaginal Love, the third one I think it was the angelic nature of being, I can't remember the third title.",
    "Anyways, we'll put all the panels up.",
    "Cheetham's work on Corbin—in fact, I recommend reading Avens and reading Cheetham before you read Corbin.",
    "So if you take a look, Corbin is doing something very important with this.",
    "He's not using this word (indicates Imagination) in the way we typically use it.",
    "And in order to bring that out, he actually makes a distinction.",
    "A distinction that's going to be important, especially when we turn to talk about Jung.",
    "Distinction Between Imaginary And Imaginal He makes a distinction between the imaginary (Fig. 6) (writes Imaginary below Imagination) and what he calls the imaginal (writes Imaginal).",
    "And it's the imaginal that is bound up with gnosis (Writes Gnosis above Imaginal).",
    "Fig.",
    "6",
    "So imaginary.",
    "The imaginary is what we typically mean when we invoke the word imagination.",
    "We mean the purely subjective experience of generating inner mental imagery, which we know is not real and that it's sort of completely in our control and we can play with it as we wish.",
    "That is explicitly, clearly, definitively not what Corbin is talking about.",
    "Corbin is talking about the imaginal (erases the board).",
    "And to try and convey the imaginal, I'm going to try and schematically represent it to you.",
    "Because if you don't get the imaginal, you don't get what Corbin is talking about.",
    "I also would say you ultimately do not get what Jung is talking about when he's talking about active imagination, because you'll just misunderstand active imagination as a purely imaginary experience, as opposed to imaginal experience.",
    "And as I'll point out later, Corbin and Jung are deeply influential of each other.",
    "Corbin is much more open about that relationship.",
    "Corbin talks about and invokes Jung, often critically, but, at least, clearly and explicitly, and with credit.",
    "Way more often than I see Jung talking about Corbin, which I think is a criticism I have of Jung.",
    "Okay.",
    "So let's try and represent this.",
    "So, first of all, think of two ways (Fig.",
    "7a) (writes Abstract) in which you sort of try and represent, come into cognitive contact with reality.",
    "One is through abstract representations, abstract—the abstract intelligible world (writes intelligible world below Abstract).",
    "The world that you get through your intellect, right?",
    "So you know, you grasp reality as a mathematical formula or something like that, or, you know, you grasp reality as a purely formal entity.",
    "And then in contrast, there is, of course, the concrete and, of course, concrete and abstract are always relative terms, they're not absolute terms.",
    "The concrete sensible world (writes Concrete sensible world below Abstract intelligible world) at the bottom here.",
    "Fig.",
    "7a So one thing the imaginal does is it actually mediates between these two (Fig. 7b) (writes Mediates between Abstract intelligible world and Concrete sensible world).",
    "It bridges between them.",
    "It allows them to come together in meaningfully structured experience because in my phenomenological experience, of course, there's both an intelligible order that I can abstract intellectually, but that intelligible order also affects the way I come into sensual contact concretely with it.",
    "Fig.",
    "7b",
    "Okay.",
    "So the imaginal mediates between these (Fig. 7c) (erases Mediates and writes Imaginal).",
    "And one of Corbin's arguments is, and you see what he's doing here.",
    "He's arguing that the Cartesian cultural grammar that basically, you know, replicates the axial two worlds, but within us.",
    "So here's the world of mind, the abstract intelligible world, pure mind.",
    "And the concrete sensible is the world of pure matter.",
    "This is the Cartesian division.",
    "And what Corbin is arguing is.",
    "Yeah,",
    "but we've lost the imaginal that bridges between those two worlds between the mind and the material.",
    "Fig.",
    "7c",
    "Now, of course, Descartes split things in another way.",
    "And the imaginable mediates between those (Fig. 7d) (draws horizontal lines on both sides of Imaginal) and these, of course, they're not the same.",
    "That's why I represent them with different axes, but they are not independent.",
    "That's why I'm putting them together within the schema.",
    "The imaginal also bridges between the purely subjective (writes Subjective to the left of Imaginal) and the purely objective (Writes Objective to the right of Imaginal).",
    "Fig.",
    "7d",
    "So to use the term I've been trying to develop with you",
    "and we saw it all through Heidegger.",
    "The imaginal is deeply transjective in nature.",
    "So it mediates and it's transjective.",
    "And then what you have to do is you have to see this whole thing sort of in motion (Fig. 7e) (draws an arrow pointing to the right below), which I can't draw for you.",
    "Because the imaginal isn't a static relation.",
    "It's also a constant transformative transframing (writes Transformative transframing).",
    "There's a movement to the imaginal.",
    "It is vibrant and vital in that way.",
    "There's a movement to it.",
    "get the notes!",
    "Fig.",
    "7e",
    "So this is what Corbin means by the imaginal.",
    "It's a use of images, but not using them subjectively, using them transjectively.",
    "We'll try to come back to that.",
    "In a way that mediates, bridges, integrates the abstract intelligible world and the concrete sensible world together.",
    "But again, not just statically, but in this ongoing transjective—sorry, in this ongoing transformative transframing.",
    "So because of the centrality of the imaginal to Corbin, Corbin—and he explicitly understood himself as doing this and stated this, right?",
    "He was deeply opposed to fundamentalism.",
    "And here you can, of course, see the connection to the Persian history I was relating to you.",
    "He's deeply opposed to fundamentalism and literalism.",
    "Why?",
    "Because fundamentalism and literalism, right?",
    "First of all, they ratify this.",
    "They make it static.",
    "And they put things into either the abstract intelligible world or the concrete sensible world, or just in to subjectivity or just in to objectivity.",
    "They freeze this and then they fracture it and thereby they completely lose the nexus of the imaginal.",
    "And for Corbin, if you lose the imaginal, you lose the capacity for gnosis (Fig. 7f) (writes Gnosis above Imaginal).",
    "And then if you lose the capacity for gnosis, you lose the capacity for waking up within the being mode, through aletheia to being and the ground of being in sacredness.",
    "Fig.",
    "7f This is going to be something we're going to keep seeing.",
    "And again, something that Heidegger doesn't make explicit, but it's explicated in Corbin.",
    "The deep ongoing criticism to fundamentalism and literalism.",
    "It's a deep component of Jung as well.",
    "Jung seas, fundamentalism and literalism as the antithetical movement of thought and being in the world to everything he is trying to promote as a response to the meaning crisis.",
    "It is also deeply antagonistic to what Barfield is talking about when he's talking about poetic participation.",
    "So we're seeing again a potential way in which we can understand Heidegger's critique of ontotheology because there is a tendency—and all of these thinkers keep pointing to it.",
    "If we get into the having mode and we get into ontotheology and we have the supreme being, and we have our propositions about this ultimate being that we can think that the way in which we should be is to have these propositions in a fundamentalist literalist fashion, and we lose all of this.",
    "And what you'll hear is you'll hear the invocation of the symbolic as a dismissive term.",
    "Yeah.",
    "So that, but yes.",
    "Yes, yes, you can read this symbolically, but, you know, it's just symbolic, meaning it has no real relevance or importance to you.",
    "Corbin is trying to argue exactly the opposite.",
    "If you have an attitude towards the symbolic, as that is dismissive, then, of course, you have lost the capacity for gnosis, which means you have lost the capacity to remember the forget—like to overcome in aletheia the forgetfulness of being, to come out of the deepest kind of modal confusion.",
    "I see somebody as exemplifying this, although I don't think he's directly influenced by Corbin.",
    "I see Jonathan Pageau is trying to bring us back to this gnosis of the symbol.",
    "And how we should not be dismissive of it.",
    "We should not try and slide it into either a conceptual world or essential world.",
    "We should not interpret it as merely subjective or reject it because we can't make it clearly objective for us.",
    "Look at all the jecting going on.",
    "Subject.",
    "Reject.",
    "Okay, so now to try and bring out the imaginal in a way that connects to dasein, your being in the world, 'cause remember your self-knowledge and your participatory knowing of being are interpenetrating and knowing together.",
    "I have to bring out something of Corbin.",
    "That if you read it and you haven't done all of this work (indicates Fig. 7f), you, I guarantee, will misread it and misunderstand it.",
    "And it's a part that's difficult for me because it pushes my buttons in ways that I don't like, which is why I keep reading this stuff.",
    "Because I have a tremendous sense that it's pushing my buttons in a way that they need to be pushed so that I could perhaps wear free from them, at least to some degree (erases the board).",
    "Corbin’s Imaginal Understanding Of The Symbol And here's where Corbin is different from Heidegger, where he really does pick up on the sense of sacredness that is going on within the imaginal.",
    "Okay.",
    "Let's talk about how Corbin understands the symbol (writes Symbol), the imaginal understanding of the symbol, as opposed to the imaginary understanding—the dismissive understanding of the symbol.",
    "Okay.",
    "So what are these features?",
    "The feature that was brought out when Chris and I had that excellent discussion, the translucency of a symbol, you look at it, but you look through it in both meanings of the word by means of it and beyond it, like the way I'm looking through my glasses.",
    "The symbol is translucent.",
    "I can look at it.",
    "But I can also simultaneously look through it.",
    "And I can put those two into a important dialogue.",
    "Why do I want a dialogue between looking at it and looking through it?",
    "(Fig.",
    "8) (writes Looking at it and Looking through it)",
    "why is it so important to have those in dialogue with each other?",
    "Because that is how the symbol can help you to capture the non-logical identity between your agent arena now in this frame and the agent arena in a more comprehensive encompassing frame (erases the board).",
    "Fig. 8",
    "So symbols are translucent.",
    "As I've already argued, they're transjective .Trying to make them either subjective or objective is aligned with a having mode dismissal of how the symbol is trying to challenge you to transcendence.",
    "If you—look, if you are not transcending in response to a symbol, you really haven't understood a symbol.",
    "If you just treat it as an allegory that you can replace with other literal terms, then you haven't really remembered through the symbol.",
    "There has been no aletheia.",
    "In your pursuit of the correctness of truth, you have forgotten the aletheia of the symbol.",
    "This is why I'm so critical of people who are so dismissive of symbolism.",
    "The symbol is not only transjective, it's trajective.",
    "It's putting you on a trajectory of transformation as I was just articulating.",
    "The symbol is transformative.",
    "Remember the transformative of the inner man, it's transformative you at a fundamental level.",
    "And the symbol is ultimately transtemporal transpatial, because it has to do with this movement between worlds, which really isn't a narrative temporal spatial movement, it's an ontological movement between, you know, a smaller frame and a larger frame.",
    "I represent it with an arrow, but it isn't movement through space and time.",
    "It isn't a narrative change.",
    "It's an ontological shift.",
    "So I'm trying to pick all of these up.",
    "The translucency of the symbol.",
    "It's transjective.",
    "It's trajective.",
    "It's transformative, and it's trans temporal transtemporal, spatial.",
    "Aletheia, through the symbol.",
    "That's how you do gnosis.",
    "Now let's give the troubling example that is central to Corbin.",
    "And I found it disturbing in ways.",
    "And some of you will too.",
    "And I hope so.",
    "What I ask for right now is be patient because I want to unpack this.",
    "I don't want to try and be dismissive, but I want to show you that Corbin is not using this notion in a way you standardly will.",
    "And like I said—I hesitate to do—so.",
    "The most important symbol of this for Corbin is what he calls the angel (Fig. 9a) (writes The Angel below Symbol).",
    "And that's why Cheetham puts it in the title of his book.",
    "And Avens puts it in the title, the subtitle of his book on The New Gnosis.",
    "Fig.",
    "9a",
    "And soon as I put that up there, many of you are now rolling your eyes as I did.",
    "There's a part of me that I can feel there's tension behind my eyes wanting to roll them.",
    "Oh no, angels.",
    "Oh, silly superstitious idea, you know, cherubs and only new age people that, you know, swing crystals and, you know, the angels and all that stuff.",
    "And it's like, oh no, what a disaster.",
    "And I deeply appreciate that.",
    "I'm not being dismissive of that.",
    "I would say that that's an imaginary understanding of angels.",
    "One that Corbett himself repeatedly and deeply rejects.",
    "What's he talking about?",
    "And why is he using this term?",
    "Okay, he's using it because it's a term that is filled, that fills, some of the literature of the Persian Sufism that he reads.",
    "get the notes!",
    "The Notion Of The Divine Double I want to propose to you an alternative way of understanding this to what our sort of cultural imaginary way of understanding this.",
    "Let me go back in our history first (Fig. 9b) (draws an arrow away from The Angel) to try and get a different way of leading into this notion and then take it into some current cutting edge analytic philosophy, believe it or not, and show how that fits well and comports well with the cognitive science we have been doing throughout this response to the four prophets of the meaning crisis.",
    "Fig.",
    "9b",
    "Okay.",
    "So, this is—now I'm making use of the seminal work of Stang (writes Stang).",
    "Stang wrote a book called the Divine Double, which is a followup to a book he wrote on pseudo Dionysus.",
    "And he's written some brilliant articles on pseudo Dionysius that brings all this out.",
    "Now.",
    "So the book is called the Divine Double (writes Divine double below The Angel) and he's pointing to a particular motif that was prevalent in the Mediterranean world during—and remember we talked about the Hellenistic domocide and thereafter and you have the rise of gnosism and early Christianity.",
    "So during this period and across many different groups, you see it within gnosticism, he makes a clear case—and you know, again, some people are gonna, No!",
    "He makes a clear case for this motif showing up within early Christianity, you can see it in Manichaeism, you can see it clearly in Neoplatonism and Plotinus, this notion of the divine double.",
    "I'm spending time on this because you won't understand Jung also, if you don't understand this divine double.",
    "Okay.",
    "So what's the notion of the divine double?",
    "It becomes prevalent through the Mediterranean spirituality of the Hellenistic and post Hellenistic period.",
    "And I didn't talk about it that much when I talked about the Gnostics and the Neoplatonists because I wanted to talk about it here because here is where I think it belongs.",
    "This was the idea.",
    "And again, part of this is how this is so antithetical to our way of thinking, especially our decadent romantic way of thinking.",
    "So the decadent romantic way of thinking, you know, that sort of goes back to Rousseau is you're born with your true self and you have to be true to your true self and you have to express it.",
    "And that's what it is to be authentic.",
    "So this has become pervasive in our culture, right?",
    "In this Mediterranean spirituality, the motif is very diff—it's this idea that I'm here",
    "and I have a self right now, or they might say a psukhe, a spirit or soul.",
    "I have myself right now.",
    "But it is bound to the divine double.",
    "There's a double of me that is archetypically more important than me and that what I am doing, my true self is actually this divine double, and my spiritual path is to reunite this self with that divine double.",
    "And to bring it, bring the two together, then that realization of their interdependence culminates in a kind of mystical union between them.",
    "Now, this is still all very fuzzy language.",
    "I'll grant you that.",
    "But first of all, notice how this is very interesting.",
    "Think—step aside from the mythos for a minute and think about the concept—so you see how this is gnostic, right?",
    "Not in the sense of gnosticism, but well, a little bit in the sense of gnosticism, because there's this transgressive—it's trying to break grammar.",
    "It's trying to break the grammar of thinking of your true self as something you have.",
    "Your identity is something you have that you're born with it.",
    "It's in you.",
    "And what you have to do is express it authentically.",
    "And that grammar is being subverted and transgressed by the idea that your true self is beyond you and you have to aspire to it.",
    "And you see there's a bit of a Socratic element there, right?",
    "That your true self is something you aspire to, rather than something you have.",
    "The true self is something realized through the being mode of self-transcendence, not through the having mode of inner possession.",
    "And so the divine double, it is pervasive.",
    "It's a pervasive mythos.",
    "And what I'm, first of all, what I recommend is—and I think this is a very fair recommendation.",
    "You understand Corbin's use of the angel as a symbolic way of talking about the divine double (draws an arrow from The Angel to Divine double).",
    "And you may say, okay, that's great.",
    "And I see why it challenges the grammar, but I don't care about this because, all right, I didn't believe in angels and I don't believe in divine doubles.",
    "So telling me about angels in terms of divine doubles, what does that gain me?",
    "That gains me nothing.",
    "An Aspirational Process Towards A More",
    "Angelic Self Well, I want you to be very, I want to be very careful here.",
    "I want to start a problem.",
    "I want to start you on a deep analysis of this, right?",
    "Let's put aside the mythos, let's put aside the metaphysical claims, right?",
    "And let's focus in on this very process of aspiration towards a better self, towards a more angelic self, right?",
    "Because it goes back to the Socratic project, but you can also see it in the depths of our current—sorry, not, that's too broad (erases the board).",
    "Sorry.",
    "You can see, you know, this process of aspiration towards a greater, better fuller self is, of course, all the way through Maslow.",
    "It's all the way through Jung.",
    "This aspirational process is central to a lot of the mythos that we have about talking about how we are going to normatively improve, not our situation, but ourself.",
    "So is the divine double a crazy idea?",
    "Well, in one sense, it is.",
    "Again, if you just sort of literalize the mythos into some sort of axial two world mythology, a metaphysics.",
    "Sure.",
    "But maybe it's not a crazy idea if we go back and try to ask this question.",
    "Instead of asking the question—look, this is what I meant about real dialogue, philia sophia, not philia nikia.",
    "Instead of asking the question, should I believe that?",
    "First, ask yourself the question.",
    "Why did so many different groups of people in that world believe it?",
    "What was going on there?",
    "What was it doing?",
    "And here is where I think I can immediately invoke the important work which I have discussed repeatedly throughout this entire—the entire argument in this entire series, the important work of LA Paul and transformative experience.",
    "And that was bound up with the way we talk about gnosis.",
    "Now I alluded to somebody else's work.",
    "Work that was influenced and from the same area, sort of—I don't know what to call it—school?",
    "As LA Paul's work.",
    "And this is the really important work of Agnes Callard and her book is entitled Aspiration.",
    "And she's arguing for a neglected form of rationality that is best understood through aspiration.",
    "And rationality?",
    "What?",
    "Remember, I don't use rationality to mean management of just the logical management of argumentation.",
    "Rationality means any systematically reliable internalized psychotechnology that reliably and systematically affords you overcoming self-deception and affords you cultivating enhanced connectedness, enhanced meaning in life.",
    "That's why the notion of rationality I've argued for is bound up with the—it can culminate, it could point towards the cultivation of wisdom.",
    "So there's, let's talk about yourself before the transformation or before you launch into the aspirational process and the self afterwards,S1 and S1 (writes S1 and S2).",
    "(Text overlay appears saying, 'This has nothing to do with the S1/S2 distinction between kinds of cognition).",
    "Now, LA Paul tends to represent this as a much more sort of rapid transition.",
    "And I think there's important truth in that, the insight.",
    "Whereas Callard is representing it much more, not incrementally, that's not the right word, but much more developmentally having a much more extended developmental trajectory.",
    "And you can reconcile those, I think, quite readily by seeing qualitative development as a sequence of insightful transformations.",
    "So I don't think there's any deep inconsistency here.",
    "Okay.",
    "So what's the problem here?",
    "Well, as I've already pointed out (Fig. 10a) (draws a line connecting S1 and S2) with any genuine quantitative—sorry, I used exactly the wrong word.",
    "I apologize.",
    "Because I've pointed out with any genuine qualitative development—quantitative elements, you just get, sort of, more things, more beliefs, more experiences.",
    "Qualitative development is why I am so different in kind from that kid that was born in Hamilton.",
    "It's a fundamental difference of competence of what I can know and what I can do and what I can be rather than just how much.",
    "Fig.",
    "10a",
    "So, I've already pointed out that you have an issue here of non-logical identity (Fig. 10b)",
    "(writes Non-logical identity under S1 and S2).",
    "Okay.",
    "So this is not an identity relation that that can be captured by the fundamental identity theorem in logic that A is identical to A (writes A=A)",
    "meaning that they share all the same properties.",
    "We do not.",
    "John in Hamilton and John in Toronto.",
    "John and Hamilton then, and John and Toronto now, we are not, we are not this (indicates A=A).",
    "Fig.",
    "10b",
    "We have a non-logical identity and I've brought that out.",
    "And how much gnosis is about the difficulties, of trying to overcome the difficulties that this poses (indicates Non-logical identity).",
    "Because of this non-logical identity.",
    "And I'm not going to repeat these arguments, go back and look at them when I talk about gnosis.",
    "We cannot reason our way through this.",
    "We cannot infer our way through this.",
    "And Callard is deeply in agreement with this aspect.",
    "You cannot deliberate your way through it.",
    "You can not decide your way through it.",
    "get the notes!",
    "So what is the nature of the relation, right?",
    "Well, Callard thinks it's aspirational.",
    "It involves what she calls aspiration.",
    "But she's at pains to point something out that LA Paul doesn't, which I think is very important.",
    "You can't, if you don't include this process (writes Rational beside Aspirational) as part of what you mean by this term, you're going to get into a deeply self-refuting position.",
    "Because my relationship to rationality and my relationship to wisdom are aspirational.",
    "I am aspiring to become rational, precisely because I am not currently that rational.",
    "And if the aspiration to rationality is not part of rationality, you're getting into a weird kind of self-refutation.",
    "The aspiration of rationality is constitutive of the ongoing process of being rational.",
    "And therefore it must be included in your notion of rationality and notice how we're getting back towards the platonic idea of the deep interpenetration of love and reason.",
    "Took a long time, eh?",
    "It took a long time to circle around back to that.",
    "Of course, this is also the case for wisdom.",
    "It's also—look, think about this this way.",
    "One of the things I need to do to become rational is to become more educated.",
    "And, but Callard argues explicitly, a genuine educa—well, there's different meanings to that word now.",
    "One is just the accumulation of facts and skills and stuff like that.",
    "But for many, and this is what—this was supposed to be, maybe it still is, the defining feature of a liberal education.",
    "Liberal, liberal, to liberate you.",
    "Gnosis.",
    "To save you.",
    "To liberate you from existential entrapment.",
    "Our liberal education is designed to make you into a better self, a better person, which is why it seems so useless to people who want to manipulate and control you.",
    "Think about that when you side with, Oh, liberal education silly, you're a—your side.",
    "I think you're getting on the wrong side because we're losing something there.",
    "Right?",
    "So a liberal education, and this is what it classically meant when you go back into the middle ages is gnosis.",
    "It's aspirational.",
    "And you don't know what it's going to be like.",
    "Remember all that stuff about LA Paul?",
    "So let me leave you with the example from Callard, and then we'll come back and talk about this in the next episode and expand this whole.",
    "What is that?",
    "What am I trying?",
    "What am I leading you towards?",
    "I'm leading you towards that this (indicates S1 and S2) is the relationship between the existing self and the divine double.",
    "Or another way of perhaps putting it the divine double is a symbol in Corbin's sense that allows you to move from yourself now to yourself then to the better self.",
    "One of the examples that Callard gives in aspiration is and think about how this fits in with a liberal arts education.",
    "Somebody who wants to come to appreciate music and notice a play and how that word appreciation means both understanding and a gratitude.",
    "It has a connotative emotional aspect.",
    "It has a denotative conceptual aspect of—cognitive aspect, I should say.",
    "I will understand music.",
    "So I want to, let's say I don't currently get classical music.",
    "But I have an inkling that's really important.",
    "I think that, you know, Charles Williams and Barfield and Tolkien and CS Lewis called themselves the inklings.",
    "I have an inkling that there's a self and a world there.",
    "Remember we talked about the person trapped in this world, but a sense that there might be a better self and a better world over there?",
    "I have an inkling that I should like classical music.",
    "But I don't currently like classical music.",
    "I have to come to be the kind of being that appreciates classical music.",
    "How do I do that?",
    "How do I bridge from me now not appreciating, not getting, not liking, not enjoying classical music to somebody who can sincerely say I love classical music.",
    "I really get it now.",
    "How can I?",
    "We use this phrase and notice how it's so rich and resonant with, you know, contact epistemology.",
    "But now I have a taste for music.",
    "I've an acquired taste for it.",
    "Let's get behind the metaphor.",
    "How is it?",
    "And notice when you taste something, you're putting it into—you're putting it into your being.",
    "It's not only contact.",
    "It's even consumption, not in the having mode sense, but taking it deeply in, right.",
    "What is it to move that way?",
    "What I'm trying to show you is that Corbin's talk about the angel is a way of him invoking and bringing into activity, all of this stuff about symbolism that we've talked about and integrate it with this process of aspirational rationality, that is so central to self-transcendence.",
    "And so central to us becoming more rational and more wise.",
    "Thank you very much for your time and attention.",
    "- END - Episode 48 Notes To keep this site running, we are an Amazon Associate where we earn from qualifying purchases Aletheia Aletheia is truth or disclosure in philosophy.",
    "Henry Corbin Henry Corbin was a philosopher, theologian, Iranologist and professor of Islamic Studies at the École pratique des hautes études in Paris, France.",
    "Gnosis Gnosis is the common Greek noun for knowledge.",
    "The term is used in various Hellenistic religions and philosophies.",
    "It is best known from Gnosticism, where it signifies a spiritual knowledge or insight into humanity's real nature as divine, leading to the deliverance of the divine spark within humanity from the constraints of earthly existence.",
    "Robert Avens Book Mentioned: The New Gnosis – Buy Here John Caputo John David Caputo is an American philosopher who is the Thomas J. Watson Professor of Religion Emeritus at Syracuse University and the David R. Cook Professor of Philosophy Emeritus at Villanova University.",
    "Book Mentioned: The Mystical Element In Heidegger’s Thought - Buy Here Angelus Silesius Angelus Silesius, born Johann Scheffler and also known as Johann Angelus Silesius, was a German Catholic priest and physician, known as a mystic and religious poet.",
    "Owen Barfield Arthur Owen Barfield was a British philosopher, author, poet, critic, and member of the Inklings Meister Eckhart Eckhart von Hochheim OP, commonly known as Meister Eckhart or Eckehart, was a German theologian, philosopher and mystic, born near Gotha in the Landgraviate of Thuringia (now central Germany) in the Holy Roman Empire.",
    "Paul Tillich Paul Johannes Tillich was a German-American Christian existentialist philosopher and Lutheran Protestant theologian who is widely regarded as one of the most influential theologians of the twentieth century.",
    "Ontotheology Ontotheology means the ontology of God and/or the theology of being.",
    "While the term was first used by Immanuel Kant, it has only come into broader philosophical parlance with the significance it took for Martin Heidegger's later thought.",
    "Physis Physis is a Greek philosophical, theological, and scientific term, usually translated into English — according to its Latin translation \"natura\" — as \"nature\".",
    "Book Mentioned: Heidegger and Asian Thought",
    "- Buy Here Book Mentioned: Heidegger’s Hidden Sources - Buy Here Tao Te Ching",
    "The Tao Te Ching is a Chinese classic text traditionally credited to the 6th-century BC sage Laozi, also known as Lao Tzu or Lao-Tze.",
    "Rumi Jalāl ad-Dīn Mohammad Rūmī, also known as Jalāl ad-Dīn Mohammad Balkhī, Mevlânâ/Mowlānā, Mevlevî/Mawlawī, and more popularly simply as Rumi, was a 13th-century Persian poet, Hanafi faqih, Islamic scholar, Maturidi theologian, and Sufi mystic originally from Greater Khorasan in Greater Iran.",
    "Neoplatonism Neoplatonism is a strand of Platonic philosophy that emerged in the second century AD against the background of Hellenistic philosophy and religion.",
    "Gary Lachman Gary Joseph Lachman, also known as Gary Valentine, is an American writer and musician.",
    "Book Mentioned:",
    "Lost Knowledge of the Imagination - Buy Here Tom Cheetham Book Mentioned: The World Turned Inside Out – Buy Here Book Mentioned: Imaginal Love – Buy Here Book Mentioned: All The World An Icon – Buy Here Jonathan Pageau Manichaeism Manichaeism was a major religion founded in the 3rd century AD by the Parthian prophet Mani, in the Sasanian Empire.",
    "Charles M. Stang Book Mentioned: Our Divine Double – Buy Here Book Mentioned: Apophasis and Pseudonymity in Dionysius the Areopagite: \"No Longer I\" – Buy Here L.A. Paul Laurie Ann Paul is a professor of philosophy and cognitive science at Yale University.",
    "She previously taught at the University of North Carolina at Chapel Hill and the University of Arizona.",
    "She is best known for her research on the counterfactual analysis of causation and the concept of “transformative experience.”",
    "Agnes Callard Agnes Callard is associate professor of philosophy at the University of Chicago.",
    "Her primary areas of specialization are ancient philosophy and ethics.",
    "Book Mentioned: Aspiration: The Agency of Becoming – Buy Here Other helpful resources about this episode: Notes on Bevry Additional Notes on Bevry Ep.",
    "47 - Awakening from the Meaning Crisis - Heidegger Welcome back to Awakening from the Meaning Crisis.",
    "Last time I finished the discussion of wisdom and connected it to enlightenment and argued for the wise cultivation of enlightenment as our deepest kind of existential response to the meaning crisis.",
    "A way in which we can awaken from the meaning crisis.",
    "I then wanted to put that scientific model of spirituality, for lack of a better phrase, into discourse with some of the central prophets of the meaning crisis.",
    "I'm using the word prophet, of course, as it's used in the old Testament sense.",
    "I'm talking about individuals who were crucial for articulating the advent and helping to propose or promise a response to the meaning crisis.",
    "I put a diagram on the board.",
    "I'm not going to re-put that diagram on the board in which Heidegger played a central role.",
    "There's many connections in there that I will not be able to fully address.",
    "I'll point out.",
    "And because some of the people are there insofar as they help us articulate the response, not to be examined for their own sake.",
    "A couple of major pointers I want to make out.",
    "First, an apology.",
    "I misspelled because of my dysgraphia and I didn't realize it.",
    "There was something bothering me the whole time about the diagram.",
    "I misspelled Heidegger.",
    "I won't make that mistake again.",
    "I mentioned the work of Nishida and Nishitani in the Kyoto school.",
    "I will talk briefly about Nishitani here, but I won't be able to go into that in-depth.",
    "I do intend to pursue this later in another series I am putting together.",
    "I'm putting together a couple of series to follow this one, and I would like to do a series that will include work on the Kyoto school.",
    "[A] series that I'm entitling The God Beyond God, in which we look at all of these great non-theistic thinkers within both Eastern and Western traditions and things like the Kyoto school that try to bridge between them.",
    "So I will have to neglect to some degree the Kyoto school in this series, but I promise to follow it up more deeply in another series.",
    "I also mentioned Derrida.",
    "And I will not be able to talk very much, probably maybe not even at all about Derrida and deconstructionism.",
    "I will also address this again when I return in the other series, The God Beyond God, especially when I'm going to talk about the relationship between Derrida and what's called negative theology.",
    "So many of you will be perhaps disappointed that I don't talk too much about Derrida or the Kyoto school.",
    "I do promise to return to that in another series that I am currently working on.",
    "So what did I do?",
    "I sort of tried to give a quick background to Heidegger.",
    "I talked about the importance of Husserl and phenomenology.",
    "And I pointed you to the work of Sokolowski as a great introduction and this slogan of back to the things.",
    "Husserl.",
    "And he does write a book with the word crisis in its title.",
    "The crisis, I think—what is it?",
    "In European Science?",
    "We talked and this is basically, he is pointing to the loss of contact epistemology, and he's trying to get that contact back.",
    "And this is done through a reflective experience through our—not just introspection, but through a reflective experiential attention, paid to the structures and processes within our experience.",
    "And I pointed out how there's two components in this.",
    "There's the intentional mental directedness, noesis, and then there's the world disclosure.",
    "There's a deep correlation between them.",
    "And this is the noema.",
    "The noesis—and notice that that's the term I've used also to describe perspectival knowing, right?",
    "And that the correlation between the noesis and noema is very much like the agent arena relationship.",
    "Now, we took a look at Heidegger's main criticism of Husserl's work and Heidegger's main criticism is that Husserl's work had not really given us back the missing contact.",
    "I would say that I'm in agreement with this.",
    "I think Heidegger's pointing to something very important in the critique of phenomenology that Sparrow has picked up in his wonderful book, The End Of Phenomenology.",
    "Anyways, Husserl's work had not given us a contact, but it has not really developed adequately participatory knowing.",
    "Now, I do think that Merleau-Ponty went a long way towards addressing this.",
    "He's after Heidegger with his notion of embodiment.",
    "And I think I tried to explain how much this notion of embodiment has been taken up into cognitive science.",
    "And how intimately it is connected with the return to contact epistemology.",
    "In fact, I would argue that there is no return to a contact epistemology without a deep existential and theoretical recognition of embodiment.",
    "So Heidegger is critiquing both the lack of participatory knowing, and perhaps that's addressed by Marleau-Ponty, but there's a further thing that I think Marleau-Ponty does not address that is also lacking for Heidegger.",
    "Is that participatory knowing is not set within ontology.",
    "It's not set within a deeper understanding of being and how we come into contact with being.",
    "Being as realness.",
    "In the sense of the groundingness, with that which grounds truth, seems to be something that Heidegger feels that was lacking in Husserl's work.",
    "Another way of putting it is that Heidegger thought that Husserl was still trapped within the Cartesian cultural cognitive grammar.",
    "Husserl's still trapped within subjectivity.",
    "And this is the main thing that Sparrow criticizes throughout his entire book.",
    "There's also a YouTube video by him that you can watch on his critique of phenomenology.",
    "That phenomenology is driven by the goal of returning to the things, but because of its approach, it actually can never return to the things.",
    "It is still locked into a kind of subjectivity or a hidden kind of idealism.",
    "That what is really needed to get to being is this aspect of Heidegger that I'm going to talk about shortly, which is to pay attention to the independence of being from our experience of it, which is something that a lot of people do not tend to emphasize in Heidegger.",
    "That's largely because I think, while it's present in Heidegger, it's not made explicit and foregrounded enough from him.",
    "But I think the work of speculative realists like Harman and others–Morton, for example, brings this aspect out and why they see their work as transcending phenomenology.",
    "get the notes!",
    "Questing Into Our Being So how—but now to return to Heidegger, how do we get to this deeper contact?",
    "Well, it's really interesting.",
    "He tries to do he's still within phenomenology.",
    "But he's trying to get it to drive towards ontology, get us back in touch with the world.",
    "And he's trying to get you to do this through a specific kind of questioning.",
    "This questioning, you have to think of this questioning as taking place, not in the having mode in which we're trying to get an answer.",
    "Trying to control the situation.",
    "You have to think of it much more as within the being mode.",
    "A being mode that is experienced as wonder.",
    "In fact, perhaps a better word is not questioning, but questing, you're trying to go on a quest with this questioning.",
    "You're not trying to have a propositional answer.",
    "You're trying to engage in a participatory transformation.",
    "Okay.",
    "Now this brings us to the central thing for Heidegger in some ways.",
    "And this is Heidegger's notion of dasein.",
    "Being there.",
    "This is our being.",
    "And notice how this is an inheritance from the Christian tradition that we're in the image of God.",
    "Somehow by Heidegger's taking this up, as somehow by paying attention, by wondering into, questing into our being, we will get a deeper understanding of being itself.",
    "Why is that?",
    "Because for Heidegger our being is the being whose being is in question.",
    "We are the type of being who actually questioned who and what we are in a way that makes a difference to who and what we are.",
    "This is the core idea of existentialism.",
    "Existentialism is that we are fundamentally without an essence and that we are—our essence, if you want to put it that way, is to have no essence.",
    "And therefore we are continually defining ourselves by how we question our being and respond to that questioning questing.",
    "So the idea by phenomenologically exploring that being, our dasein, the way we are, the being whose being is in question, we can simultaneously come into contact with our modal existence.",
    "Come into the being mode and come into contact with the mystery of beer.",
    "A History Of Nihilism Now coming into contact with this is central to Heidegger and Heidegger is deeply responding to the meaning crisis.",
    "One of Heidegger's most famous, or perhaps infamous thesis, the one that runs as a constant thread throughout his work is this idea, that the history of metaphysics, which Heidegger tends to use in a pejorative term, the history of philosophical, existential, and perhaps also religious responses to dasein, to the kind of being we have, the history of that response is metaphysics.",
    "And for Heidegger, that history is the history of nihilism.",
    "Heidegger sees that whole project as fundamentally misconstrued.",
    "To use our language, the whole project is a fundamental misframing of our relationship to being and because of that, that has produced this deep loss of contact with our being and therefore simultaneously with being itself.",
    "And that for Heidegger is the meaning crisis.",
    "So this whole history that we saw, for example, in the first half of this series is the history of nihilism.",
    "And you can see, I hope, if you remember that argument, that there's a lot of great truth in what Heidegger has to say about how the unfolding of the history of that metaphysical project has led us into the meaning crisis.",
    "Now, Heidegger does not use the word ontology in a pejorative way, the way he uses the word metaphysics.",
    "Most people don't have this distinction as clearly as Heidegger stipulates it to be.",
    "So when we're talking about Heidegger, metaphysics is a pejorative term.",
    "It is a misframing of the ontological project.",
    "The ontological project is the project of understanding our being and thereby understanding our relation to being and thereby understanding being itself.",
    "Heidegger is volumous, he's incredibly prolific.",
    "He's also famously difficult to read.",
    "His works have an aporetic structure to them.",
    "They often fail to come to any clear conclusion.",
    "Part of that I think is legitimate.",
    "Part of that is him wrestling with trying to break out of the cultural cognitive grammar.",
    "But I also think, and this is part of my criticism of Heidegger, part of it is self-promotional.",
    "By constantly being the person for, you know, announcing these deep mysteries and how difficult it is to think about them and how he's exemplifying that difficulty.",
    "He was also—and I think it's pretty clear this is part of what he's doing.",
    "He was also—you can see this from some of those who knew him more intimately—he was also building a mystique around himself.",
    "So you have to take that into account.",
    "So all of that makes it difficult to sort of point to here's where I'm going to point to the core of Heidegger.",
    "But I do think there's something for me and for many other people, and I hope for you, there is a particular place where I think we can zero in on what Heidegger is doing.",
    "And this is his important essay Vom Wesen der Wahrheit which is on the essence of truth.",
    "And I think this is important because it will tie into many of the themes we have discussed.",
    "I want to read an extended quote from this, and then I'll comment on it.",
    "So this is the quote from Heidegger, obviously translated into English, which is a difficult task I've been told.",
    "You have to read a lot of Heidegger before you read Heidegger well because you will misread Heidegger for a long time.",
    "And part of why he makes his writing so torturous is to sort of tear you out of the mistake in ways you will misunderstand him.",
    "The Essence Of Truth",
    "Okay.",
    "So here is the translation.",
    "\"A statement is invested with its correctness,\" so this is a sense of true—when we say that statement is true when we mean it's correct.",
    "\"A statement is invested with its correctness by the openness of comportment.\"",
    "So I'm going to try and unpack all of this for you.",
    "The openness of comportment.",
    "How you were comported towards things.",
    "\"For only through the latter,\"—the openness and comportment.",
    "And I'll try and explain to you what that means.",
    "\"Can what is opened up really become the standard for the presentative correspondence.\"",
    "Okay, so let's stop here.",
    "So here's the idea of truth as correspondence (Fig. 1a) (writes Truth as correspondence).",
    "A standard notion of truth.",
    "So the idea here is this is how truth works.",
    "Here's the statement (writes Statement below Truth as correspondence) and what makes it correct is that it corresponds (draws an arrow from Statement) in some important way to reality (writes Reality).",
    "What's in the statement and what's in reality correspond.",
    "That's what makes it correct.",
    "That's what it is to be true.",
    "Now, of course, famously philosophers have argued for a long time in many different ways about what this correspondence is.",
    "But what Heidegger is trying to say is that debate about the correspondence has missed something.",
    "And here's part of the misframing.",
    "It's missed that this corresponding relationship is grounded (draws an arrow below the arrow connecting Statement and Reality and writes Grounded), is dependent on, and is sustained by a deeper relationship.",
    "Right?",
    "Fig.",
    "1a \"Open comportment must let itself be assigned the standard.\"",
    "So this lower.",
    "relationship, this open comportment is an affordance of an ability to set up correspondence between statements and reality, such that we find them true.",
    "Now start to think about what this means.",
    "That must mean that in making the statement, the person is directed and connected.",
    "It also means that the statement is picking up on some aspect of reality that is disclosed and there's some kind of connection there.",
    "So he's trying to point towards that.",
    "This means that it must take over a pre-given standard for all presentation.",
    "This belongs to the openness of comportment.",
    "So the normative standard, what we normally call truth, truth as correctness, as correspondence between statement and reality is ultimately grounded, dependent on how this deeper relationship, which we haven't quite articulated yet, affords and makes possible this (indicates Statement and Reality).",
    "But you can see something here, to use some of our language.",
    "You can see how the agent and the arena (Fig. 1b)",
    "(writes Agent and Arena and draws a double-headed arrow between) have to be shaped to each other, such that what the agent does or says is meaningful in that arena (draws a double-headed arrow above Agent and Arena).",
    "So the agent and arena relationship makes possible and affords this correctness.",
    "But, of course, what Heidegger's pointing to is, yes, but what grounds (draws a downward arrow below Agent and Arena and writes ?)",
    "this agent arena relationship?",
    "Now I've tried to argue that it's ultimately the process of relevance realization.",
    "We're going to come back to that.",
    "Heidegger's gonna (draws an arrow from Grounded)—I'm gonna argue that this is relevance—I've argued this is relevance realization (writes RR beside ?).",
    "Heidegger talks about this in terms of attunement (writes Attunement beside ?)",
    "He uses attunement, which is very nice, cause it picks up on musicality.",
    "Fig.",
    "1b",
    "So here's another quote by Heidegger, specifically mentioned this.",
    "\"However, being attuned, attunement can never be understood as experience.\"",
    "Notice what he's saying here.",
    "\"Attunement can never be understood as experience and feeling.\"",
    "He is rejecting any subjective interpretation of attunement.",
    "Why?",
    "Because—to continue, \"it is thereby simply deprived of its essence.\"",
    "You have lost the essence of attunement if you understand it subjectively.",
    "It is not an experience.",
    "It is something that makes meaningful experience possible.",
    "And I argued that that, of course, was the case, for relevance realization.",
    "Let's continue with his quote, \"being attuned,\" that is what he calls eksistent, standing out exposedness.",
    "So this standing out, he spreads—he takes the word existence.",
    "And he plays with words a lot.",
    "(Fig. 2) (writes Ek-sistence) this is standing out (writes Salience below ek-sistent), which of course is analogous to how what the word salient means.",
    "Standing out.",
    "Fig.",
    "2",
    "\"This eksistent exposedness,\" how things—exposedness to being—this is us—\"as a whole can be experienced and felt.\"",
    "And he puts both those words in sort of scare quotes to question them.",
    "\"Only because the man who experiences\"—again in question, because he's challenging this whole natural way of thinking and talking, that's why he's putting it all in these scare quotes, right?",
    "\"Without being aware of the essence of attunement is always engaged in being intuned in a way that discloses beings as a whole.\"",
    "So attunement is not subjective.",
    "Any subjective feeling or experience of it is actually grounded in the attuning relationship that precedes and grounds our cognitive appraisal or appropriation within the agent arena relationship.",
    "get the notes!",
    "Okay.",
    "So because we have got locked up here (indicates Truth as correspondence), what I would call the propositional level, we have forgotten this (indicates Attunement and RR).",
    "We have forgotten the attunement relationship, which for Heidegger is the essence of truth.",
    "Because it's what makes correctness of statements possible for us.",
    "\"Because of this forgetfulness,\" Here's another quote, \"man, clings to what is readily available and controllable even where ultimate matters are concerned.\"",
    "Remember that word ultimate when we come back to Tillich please.",
    "So what happens is we get trapped into the having of propositions.",
    "We get trapped into the having mode (indicates Truth as correspondence).",
    "All right.",
    "\"What is readily available and controllable.\"",
    "There's a deep, modal confusion at this deep existential level.",
    "The forgetting of the grounding attunement also traps us within propositional processing and it traps us in the having mode, the having of correct propositions.",
    "And this goes right back, right?",
    "This goes right back to Plato.",
    "I don't think Heidegger would like me doing this.",
    "His attitude towards Plato is very ambivalent.",
    "Plato consistently makes a distinction between philia sophia (writes Philia Sophia), right?",
    "The love of wisdom.",
    "And philia nikia (writes Philia Nikia).",
    "The love of victory.",
    "The love of victory.",
    "The having of the correct answer that defeats the opponent.",
    "And one of Plato's ongoing points is this is the deepest kind of bullshitting (encircles Philia Nikia) because this looks like we're arguing.",
    "This looks like we're reasoning (indicates Truth as correspondence), but what we're doing is manipulating propositions, and trying to assert correctedness, but we're forgetting all of this and we're forgetting the pursuit of wisdom (Fig. 3) (draws an arrow from Philia Nikia to Philia Sophia).",
    "The transformative existential project that Socrates advents for us.",
    "Fig.",
    "3",
    "This is why I continually criticize people who I suspect present themselves as if they're doing this (indicates Philia Sophia), but are often doing this (indicates Philia Nikia).",
    "This is why I am so critical of people who want to debunk, demolish and debate to the point of victory their opponents, people who really are incapable of getting out of modal confusion.",
    "They cannot remember the being mode.",
    "Because they can't listen.",
    "Look, this is how, you know, somebody is listening.",
    "They will say, I did not know that I have just learned something from you or I was wrong.",
    "I was mistaken about this.",
    "Those are the marks of philia sophia Heidegger is trying to get us to remember philia sophia.",
    "And he definitely sees the history of metaphysics is becoming more and more bound up with philia nikia, the pursuit of victory.",
    "The ultimate theory that crushes all opposition.",
    "So we have to wake up according to Heidegger.",
    "And again, this is why his language is so torturous because we are in a state of deep forgetfulness, deep modal confusion.",
    "And if we read his text, we are deeply tempted to read them from that forgetfulness and that modal confusion and thereby fundamentally misunderstand him.",
    "So his texts are right deliberately Socratic in they're constantly trying to undermine that cognitive cultural grammar that we habitually bring to things.",
    "So he wants us to remember Sati.",
    "The forgotten mystery of dasein.",
    "So he says this, \"Whenever the concealment of being as whole is conceded only as a limit that occasionally announces itself, concealing as a fundamental occurrence has sunk into forgetfulness.\"",
    "If we only sort of acknowledge the way beings transcend our framing of them, sort of, at the limit.",
    "Yes, yes.",
    "Yes.",
    "Reality is combinatorially explosive.",
    "Yes, yes, yes, yes.",
    "And we turn that and we wave our hands",
    "and then we go back to, yes, but within this, within my framework, blah, blah, blah, blah.",
    "And what Heidegger says is, when you only acknowledge it as a limit, you have actually deeply forgotten it.",
    "That relationship to the combinatorial explosive nature of things has to be a ongoing feature of your thinking.",
    "Now, I try to argue for this within the relevance realization framework, which I'm going to show you is deeply appropriate to Heidegger in terms of this idea of sacredness as an acted, enacted participatory resonance to the moreness, the inexhaustibleness of reality.",
    "The Thing Beyond Itself In order to pick this up, I want to briefly, because as I said, I can't go into this a great deal.",
    "I hope to go into this again later in another series (erases the board).",
    "But I want to briefly talk about the work of Harman and what's called object-oriented ontology, or what's also known as speculative realism.",
    "And this is the position from which Sparrow and Harman and others have critiqued phenomenology as being inadequate.",
    "So what—and again, I'm trying to summarize all of this, which is like some of the most exciting work that's recently being done in ontology.",
    "You know—and I respect people who say, we need to talk about ontology more, but if you want to talk about ontology, just don't do old talk about ontology.",
    "Pay attention to the insightful, inventive, new work being done on ontology within speculative realism.",
    "So again, I wouldn't even claim this as a summary.",
    "I'm just picking up one important thread of the speculative realism as a way of trying to develop this important idea of Heidegger and cause we're gonna need it when we talk about other people like, you know, Tillich and Jung and Barfield.",
    "So the core of this is not the Kantian picture of the thing in itself, veiled by subjectivity.",
    "So remember that Kantian picture, right?",
    "Our subjectivity completely veils the thing in itself and makes it ultimately inaccessible to us.",
    "So what Harman is picking up on is that this transjective attunement, and we'll come back to see that they have a difficulty with limiting ontology to what they call correlation, just our experience of ontology.",
    "But nevertheless, let's start here.",
    "This transjective attunement makes both the subject and the object possible in experience, in phenomenological experience.",
    "So.",
    "What does that mean?",
    "Well, there's a different way about thinking about how you encounter objects.",
    "Instead of the Kantian thing in itself that is veiled from us by our subjectivity, instead, think about two things happening simultaneously.",
    "This is picking up on what Heidegger's talking about.",
    "Think about the thing shining into subjectivity and that's what phenomenology originally means.",
    "Heidegger picks up on it.",
    "The Greek term, phenomenon, actually means to shine forth.",
    "So simultaneously the thing is shining into my subjectivity, but that is interpenetrated, inter-afforded with, it is simultaneously withdrawing from my framing.",
    "It is always beyond my framing as well.",
    "And that that beyondness is not something in my phenomenology, but it contributes to the sense of the realness of my phenomenological experience.",
    "Think about how in virtual reality, when there—if the world, if you get a sense that the world is closed, if that you can drain it dry with your activity, it loses its realness, but only, and here's now the openness that Heidegger is talking about—the comportment, but only if there's a realness, a way in which the world withdraws beyond you continually.",
    "So there's always a horizon of your experience.",
    "A horizon is something you always move towards, but you can never reach.",
    "So as long as that world constantly withdraws, as it also shines into your experience, then it is real to you.",
    "Now, the moreness is not something in your experience.",
    "It's not an object of your experience, but it's a feature.",
    "The withdrawal is as much a contributor to the realness of things as they're shining into your subjectivity.",
    "This is I think, a profound way in which Harman and others have explicated Heidegger's idea and then gone beyond it.",
    "Now, the way I want to put it and the way I've argued it earlier, I think this lines up with what I've tried to argue.",
    "That our framing, which is transjective in nature about attunement simultaneously discloses and conceals.",
    "Right.",
    "So I want to replace the Kantian term, the thing in itself with another way, another term.",
    "The thing beyond itself (writes The thing beyond itself).",
    "Everything is both shining into our subjectivity and withdrawing beyond our framing of it.",
    "And those are inter-affording.",
    "They're interpenetrating.",
    "They co-contribute to the realness of the object for us.",
    "And it's precisely the withdrawing, according to Harman and others that was missed by phenomenology because of the way it was still bound within a Cartesian subjective framework.",
    "Whether or not that is completely fair to phenomenology is another question.",
    "And I'm not trying to get into that theoretical debate right here.",
    "My main focus here is trying to understand what's going on, but I think we can take from speculative realism, this idea, this term I've coined the thing beyond itself.",
    "It's clearly a central idea in Harman's idea of the object, the thing beyond itself.",
    "Truth As Aletheia Okay.",
    "So this takes us to now a new understanding of truth (erases the board).",
    "How do we get an attunement that discloses things as things beyond themselves, things that are simultaneously shining into our subjectivity, but also withdrawing into their objectivity where this no longer means an object of thought.",
    "It means a depth beyond our framing.",
    "An independence beyond our experience and how those are transjectively interpenetrating for us in the sense of realist.",
    "What does it mean to be connected to things in this way?",
    "And this is Heidegger's famous notion of truth as aletheia (writes Truth as Alethea).",
    "Truth as aletheia.",
    "So lethe means to cover or to forget.",
    "In Greek mythology, the underworld, you passed through the river of Lethe and it made you forget all of your previous life as you went into the underworld.",
    "And this (indicates 'a-' in aletheia) is of course the negation of it.",
    "So it means it's sort of a deep remembering, sati and a deep disclosure.",
    "So always remember these two poles.",
    "It's a deep remembering.",
    "You have to modally remember, not just like our normal sense of remembering.",
    "Like Sati, you have to remember the being mode and this (indicates Aletheia) discloses this aspect of reality that it is simultaneously shining and withdrawing.",
    "So truth is aletheia is this attuning to the mutual disclosure.",
    "Fittedness within the mystery of being.",
    "You're getting attuned to things.",
    "You're deeply remembering things.",
    "And again, not just cognitive memory, existential memory.",
    "You're remembering things.",
    "This is to be in contact with them when you're attuned to how they are simultaneously appearing, shining and withdrawing.",
    "Now I've been throughout using the language of relevance realization that I've argued for to talk about Heidegger.",
    "And some of you may be a little bit sort of like, eh, I don't like that.",
    "You know, Heidegger's, you know, I like the Heidegger talking, it's all ontological and I don't want this scientific talk and, you know, and you might have a deeper point there because the scientific talk for Heidegger is at a higher level than ontology.",
    "Nevertheless, I want to continue to making this argument and I want to make it because there's an explicit and important history (erases the board).",
    "And this is the connection that has had a profound impact on my work and the argument that you have seen between Heidegger, Herbert Dreyfus (Fig. 4a) (writes Heidegger = Dreyfus), and third generation, third gen, 4E cog sci (writes 3rd gen and 4E cog sci) that I have articulated and exemplified to you.",
    "Fig.",
    "4a Dreyfus is an important interpreter of Heidegger, but he's also one of the founding figures of this version of cognitive science.",
    "In fact, Dreyfus, tried to formulate or is continuing to formulate important aspects of third generation 4E cognitive science as a way of trying to articulate the importance of Heidegger for understanding the nature of mind, the nature of cognition, the nature of consciousness, et cetera.",
    "So there's a book he wrote on Heidegger's most central work, Being And Time and Dreyfus's book is called Being-In-The-World.",
    "And I want to read you a quote for that.",
    "And the quote is exemplary.",
    "It's not a unique, isolated moment in Dreyfus's work.",
    "It's something that is exemplary of a theme running throughout Dreyfus.",
    "Here's the quote from that book on Heidegger.",
    "Okay.",
    "Quote, \"Facts and rules are, by themselves, meaningless.",
    "To capture what Heidegger calls significance or involvement, they must be assigned relevance.\"",
    "And those two terms are emphasized in the original.",
    "My emphasis of them is just reporting the emphasis that he gave them.",
    "\"They must be assigned relevance.",
    "But the predicates that must be added to define relevance, are just more meaningless facts;\" You can't capture it with a definition.",
    "\"And paradoxically, the more facts the computers are given.",
    "\"—sorry, \"the more facts the computer is given,\" notice how he immediately links Heidegger to a computational psychology and a deep critique of it.",
    "\"The more facts the computer is given the harder it is to compute what is relevant to the current situation.\"",
    "You get into combinatorial explosion if you stay at the propositional computational level, and you lose your ability to fit yourself to the current situation, to cope with the current situation.",
    "This is why Dreyfus because of Heidegger was one of the founding and remains one of the ongoing critics of a purely computational Cartesian approach to cognitive science, to AI, to artificial intelligence.",
    "He literally wrote the book entitled, What Computers Can't Do that founded this whole criticism of computational psychology.",
    "It's considered the discovery of the frame problem within cognitive science.",
    "He later updated it with a later book called What Computers Still Can't Do.",
    "This has been an ongoing thing.",
    "He sees Heidegger as the deep forerunner of the criticism that we should understand the mind only in propositional computational terms.",
    "This is why Dreyfus went on to pick up also work from Marleau-Ponty, which I've referenced and helped to develop (draws a downward arrow from Dreyfus) this notion of optimal grip that I've discussed at length (Fig. 4b) (writes Optimal grip below Dreyfus).",
    "Optimal grip.",
    "It is a process that is doing this relevance realization.",
    "And it is something that is deeper than propositional knowing.",
    "Fig.",
    "4b",
    "So I'm trying to show you that my attempts to connect Heidegger, who is a prophet of the meaning crisis to the machinery of relevance realization and participatory knowing, and optimal gripping is not misplaced.",
    "Dreyfus is not a single figure.",
    "There are other figures like this.",
    "See the work by Dreyfus and Charles Taylor, Retrieving Realism, about how we can get back to really being in contact by making use of these ideas.",
    "I'm going to keep doing that.",
    "I'm going to keep showing you, because what I'm trying to show you is that the framework we have built allows us to enter into deep dialogue with the central prophets of the meaning crisis in a way that I think insightfully discloses aspects of their own theorizing and affords potentially synoptically integrating them together into a more comprehensive response to the meaning crisis.",
    "This is the final part of the argument I'm making.",
    "get the notes!",
    "A Dynamic Coupling I want to continue on.",
    "Do a little bit more about Heidegger and leading into the other thinkers we want to examine.",
    "So Avens in his wonderful book, The New Gnosis, in which he talks about Heidegger, Corbin, Jung, Hillman, right?",
    "It's a book in which she links Heidegger to Corbin into Jung.",
    "And he puts it this way when he's discussing Heidegger's thought.",
    "Here's a quote.",
    "Again, it picks up on something we've already been talking about.",
    "Here's the quote, \"A questioning that involves the questionner in the matter of thoughts so deeply, he becomes, in a sense, one with it.",
    "At this point, knowing is no longer divorced from being.",
    "We know the way we are, and we are the way we know.",
    "In the Platonic tradition, this is expressed in the axiom, like can only be known by like.\"",
    "He is pointing directly.",
    "And this is not something you get in Heidegger.",
    "But that's why I turned to Avens.",
    "He's pointing to how Heidegger is actually bringing back this deeply Neo-Platonic idea of knowing, of participatory knowing as a deep kind of conformity between you and the world.",
    "This is a participatory knowing that is a dynamic coupling.",
    "Now Avens immediately points and connects this kind of participatory knowing, this dynamic coupling, like known by like.",
    "And reality is dynamic.",
    "So you have to be dynamic and dynamically coupled to it.",
    "He immediately points from Heidegger to Corbin because Corbin is deeply influenced by Heidegger.",
    "Corbin was one of the first important translators of Heidegger into French.",
    "His translation of introduction to metaphysics was seminal for how Heidegger was spread into France, for example.",
    "But Corbin explicitly calls this participatory knowing that is a dynamical coupling, a dynamical conformity, he explicitly calls this gnosis.",
    "A term we've already examined right.",
    "\"Gnosis,\" for Corbin, this is quote \"is a.\" Here's the quote, \"a salvational redemptive knowledge, because it has the virtue of bringing about the inner transformation of man,\" sorry for the sexist language on his part.",
    "\"It is knowing that,\" quote, \"changes and transforms the knowing subject.\"",
    "You see, it's that dynamical coupling in which, you know, by being coupled to something.",
    "And it's participatory knowing because you know it insofar as you are changed and your knowing of yourself and your knowing of the object are coupled together.",
    "But that is what you need to respond appropriately to dasein.",
    "Look, you are the being whose being is in question.",
    "And by questing into that, you quest into being.",
    "You are only going to get a response to that quest when you add something that's simultaneously in an inter-penetrative, inter-affording fashion is both knowing yourself—not your autobiographical knowing, knowing the depths of your being.",
    "Knowing yourself and knowing the world, coupled together, mutually affording each other.",
    "That this is what Corbyn is calling gnosis.",
    "And this is what he's saying he's getting from Heidegger.",
    "Now let's take a look at what we've done so far (erases the board).",
    "And I want to show you how these two things are not irrelevant (Fig.",
    "5a)",
    "(writes Heidegger and draws two lines below it).",
    "They're deeply relevant to each other.",
    "You've got Dreyfus over here (writes Dreyfus below Heidegger) and the whole aspect of relevance realization (writes RR below Dreyfus) and Dreyfus is clearly pointing out that this is non-propositional.",
    "Coming from Heidegger, non-computational in the sense where computation is the inferential manipulation of propositions to draw out implication relations (writes Non-propositional, non-computational under RR).",
    "Fig.",
    "5a",
    "And then over here, you have Corbin (Fig. 5b) (writes Corbin below Heidegger).",
    "And he's calling all of this gnosis (writes Gnosis under Corbin), this participatory, mutually self- and world-transformative kind of knowing.",
    "But what Corbin is doing with the gnosis that isn't apparent in Dreyfus is that he's pointing out how this (indicates Gnosis) is redemptive.",
    "How it saves us.",
    "Right?",
    "Remember the Gnostics are trying to free us to liberate us from existential entrapment.",
    "Fig.",
    "5b",
    "So Corbin is pointing out that how this, Corbin is making explicit, that this machinery that we're talking about here is a way of responding to the modal confusion.",
    "It is a way of responding to the forgetfulness of being.",
    "It is a way of awakening from the meaning crisis.",
    "So let's try and do this again a little bit more carefully (erases the board).",
    "Forgetfulness Of Being Let's try to do this a little bit more carefully.",
    "What is this forgetfulness?",
    "This modal confusion.",
    "Okay.",
    "So on one hand, we have the being mode (Fig. 6a) (writes Being mode).",
    "And remember Fromm ultimately gets this from Heidegger as does Stephen Batchelor.",
    "Being mode, the having mode (writes Having mode).",
    "Fig.",
    "6a",
    "Do you see what I'm trying to do though?",
    "I'm trying to show you how seamlessly you can interweave this language from Heidegger with the language we've been developing in the second half of this series.",
    "So the being mode.",
    "So what's going on in the being mode?",
    "The being mode is the transformative participation in the mystery of being.",
    "Okay.",
    "That transformative participation in the mystery of being (Fig. 6b) (draws a downward arrow from Being mode), this leads, of course, to aletheia (writes Aletheia below Being mode), which I have discussed.",
    "And there's two components to this that we've discussed.",
    "There's the attunement (writes Attunement below Alethea), right.",
    "But there's also the independence of Being (writes The independence of Being).",
    "Fig.",
    "6b I think the attunement clearly points to relevance (Fig.",
    "6c)",
    "(writes Relevance below Attunement).",
    "I've already argued that repeatedly.",
    "I hope I don't have to argue it anymore.",
    "But this independence of being.",
    "This is independent of the correlation between us and being.",
    "That being always transcends how it is being known and being experienced by us.",
    "This is the moreness, the withdrawal that is simultaneously pres—I almost made a mistake.",
    "Sorry about that.",
    "The moreness is simultaneously with the presence of the shining, right?",
    "Fig.",
    "6c This (draws a downward arrow from The independence of Being), I think, is, what Harman and others would argue, ultimately gives things—I don't want to say all of their realness, but an important pull of their realness that we have neglected.",
    "You can see what I'm trying to point to there's.",
    "I mean, the word I would, you know, it might not even seem natural to put here—of the independence of Being is truth (writes Truth below The independence of Being) but that's not quite right (erases Truth).",
    "Because we've seen that truth belongs up here in the discussion of aletheia (writes Truth beside Aletheia) or even higher up in the correctness of our propositions based on Aletheia (erases Truth).",
    "get the notes!",
    "So this is relevance (indicates Relevance).",
    "And why am I stopping here?",
    "Because remember, we don't want to confuse relevance and we don't want to disconnect relevance from truth or realness.",
    "Right?",
    "I want to put to something deeper here.",
    "That the relevance must always be open to—and remember, not just as an acknowledgement of the limit.",
    "It has to be an ongoing constraint.",
    "The relevance has to have an ongoing constraint in its connectedness to a sense of the moreness (Fig.",
    "6d)",
    "(writes Moreness below The independence of Being), the inexhaustibleness of the thing beyond itself (writes Inexhaustibleness of the thing beyond itself).",
    "Fig.",
    "6d So what would be going on over here in the having mode?",
    "And where we're getting the modal confusion?",
    "So now we think of an object.",
    "We think of its being in terms of how it can be manipulated by us, not just physically, conceptually.",
    "Remember, this is the quintessential point of the having mode.",
    "We have control.",
    "We can grasp it.",
    "We can manipulate it.",
    "We can use it.",
    "We're not confronting mysteries.",
    "We're solving problems.",
    "And so what happens when we get into the having mode into conceptual manipulation and the having of propositions and we forget the being mode and all of this over here (indicates the Being mode)?",
    "What can happen to us?",
    "Onto-theology And Nihilism Well, and this is one of Heidegger's main critiques.",
    "We start to misunderstand in a modal sense.",
    "You start to misunderstand Being as a particular being.",
    "Misunderstand being as a being (Fig. 6e) (writes Being as a being).",
    "So I'm using Being as with a capital B to Being.",
    "Where I'm using a little be to mean a particular being like this, you know, marker or this eraser or this body.",
    "Alright?",
    "Fig.",
    "6e",
    "And then we misunderstand the attempt to try and get back to this (indicates The independence of Being) by positing it within—this is a modal confusion.",
    "We try to capture—something's wrong, just thinking, we're not quite getting being with a big B.",
    "When we look at this being (picks up the eraser) or think of being as a particular bit, we've got to do something more.",
    "And so what we'll do is we'll put it to the limit.",
    "Remember what Heidegger says?",
    "That's insufficient.",
    "We'll understand Being, as the supreme being (Fig. 6f)",
    "(writes As the supreme being below Being as a being), the highest being.",
    "The highest subject, perhaps.",
    "The highest person at the highest force.",
    "The highest thing.",
    "Fig.",
    "6f",
    "And for Heidegger, this is the ultimate modal confusion (indicates Having mode) for this is to try to turn being in a problem, being into a problem that can be solved by the conceptual manipulation of a propositionally defined object.",
    "And, of course, what's being alluded to here (Fig. 6g) (draws an arrow from As the supreme being and writes God) is classical Theism's sort of traditional presentation of God.",
    "Now, whether or not this is going to be fair to how many people have written about God, we're going to come back to that when we talk about Tillich and Barfield.",
    "Fig 6g Heidegger is certainly right that there is a longstanding tradition within metaphysics understood in a pejorative sense in which God is understood in this limit sense.",
    "God is understood within the having mode.",
    "God is understood as the supreme being that somehow grounds and makes all other beings.",
    "And this is a fundamental mistake for Heidegger.",
    "This is a fundamental misunderstanding.",
    "It's a fundamental problem.",
    "So this is known as the problem of onto-theology (Fig.",
    "6h)",
    "(writes Onto-theology below As the supreme being), where we try to understand Being theologically, in terms of a supreme being.",
    "And for Heidegger that, and this may strike many of you who come from religious framework as, you know, almost bordering on offensive.",
    "I'm trying to present in a way I think that's more accessible to you.",
    "That's not the case.",
    "But what Heidegger is saying—sorry, it's not the case that—I misspoke there.",
    "It's not the case that you should simply—I'm sorry.",
    "I'm not, I'm stating something where I should be requesting something.",
    "That's why I'm stumbling.",
    "I'm requesting of you that you are not simply offended, that you try and understand what Heidegger is trying to articulate here, because he's going to make this claim.",
    "Fig.",
    "6h",
    "And here's the claim that you might find offensive.",
    "That there's a deep connection between the understanding of Being in terms of the supreme being, God, onto-theology, and nihilism.",
    "This is Heidegger trying to articulate Nietzsche.",
    "And Nietzsche is another big influence on Heidegger.",
    "Understanding Being or the ground of Being as a supreme being, onto-theology, is the deep forgetfulness that just caused us existentially adrift in modal confusion and fundamentally misframing our relationship to Being and therefore being subject to a disconnectedness from realness, which is at the heart of the meaning crisis.",
    "So Tillich is going to pick this up.",
    "It's going to be deeply influenced by Heidegger, but also I think I prefer Tillich to Heidegger and there's also personal reasons for that.",
    "Tillich was the first non-Jewish academic to be basically, well, persecuted by the Nazis because he, from the very beginning, opposed them and resisted them.",
    "He had to leave Germany because of that.",
    "get the notes!",
    "Unlike Heidegger.",
    "And this is not something you should dismiss like some people do",
    "and I want to return to it, discuss it later.",
    "Heidegger joins the Nazi party and becomes an official member.",
    "An official within the Nazi party.",
    "And that is something to pay attention to.",
    "And it's fair to bring this up because Heidegger presents his entire position as an existential one, not just a theoretical one.",
    "And if you try to dismiss his participation in the Nazis by just saying, Oh, but that has nothing to do with his theory, you have sort of fundamentally missed how he is involved with the very presentation of his own theory.",
    "Now Tillich is going to take up nevertheless, Heidegger's critique onto-theology, but he's going to do something very interesting with it.",
    "He's going to use this as a way of bringing back the very traditional religious notion of idolatry and what is wrong with idolatry.",
    "So next time, what I would like to to explore with you as a way of developing further what Corbin is talking about with gnosis and gnosis as serious play.",
    "I want to try to finish up.",
    "What would it be like to have the gnosis?",
    "To remember Being through aletheia What would it be like to be remembering Being through aletheia?",
    "What would that be like?",
    "How would we be?",
    "How would the world be to us?",
    "And this is a way of trying to say, what would it be like to experience the remembering that is a way of awakening from the meaning crisis.",
    "Thank you very much for your time and attention.",
    "- END - Episode 47 Notes To keep this site running, we are an Amazon Associate where we earn from qualifying purchases Heidegger Martin Heidegger was a German philosopher who is widely regarded as one of the most important philosophers of the 20th century.",
    "He is best known for contributions to phenomenology, hermeneutics, and existentialism.",
    "Book Mentioned: Basic Writings – Buy Here Kitaro Nishida Kitarō Nishida was a prominent Japanese philosopher, founder of what has been called the Kyoto School of philosophy.",
    "Keiji Nishitani Keiji Nishitani was a Japanese university professor, scholar, and Kyoto School philosopher.",
    "He was a disciple of Kitarō Nishida.",
    "Kyoto School The Kyoto School is the name given to the Japanese philosophical movement centered at Kyoto University that assimilated Western philosophy and religious ideas and used them to reformulate religious and moral insights unique to the East Asian cultural tradition.",
    "Jacques Derrida Jacques Derriad, born in Algeria, was a French philosopher best known for developing a form of semiotic analysis known as deconstruction, which he analyzed in numerous texts, and developed in the context of phenomenology.",
    "Negative Theology Apophatic theology, also known as negative theology, is a form of theological thinking and religious practice which attempts to approach God, the Divine, by negation, to speak only in terms of what may not be said about the perfect goodness that is God.",
    "Edmund Husserl Edmund Gustav Albrecht Husserl was a German philosopher of Jewish origin, who established the school of phenomenology.",
    "Book Mentioned: The Crisis of European Sciences and Transcendental Phenomenology – Buy Here Phenomenology Phenomenology is the philosophical study of the structures of experience and consciousness.",
    "Robert Sokolowski Monsignor Robert Sokolowski is a philosopher and Roman Catholic priest who serves as the Elizabeth Breckenridge Caldwell Professor of Philosophy at The Catholic University of America.",
    "Noesis Nous, sometimes equated to intellect or intelligence, is a term from classical philosophy for the faculty of the human mind necessary for understanding what is true or real.",
    "Noema",
    "The word noema (plural: noemata) derives from the Greek word νόημα meaning \"thought\", or \"what is thought about\".",
    "Tom Sparrow Book Mentioned: The End of Phenomenology – Buy Here Maurice Merleau-Ponty Maurice Jean Jacques Merleau-Ponty was a French phenomenological philosopher, strongly influenced by Edmund Husserl and Martin Heidegger.",
    "Book Mentioned: Phenomenology of Perception – Buy Here Speculative Realism Speculative realism is a movement in contemporary Continental-inspired philosophy (also known as post-Continental philosophy) that defines itself loosely in its stance of metaphysical realism against its interpretation of the dominant forms of post-Kantian philosophy (or what it terms \"correlationism\").",
    "Graham Harman Graham Harman (born May 9, 1968) is an American philosopher.",
    "He is Distinguished Professor of Philosophy at the Southern California Institute of Architecture in Los Angeles.",
    "Book Mentioned: Object-Oriented Ontology: A New Theory Of Everything – Buy Here Timothy Morton Timothy Bloxam Morton is a professor and Rita Shea Guffey Chair in English at Rice University.",
    "Dasein Dasein is a German word that means \"being there\" or \"presence\", and is often translated into English with the word \"existence\".",
    "Existentialism Existentialism is a form of philosophical inquiry that explores the problem of human existence and centers on the lived experience of the thinking, feeling, acting individual.",
    "Nihilism Nihilism is a philosophy, or family of views within philosophy, expressing negation (i.e., denial of) towards general aspects of life that are widely accepted within humanity as objectively real, such as knowledge, existence, and the meaning of life.",
    "Plato Plato was an Athenian philosopher during the Classical period in Ancient Greece, founder of the Platonist school of thought and the Academy, the first institution of higher learning in the Western world.",
    "Sati Sati is mindfulness or awareness, a spiritual or psychological faculty (indriya) that forms an essential part of Buddhist practice.",
    "Object-oriented ontology In metaphysics, object-oriented ontology (OOO) is a 21st-century Heidegger-influenced school of thought that rejects the privileging of human existence over the existence of nonhuman objects.",
    "Paul Tillich Paul Johannes Tillich was a German-American Christian existentialist philosopher and Lutheran Protestant theologian who is widely regarded as one of the most influential theologians of the twentieth century.",
    "Carl Jung Carl Gustav Jung, was a Swiss psychiatrist and psychoanalyst who founded analytical psychology.",
    "Owen Barfield Arthur Owen Barfield was a British philosopher, author, poet, critic, and member of the Inklings.",
    "Aletheia Aletheia is truth or disclosure in philosophy.",
    "Hubert Dreyfus Hubert Lederer Dreyfus was an American philosopher and professor of philosophy at the University of California, Berkeley.",
    "Book Mentioned: Being-In-The-World – Buy Here Book Mentioned: What Computers Can’t Do – Buy Here Book Mentioned: What Computers Still Can’t Do – Buy Here Charles Taylor Charles Margrave Taylor CC GOQ FRSC FBA is a Canadian philosopher from Montreal, Quebec, and professor emeritus at McGill University best known for his contributions to political philosophy, the philosophy of social science, the history of philosophy, and intellectual history.",
    "Book Mentioned:",
    "Retrieving Realism – Buy Here Henry Corbin Henry Corbin was a philosopher, theologian, Iranologist and professor of Islamic Studies at the École pratique des hautes études in Paris, France.",
    "Robert Avens Book Mentioned: The New Gnosis – Buy Here Erich Fromm Erich Seligmann Fromm (/frɒm/; German: [fʁɔm]; March 23, 1900 – March 18, 1980) was a German social psychologist, psychoanalyst, sociologist, humanistic philosopher, and democratic socialist.",
    "Stephen Batchelor Stephen Batchelor is a British author and teacher, writing books and articles on Buddhist topics and leading meditation retreats throughout the world.",
    "He is a noted proponent of agnostic or secular Buddhism.",
    "Other helpful resources about this episode: Notes on Bevry Additional Notes on Bevry Ep.",
    "46 - Awakening from the Meaning Crisis - Conclusion and the Prophets of the Meaning Crisis Welcome back to Awakening for the Meaning Crisis.",
    "So last time, I attempted to draw together all of the other theories, and I don't mean just the psychological theories, although they're most salient right now, but also the philosophical theories into an account of wisdom.",
    "I presented a model to you, a theory of a wisdom developed by myself and Leo Ferraro from 2013, in which we are enhancing inferential processing through active open-mindedness enhancing insightful processing through mindfulness.",
    "We're enhancing the capacity for internalization by internalizing the sage and cultivating sophrosyne.",
    "Our salience landscape naturally organizes away from self-deception and tempts us towards the truth.",
    "What's true, good, and perhaps beautiful.",
    "That's perhaps a better way of putting it.",
    "And that that coordinates the propositional knowing associated with inference, the procedural knowing associated with insight, the perspectival knowing associated with internalization together.",
    "And that, as I said, that is directed towards realizing sophrosyne.",
    "And that that can help cultivate a more moral existence, the connection to virtue.",
    "Mastery in the sense of coping and caring and meaning in life.",
    "But of course, one of the criticisms I made was that the notion of meaning in life there was too simplistic and it needs to be integrated, with a much more developed account that's already in the literature and I'm contributing to that by work I'm doing with others on meaning in life.",
    "I pointed out that the Vervaeke and Ferraro model is missing participatory knowing.",
    "It's missing the relationship between, or at least I think it misrepresents, misaligns the relationship between the kinds of knowing.",
    "Understanding is missing.",
    "Transformational experience is—transformative experience is missing.",
    "Aspiration is missing.",
    "Gnosis is missing.",
    "So all of these things need to be deeply integrated together.",
    "I tried to give you an account of what I think—sorry, that's too, that's too grandiose.",
    "I tried to suggest the beginnings of an account of how we turn basic understanding, which is to grasp the relevance of our knowledge into profound understanding by integrating the account of understanding with the account of plausibility.",
    "So that profound understanding is the generation of plausibility by having convergence onto a contextually sensitive optimal grip that is transformatively transferable in a highly effective manner in problem finding and many different problem finding formulating and solving in many different domains.",
    "I also brought out the idea that in addition to inspiration, this is a term I'm giving for sudden, more sudden insight-laden transformative experience.",
    "You can have as what Callard calls, aspiration.",
    "It's more incremental.",
    "It's still can't be solved in an inferential decision theoretic fashion.",
    "She agrees with Paul on that.",
    "She does argue though, and I agree with this argument, that aspiration must be considered a form of rationality, which he calls proleptic rationality, because you're going to get into a performative contradiction.",
    "If my aspiration for rationality and my love of wisdom are not themselves rational processes, I'm kind of in trouble in my model of rationality.",
    "Then, given all of that philosophy, what's missing as I argued is an extensive psychology of aspiration.",
    "I know one of my colleagues, Jeunsung Kim is working on exactly that problem and he's, of course, doing it in connection with a psychology of wisdom.",
    "I did suggest to you that we could see one of Callard's ideas of how we do this by we create something that's double-faced.",
    "I argued, ultimately, symbolic.",
    "Having aspects of gnosis in it that allows us to make the jump, the leap, even if it's an incremental one from who we are now and what we value now to the place where I've acquired some new thing that I value for its own sake.",
    "We use the example of music appreciation, et cetera.",
    "But unlike Callard, I see that as inherently relying on our symbolic capacity, our capacity for enacted symbolic behavior.",
    "What I call gnosis serious play.",
    "And because I mean in serious play, it's always like when I'm playing with something, I'm treating it as, for example, a sword, but it's actually a piece of plastic.",
    "That kind of serious play that symbolic ability, especially the inactive one that gives me anagoge and analogia.",
    "That's really important, I would also argue, for aspiration.",
    "At least for the placeholders that do important work within aspiration.",
    "I also argued that aspiration also probably has an affective component to it.",
    "And I suggested that wonder—and we saw how central wonder is to the cultivation of wisdom—that wonder is the affective state that's most conducive to aspirational progress because of the way it opens up our identity and our world triggers the transjective relationship and puts it into—that participatory knowing puts it into a developmental trajectory.",
    "So all of that needs to be integrated together into an account of wisdom.",
    "And then I suggested to you drawing it all together is that wisdom is an ecology of psychotechnologies slash cognitive styles.",
    "That dynamic—and that means, reciprocal optimization that dynamically enhances the relevance realization that's central to inference, to insight and intuition to internalizing, especially internalizing the sage, to understanding gnosis and related to gnosis to the relevance realization at work within transformative experience and aspiration.",
    "And then I already noted to you that that enhancement, that way in which I'm talking about wisdom, that dynamic system, that ecology is already overlapping and as it should.",
    "As it should.",
    "It is overlapping with the account we gave of enlightenment where enlighten—a crucial element of enlightenment was to create a counteractive dynamical system that counteracts parasitic processing.",
    "And I'm showing you that I think of wisdom.",
    "I'm arguing that wisdom is a kind of dynamical system that is counteractive for overcoming self-deception and therefore would be counteractive for overcoming parasitic processing and foolishness.",
    "This is a processing account.",
    "get the notes!",
    "You can see, I think, given what I've just said, how it would ameliorate foolishness, we've already talked about how it might enhance flourishing by—and we did that in connection with Sternberg, how it's going to help you be better connected to yourself, to other people and to the world.",
    "But I would argue that, especially where it's overlapping with enlightenment, that what wisdom is doing in order to enhance meaning in life is it's enhancing religio.",
    "So we've got wisdom here, right?",
    "(Fig 1a) (draws an oval and writes Wisdom inside)",
    "And I'm sort of saying there's a significant overlap with enlightenment (draws an oval overlapping the first oval and writes Enlightenment).",
    "And one of the things that wisdom is doing that's also really important is it's enhancing religio (writes Religio below Wisdom).",
    "That's a way in which you can powerfully—and we saw already the connection.",
    "Remember Ardelt?",
    "The connection to agape, it's enhancing religio.",
    "And I would say that the enhancement of religio is already—and remember the role of wonder, potentially even awe here?",
    "It's taking us into sacredness (writes Sacredness below Religio), the notion of sacredness that I already have argued and articulated for.",
    "Fig.",
    "1a",
    "And so given this connection, right?",
    "So this is how it's enhancing meaning in life (Fig. 1b)",
    "(draws an arrow from Religio and writes Meaning in life beside Religio).",
    "And of course, this is also an enhancement (draws a double-headed arrow from Meaning in life to Sacredness).",
    "These are all connected is what I'm saying.",
    "It's enhancing sacredness.",
    "I think it's plausible therefore to argue not to conclude decisively, but to argue that I've shown, I've explicated and explained the deep connections between wisdom, enlightenment, the enhancement of religio and thereby the relationship to sacredness (erases the board).",
    "Fig.",
    "1b",
    "Wise Cultivation Of Enlightenment I want to draw that all together in this notion of the wise cultivation of enlightenment (writes W.C.E).",
    "The wise cultivation of enlightenment.",
    "What that carries with it, of course, the enhancement of religio, the encountering with the sacred, the enhancement of meaning in life, et cetera.",
    "The wise cultivation of enlightenment.",
    "I think if the wise cultivation of enlightenment is situated within two things...",
    "If it's situated within a worldview that affords worldview attunement (Fig. 2a) (draws an arrow from W.C.E. and writes Worldview), if it's situated within a worldview—and I've tried to do that by throughout consistently, I believe, at least, making this account consistent with a scientific worldview by running it all off the machinery of relevance realization that can ultimately be given a naturalistic explanation.",
    "So and we've already argued how 4E cognitive science, third-generation cognitive science can give us this worldview.",
    "And notice how much the discussion of wisdom was invoking a lot of the theoretical machinery that we got from third-generation 4E cognitive science was all through it.",
    "Fig.",
    "2a",
    "Okay.",
    "So that (indicates W.C.E) is situated into a, basically an enabling and encouraging worldview.",
    "And it is also situated within some of the things I suggested where we have a co-op network of communities of practice (Fig.",
    "2b) (writes Co-op network of communities of practice).",
    "[...] I already talked about what that is and that that is in a reflective equilibrium, dynamic, ongoing one with a wisdom Wiki (draws an arrow from Co-op network and another pointing to Co-op network, writes Wisdom wiki).",
    "And this has both a top-down like there's researchers (writes Researchers below Wisdom wiki), the researchers in wisdom that I've talked about here and then drawn from here, of course, is we want to talk about the practitioners (draws an arrow from the Co-op network of communities of practice and writes Practitioners).",
    "People are practicing.",
    "Fig.",
    "2b",
    "And of course, this is a bottom-up top-down relationship (Fig. 2c) (draws a double-headed arrow between Researchers and Practitioners).",
    "So (erases the arrow between Co-op network to Practitioners)",
    "we have the practitioners, we have the researchers, top-down, bottom-up relationship, and they are in that fashion contributing to the wisdom wiki (draws an arrow pointing to Wisdom wiki).",
    "The wisdom wiki is acting—it's taking on a credo function, right?",
    "But it's always in service of religio and therefore it's being created in a large part by these communities of practice.",
    "Fig.",
    "2c",
    "Now I think if you put this all together... here it is!",
    "This is how I think we can awaken from the meaning crisis.",
    "I think that we can draw all of the machinery together for overcoming the perennial problems dealing with historical issues.",
    "How to connect wisdom and enlightenment together in a comprehensive fashion and to connect that with enhancing and meaning in life and overcoming self-deception, et cetera, all of that machinery, and then situating it within this kind of socio-cultural framework.",
    "I think that's how we can individually and collectively awaken from the meaning crisis.",
    "And why I want to one more time—and that's what's part of this (indicates Worldview)—emphasize that all of this has been explained and can be engineered from within a secular scientific worldview.",
    "It's not a view that is in any way, I think, hostile to religion.",
    "I am genuinely and sincerely respectful throughout.",
    "But it is a way that is not dependent on religion, nor is it dependent on a political ideology.",
    "A lot of this, this part of it (indicates Co-op network) is already nascent.",
    "It's already coming into existence.",
    "We have some existing examples of this that are being developed.",
    "We can apply it.",
    "I've tried to give you an account of this (indicates W.C.E.), and I've tried to give you an account (indicates Worldview) of how the cognitive science really situates us within a scientific worldview.",
    "get the notes!",
    "Roadmap Of Central Prophets Of The Meaning Crisis What I now want to do is to put this into, I hope, a constructive dialogue with other responders to the meaning crisis, I would even call them prophets in the Old Testament sense of people that were telling forth the meaning crisis, trying to awaken us to it and trying to galvanize us in response to it.",
    "So what I want to do is take everything that I've done.",
    "It's summarized by this schema (indicates Fig. 2c), right?",
    "I hope it's not oversimplified by the schema.",
    "It's summarized.",
    "And I want to put this into dialogue.",
    "I hope like I said, constructive dialogue with some of the central prophets of the meaning crisis, especially in the 20th and 21st century.",
    "Now, inevitably I cannot do everybody.",
    "Your favorite philosopher might not make it here.",
    "Both for lack of time and lack of expertise, I'm not going to talk for example, very much about Wittgenstein.",
    "Although I think he's important.",
    "I've taken a lot of undergraduate courses, graduate courses, read a lot of Wittgenstein.",
    "Wittgenstein has deeply influenced me the notion of cultural cognitive grammar is Wittgensteinian through and through in important ways.",
    "But trying to connect Wittgenstein to the meaning crisis is not something that I feel I have the requisite expertise.",
    "Whitehead is a philosopher I am currently again, trying to understand.",
    "He is somebody who is wrestling very deeply with the meaning crisis and trying to come up with a way of resituating us within a scientific worldview.",
    "I've read quite a bit.",
    "I'm reading quite a bit.",
    "I'm not confident yet about that.",
    "Whitehead, of course, has been terrifically relevant to process theology in some of the new theological innovations in the 20th century.",
    "And part of that could be, I've seen, I could argue, I think many people could argue, is it a way of—that theological innovation is designed to respond to the meaning crisis.",
    "There's other people, those are just two clear examples.",
    "But what I want to offer is instead I've chosen the people I've chosen for two reasons.",
    "I've chosen them because I think I have some relevant knowledge, relevant expertise to bring to bear.",
    "And secondly, because they form a network they're—I don't want, I don't need to present them sort of a piecemeal fashion.",
    "They have relations of contrast, connection, causal influence with each other.",
    "So there's a network of people.",
    "I want to talk about that are in a sense harbingers of the meaning crisis.",
    "But again, in that prophetic sense, they're trying to awaken us and arouse us to respond.",
    "So what I want to do is, first of all, put up what that network is going to look like to give you an overarching roadmap of where we're going (erases the board).",
    "Overarching roadmap of where we're going.",
    "And then what I'm going to be doing throughout is presenting that material.",
    "And then as I said, trying to put it into constructive dialogue with the argument I have made.",
    "My attempt is not to sort of say that they are all—my account is better or they're all just saying what I was saying.",
    "But what I want to show is that the account I've made can be, I can argue, that it is deeply responsive and responsible to the work of these prophets of the meaning crisis.",
    "So the figure that it's—and he's a controversial figure.",
    "And I think it's fair to say my philosophical attitude towards him is one of ambivalence, but sort of a pivotal figure in this is Heidegger (Fig.",
    "3a) (writes Heidegger).",
    "So I want to take a look at Heidegger.",
    "Right now, I'm not going into discussing these people.",
    "I want to draw out some important connections.",
    "We'll have to go behind Heidegger a little bit and talk about Husserl (writes Husserl) and phenomenology.",
    "That's important.",
    "Heidegger is, I would argue, also deeply influenced by sort of the gnosis (writes Gnosis) underground running through Germany, especially in the 20th century and especially, like between the Wars and that comes through, at least explicitly, you can see this in John Caputo's book on The mystical element in Heidegger's thought.",
    "This comes through the Rhineland mystic of Meister Eckhart (draws a line from Gnosis and writes Eckhart), has a huge impact on Heidegger (draws an arrow connecting Eckhard to Heidegger).",
    "So those are definitely important aspects, important influences.",
    "Fig.",
    "3a Another, of course, titanically important 'cause he just influences everybody is Kant (Fig. 3b) (writes Kant and draws an arrow pointing to Heidegger) and behind Kant, of course, is Descartes (writes Descartes above Kant).",
    "Fig.",
    "3b",
    "So many of these people I'm not going to talk about at length because I've already talked, but I'm trying to map this out because I'm trying to show you what I'm going to invoke and then what I'm going to discuss in order to try and draw this all together.",
    "So another really important figure, and you've heard me mention him several times..",
    "And he directly gets into that connection with theology, but he is one of the, I think, one of the great writers about the meaning crisis.",
    "It's, of course, Paul Tillich and his masterpiece, The Courage To Be is all about, you know, a prophetic announcement of the meaning crisis and an attempt to seriously revise, you know, theology, to take that into account.",
    "And there's also, and it's, I think both this way (indicates Gnosis and moves his hand down the diagram), and it's not clear if it's also independently, but there are Gnostic element, deeply Gnostic elements.",
    "So I'll put that in sort of a dotted line (Fig. 3c) (draws a dotted line from Gnosis to Tillich) in Tillich, Tillich famously argues that we need to get to the God beyond the God of theism, which is about a Gnostic, a statement as you could possibly make.",
    "Fig.",
    "3c",
    "So Heidegger also has a lot of influence on somebody you've heard me mention, especially with ideas of transjectivity, but he (Fig. 3d) (writes Corbin) has a lot to say about symbolism and the meaning crisis, and this is Corbin.",
    "And so I'm going to have to talk a lot about Corbin, because I haven't discussed him at length, but his work is again, very pivotal in trying to respond to the meaning crisis.",
    "The work that Cheetham has done in his trilogy of books, or maybe there's four books maybe.",
    "But I've read three—well, I've read two.",
    "And I'm currently reading the third.",
    "Books on Corbin really helped to make a good case for how important it is.",
    "Many people don't know about, for example, the deep connections between somebody else that we're going to talk about.",
    "Corbin and Jung (writes Jung below Corbin).",
    "And of course, Jung is directly influenced by the Gnostics (draws an arrow from Gnosis to Jung) and directly influenced by Kant (draws an arrow from Kant to Jung).",
    "Fig.",
    "3d This allows me to bring out another important connection.",
    "Which is the work of Dourley (Fig. 3e) (draws a bracket between Jung and Tillich and writes Dourley) because what he does, which is really impressive, is he shows the deep similarities between Tillich and Jung.",
    "No doubt because of their sharing—well, I'm arguing—the sharing of the Gnosis background.",
    "I would also argue that both Tillich and Jung are in an important sense, nontheists.",
    "And we'll talk about what nontheism is as we get into that.",
    "But one of the core shared idea, and this is actually the title of one of Dourley's book, is the psyche as sacrament.",
    "Both Tillich and Jung view the psyche in a sacramental fashion.",
    "And that is part of the way in which they attempt to respond to the meaning crisis.",
    "Both of them have profound things to say about symbols and the relationship to the spiritual life broadly construed.",
    "So we're going to talk about Jung.",
    "Fig.",
    "3e Now somebody that's also here directly influenced by Gnosis (Fig.",
    "3f)",
    "(draws an arrow pointing away from Gnosis), influenced by Kant through the romantics (writes Romantics below Kant), which we've talked about, at least the early German romantics.",
    "People like, Schlegel, for example.",
    "And therefore, through Coleridge, right?",
    "And the person I'm drawing in here, this is going to be Barfield (draws a line from Romantics and writes Barfield).",
    "And you've heard me, mentioned him a couple times.",
    "And Barfield's notions of participation have a lot to say.",
    "So you can see how there's sort of a network here.",
    "Fig.",
    "3f",
    "And then there's another one that, of course, we need to talk about.",
    "And this is the connection between Heidegger and what's been sort of a—I don't like this term, but, and people abuse it as postmodernism.",
    "As if everybody who's a postmodernist were saying the same thing.",
    "We should more carefully look at individual thinkers and their individual arguments.",
    "get the notes!",
    "One potential connection here is, we'll take a look at it, Derrida (Fig. 3g) (draws a line from Heidegger and writes Derrida).",
    "Now, whether or not we should call these, these two other people, postmodern is not clear.",
    "They are deeply responding to postmodernism and that's Graham Harman (writes draws an arrow from Heidegger and writes Harman) and the terrific work of Timothy Morton are also doing.",
    "And this is what's known as speculative realism (writes Speculative realism below Harman).",
    "It's also known—Harman's particular version of it is known as triple O (writes OOO below Speculative realism).",
    "This stands for object-oriented ontology.",
    "This is the attempt to deeply bring back a profound kind of realism and contact with reality.",
    "Another person that's influenced by [Heidegger] (draws a line from Heidegger) is Han (writes Han).",
    "Current philosopher and cultural critic.",
    "Fig.",
    "3g We'll talk a little bit about...",
    "I can't give all of these people equal work.",
    "I'm going to talk about Barfield, Jung, Corbin, and Tillich, and Heidegger quite extensively.",
    "Harmon, not as much.",
    "Han, not as much.",
    "Derrida, not as much, but I'll at least touch upon them.",
    "Okay.",
    "Because I want to do this as a way of trying to connect the meaning crisis to what's been called postmodernity broadly, very broadly construed.",
    "And offer an alternative response to postmodernism to both sort of wholesale adoption of it.",
    "Or the wholesale demonization of it.",
    "I think these are both overreactions that we should have a much more nuanced and careful response to.",
    "Now Heidegger has a huge influence in an area many people don't know about (Fig. 3h)",
    "(draws an arrow away from Heidegger).",
    "And part of this influence is also James (writes James).",
    "And part of this influence is also Buddhism (writes Buddhism beside James).",
    "And you've heard me mention this, and this is the Kyoto school (writes Kyoto below James and Buddhism), and they are deeply about responding to the meaning crisis and especially the work of Nishida (writes Nishida below Kyoto) who really is the pivotal figure in founding this.",
    "Fig.",
    "3h",
    "And then the person who I think wrote one of the masterpieces on responding to the meaning crisis.",
    "And this is Nishitani (writes Nishitani below Nishida).",
    "His book, Religion and Nothingness.",
    "I've read that book twice.",
    "I would put that book in the top five books of responding to the meaning crisis.",
    "It is not an easy book, but that's why I've not yet put it into like the Twitter book recommendations.",
    "I've recommended the Kyoto school and some of Robert Carter's excellent work, introducing you to these people.",
    "Directly reading Nishitani is very difficult.",
    "You need to know Heidegger well, James well, Buddhism well.",
    "And then the Kyoto school people, I won't talk about like Maso Abe.",
    "They put this into dialogue with people like Tillich and Whitehead.",
    "So this is what I want to try and address.",
    "Again, I will be giving some of these people much more priority.",
    "I only have four lectures left after this, so four hours.",
    "So I'll be giving some people much more priority than others.",
    "Obviously, Heidegger is taking a central role here.",
    "But we'll be spending quite a bit of time with Barfield Jung, Corbin and Tillich, and with Nishitani.",
    "Okay.",
    "So those are the ones that I'm going to give priority to.",
    "These other people, I'll try to do my best to represent them, but I will have to prioritize.",
    "I want to keep my commitment to you that I finish this in the 50 episodes.",
    "I also do not want to drag my video crew to some kind of a video version of a death march until they are exhausted beyond all recognition.",
    "So where to start on here?",
    "Well, I'm going to start at the center.",
    "So this is very complicated, and this is not meant to be an explanatory schema.",
    "This is meant to be a roadmap to show you where I'm going.",
    "How are things connected?",
    "You can take this down.",
    "And like I said, then you can use this to retrace the connections as I try to explicate and explain them (erases the board).",
    "Husserl: Phenomenology Alright.",
    "So before we get to Heidegger, we have to talk about, I don't have to talk about Eckhart and the Rhineland mystics, because I've already done that.",
    "Or gnosis cause I've already done that quite extensively.",
    "Or Kant because I've done that.",
    "But one person that I haven't discussed and by his titanic influence on Heidegger is Husserl.",
    "Edmund Husserl.",
    "And Husserl (writes Husserl), of course, is famous for founding a whole philosophical movement called phenomenology (writes Phenomenology beside Husserl).",
    "Existentialism comes out of phenomenology via Heidegger, by the way, that's how you get existentialism.",
    "Heidegger does something to phenomenology and it leads to existentialism.",
    "But it also leads to deconstructionism postmodernism, blah, blah, blah, blah.",
    "I've already pointed out.",
    "Now again, you need an entire course to get clear about what phenomenology is.",
    "I really recommend the introduction to phenomenology by Sokolowski (Fig. 4a) (writes Sokolowski beside Phenomenology).",
    "Sokolowski and the book experimental phenomenology by Don Ihde (writes Ihde below Sokolowski).",
    "This sort of gives you an idea of what phenomenology is.",
    "A very good, very clear i—and this gives you a sort of way of practicing some phenomenological techniques to get a more inside feel of what phenomenology is like.",
    "Fig.",
    "4a Now phenomenology—and Husserl even writes a book where he invokes the word crisis—crisis in European sciences.",
    "Phenomenology was Husserl's attempt to try and I would argue the following thing.",
    "It's an attempt to try and get us back to a contact epistemology.",
    "And that's why you can see people who are deeply influenced by the phenomenological tradition, like Dreyfus and Taylor talking about the loss of a contact epistemology, because they're aware of the idea of a contact epistemology, I would argue, from their phenomenological heritage.",
    "Husserl famously argued about getting back to things, getting back to the things.",
    "That we had gotten so abstracted and removed, we had lost contact with the world.",
    "We were out of touch in a profound way, and that's why phenomenology has had such a big influence on those aspects of cognitive science that are trying to show how deeply embedded, embodied and connected we are to the world.",
    "So the attempt to get back at contact epistemology was really central.",
    "And one way of understanding that contact (Fig. 4b) (writes Contact under Husserl), and to put it into dialogue with the language we've been using in this course, is that you get this contact by—see, phenomenology shouldn't be confused with merely introspecting.",
    "That's the everyday or common sense stance.",
    "The phenomenological attitude is not the same thing as your commonsensical everyday introspection.",
    "Instead, phenomenology is a much more disciplined practice in which you're trying to pay reflective and, following Ihde, a kind of an experimental attention, this probative attention to the way in which we are in contact with the world.",
    "Fig.",
    "4b So let's say that what I'm doing in phenomenology is I'm playing (Fig.",
    "4c) (draws an arrow from Phenomenology to Contact) this reflective, experimental, exploratory, probative attention (writes Reflective/ Experimental/ Exploratory/ Probative attention).",
    "Attention to contact.",
    "And then how can we understand contact?",
    "Well, what Husserl emphasized, he emphasized intentionality (writes Intentionality below Contact).",
    "That's one pole of this relation.",
    "Okay.",
    "Now I've got to explain this.",
    "Normally when we use this word, it's correct, but it's a species of the broader sense of intentionality—when you say you do something intentionally, it means you're doing it on purpose.",
    "Intentionality here in phenomenology and philosophy, in general, is much more broader.",
    "It means any mental directedness, any mental directedness.",
    "So when my perceptions are of the bottle or my actions are towards the book.",
    "Or I'm thinking about Paris.",
    "Those are all intentionality and that I have a mental directedness.",
    "Fig.",
    "4c",
    "So there's the intentionality.",
    "And it's in this reciprocal relationship with the way in which a world is disclosed (Fig. 4d) (draws a double-headed arrow beside Intentionality and writes World disclosure), where world doesn't mean a planet.",
    "It means something that we've been talking about throughout.",
    "It means a meaningfully structured environment.",
    "A meaningfully structured environment, what I've tried to often capture by this notion of an arena.",
    "This (indicates Intentionality) is kind of a core kind of agency.",
    "This is at least mental agency, this mental directedness.",
    "So, and this is important.",
    "This is—Heidegger (text overlay appears saying, 'John means Husserl instead of Heidegger') used the word noesis (writes Noesis below Intentionality and draws a double-headed arrow beside it) here for this and the noema is here and there's of course, all kinds of debates about what does this refer to.",
    "Just something in my consciousness or something in the world.",
    "Fig.",
    "4d We're going to come back to some of this.",
    "We're going to come back to it when in order to get to Harman.",
    "And we are going to look at Sparrow's critique of phenomenology.",
    "Cause he's going to argue that the very goal of phenomenology, which was to get back into contact with reality and therefore to be a kind of realism, is thwarted by, this setup.",
    "So he's going to argue that phenomenology ultimately fails as a form of realism and that it's ultimately a kind of idealism and therefore it doesn't give us what it purported to do.",
    "We'll come back to that.",
    "That's just a forewarning.",
    "What I would argue using this is what phenomenology",
    "[is] using—what I mean by this is the language and some of the conceptual vocabulary and theoretical grammar we've developed together—using that, I would describe phenomenology as this reflective, experimental, exploratory, [...] a probative attention on the transjective relationship.",
    "And the fact that he's invoking this term, remember noesis, right?",
    "This is perspectival knowing.",
    "So putting it together, it's this reflective—meaning all of these (indicates Reflective/ Experimental/ Exploratory/ Probative attention).",
    "It's this reflective attention paid to your perspectival knowing of the transjective relationship.",
    "And in that sense, it's deeply consonant with, and that's no coincidence this has had a huge influence on me and my thinking and many of the people in third-generation cog sci, 4E cognitive science have been deeply influenced by this because they're trying to understand meaning in this transjective way, making sense.",
    "get the notes!",
    "Heidegger: Criticism Of Husserl Now, as I said, we're going to come back to criticize that, but right now—in order to make the bridge to Harman.",
    "But what I want to do now—and to speculative realism.",
    "What I want to do now is what did Heidegger do with this?",
    "(indicates Fig. 4d)",
    "What did Heidegger do with this?",
    "Well, one way of putting this, that I think draws together, two of his criticisms is to...",
    "Well, no, no, maybe a better way.",
    "Let's do the two criticisms then draw them together.",
    "One of Heidegger's main criticism is that Husserl's work, and this is going to be developed by Sparrow and the speculative realists, is that Husserl's work had not really given us contact.",
    "Now one way I would put it is because it had not really developed an account of participatory knowing.",
    "It had not really developed an account of how the agent and the arena were fundamentally related together so that this perspectival relationship could unfold.",
    "Now, I don't think that's the case for all of phenomenology.",
    "I would make the case that Meleau-Ponty's—and we talked about this.",
    "Merleau-Ponty's ideas about embodiment and embeddedness are trying to get at the connection between the perspectival knowing of Husserlian phenomenology and the participatory knowing.",
    "Nevertheless, what Heidegger was innovating, and this is how he was bringing in an existential aspect.",
    "He was trying to point out that the modal relationship between the agent and the arena, using our terms, was not properly accounted for within the Husserlian framework.",
    "So participatory knowing was deeply missing.",
    "And that's sort of our fundamental way in which we're connected, in contact with being.",
    "And then in connection with that.",
    "That that participatory knowing had not been set within an ontology that this (indicates Intentionality, World Disclosure, Noesis, Noema) needed to be set within participatory knowing (Fig. 4e)",
    "(draws an arrow from Noesis and Noema and writes Participatory knowing).",
    "Existential modes.",
    "Existentialism comes out of Heidegger.",
    "And that in turn needed to be set into a proper ontology (writes Ontology beside Participatory knowing) and a proper account of the structure of being.",
    "To use our language.",
    "How does the transjective relationship sit within an overall account of the structure of being itself?",
    "Fig.",
    "4e",
    "If we don't have that, then we don't have a con—we have not really got contact back.",
    "Cause we're still out of touch with our being and through our being of being itself.",
    "We're out of touch with our being and through our being of being itself.",
    "This is Heidegger's main criticism.",
    "A related criticism that is pervasive—although for a long time implicit and I don't like the way Heidegger, of course, eventually turned on Husserl for despicable reasons.",
    "But one of the criticisms Heidegger was making is Husserl was still trapped within the Cartesian cultural cognitive grammar.",
    "Husserl is deeply Cartesian.",
    "He entitles one of his books, Cartesian Meditations.",
    "So in that sense, Heidegger feels that Husserl is still bound within the Cartesian grammar.",
    "And he sees that that Cartesian cultural cognitive grammar, for ways we've articulated, radically cuts us off from the world.",
    "And this is, of course, a way of saying that we're still sort of trapped within our subjectivity.",
    "And in that sense, we'll see what Sparrow means by Husserlian phenomenology is maybe still understandable as a kind of idealism, which doesn't get us back to realism as much as the phenomenologist claim and wanted it to do so.",
    "So, how do we get to this deeper contact?",
    "In an organized fashion, how do we do those two things?",
    "How do we open up participatory knowing situated within an ontology and break free from the Cartesian, ultimately, he would say platonic—I think that's incorrect, maybe Aristotelian—but how do we escape the strictures, the restrictions of the Cartesian cultural cognitive grammar that keep us out of contact with reality?",
    "So, what we need to do is phenomenologically, not just theoretically, but phenomenologically, within participatory knowing by transforming it in a reflective, experimental, exploratory, probatory way, our attention, right?",
    "By directing our attention to this fashion.",
    "I'm opening up our perspectival knowing.",
    "So we're going to phenomenologically realize, but we're going to direct that phenomenological realization towards something important.",
    "We're going to direct it towards our being.",
    "Who and what we are.",
    "That's how we're going to connect to the participatory knowing.",
    "What's that going to do?",
    "Well, we're going to have a phenomenological realization, which, of course, is then going to become an existential realization.",
    "That we are the beings whose being is in question.",
    "We are the beings whose being is in question.",
    "Now, if you heard the word being and you think, well, you're a homosapiens and you've got DNA.",
    "That's not what's being meant here.",
    "Okay.",
    "That's not what's being met here.",
    "So be patient because it's what's grounding this (indicates Noesis and Noema) that is our being, I'm not talking about a biological phenomenon, at least in any direct sense.",
    "Although I think it's ultimately grounded biologically, right?",
    "We're talking about what grounds this Husserlian framework in a participatory knowing.",
    "What does it mean to say we are the being whose being is in question?",
    "Well, you can even get a sense of this from the term existentialism.",
    "Remember we talked about this.",
    "That existentialism says human beings don't have an essence like other creatures, other things.",
    "So, you know, a gazelle is born.",
    "It is a gazelle.",
    "Its identity is set.",
    "It's going to develop into a gazelle.",
    "Right.",
    "But for us, at least in so far, I would argue, as we are persons, our—who and what we are, whether or not it makes sense to call it our essence, because essence should be a widely shared property—but our being then—maybe that's a better word to use right now—who and what I am as the person, John Vervaeke, that has been in question.",
    "I existed before who and what I am has come into being.",
    "And in fact, it's still unclear to me who and what I am.",
    "And that's also the case, so you are in question.",
    "You exist before you have an essence before you have who and what you are.",
    "That's what existentialism (writes Existentialism) ultimately points to.",
    "That your existence precedes your essence.",
    "That's one of the things existentialism takes out of Heidegger.",
    "But the key idea is that you are—this thing's being (lifts his water bottle) is not in question to it, or even ultimately to us.",
    "I mean, we can sort of do philosophy on it, but there's from within phenomenology, from within a phenomenological perspective—I'm sorry, phenomenological stance is perhaps a better word, we are in question to ourselves.",
    "Who am I?",
    "What am I?",
    "What is it to be a person?",
    "What kind of person am I?",
    "You see why this is relevant?",
    "This question goes fundamentally, it's bound up with this question: what is the meaning of my life in this—not in the sense of destiny, but like, how is my life meaningful?",
    "What makes it meaningful?",
    "What makes it meaningful to me?",
    "What is its meaning?",
    "Dasein I notice—and we've already talked about Heidegger's connection to this.",
    "Heidegger is trying to get you into something like an aporia.",
    "And he's trying to get you to remember the being mode, the mode in which you're not trying to manipulate even yourself and solve problems.",
    "You're stepping back and confronting mystery because you're engaged, not with controlling things and satisfying you're having needs, you're engaged in a process of development of becoming.",
    "And so you're confronting mystery 'cause you're going through transformative experience.",
    "You are the being whose being is in question and he's trying to wake you up, not theoretically.",
    "Phenomenologically, he's getting you to phenomenologically realize that you are the being whose being is in question.",
    "That's what we all are.",
    "So instead of referring to us as persons or human beings or things like that, he crashed this new word.",
    "Heidegger is famous for neologisms.",
    "And part of what he's trying to do with the neologisms is break us out of the familiar terms and thereby break us out of the Cartesian grammar.",
    "He comes up with this word Dasein (writes Dasein), which means being there.",
    "It has to do with, you know, I exist.",
    "Being there, being there I'm sort of flowing into the world, throwing into existence.",
    "And my being is in question.",
    "That dasein, being there.",
    "So what's interesting is my participatory knowing, the way in which I try to connect to how I'm situated in being, has an aporetic element to it.",
    "Aporia, right?",
    "In which I realize that central to me, according to Heidegger, is that my being is in question.",
    "Now if my being, that's my participatory knowing, my groundedness in being is in question, that is how I can link participatory knowing to ontology itself.",
    "Do you see?",
    "Because—and this is why it's participatory knowing—my self-knowledge will also get me into my knowledge of ontology because by knowing my self as the being whose being in question, I can put ontology itself into question.",
    "I can put, —sorry, no, not—sorry, I can frame an ontological question of putting being itself into question.",
    "I know my self as a being whose being is in question.",
    "And knowing my self that way is also to put being into question.",
    "Right?",
    "And so I got this deep participation in the code determining mysteries of who I am and what being is.",
    "So by phenomenologically exploring that being, the being of dasein, we can simultaneously come into contact with our modal existence.",
    "We can remember the being mode.",
    "We can be opened up to the wonder of our own being that we are ultimately self-defining.",
    "At least in some important aspect, we're ultimately self-defining self-making things.",
    "But, of course, we don't do that egotistically or egocentrically because we're bound to the world for reasons I've already given you multiple times.",
    "So by phenomenologically exploring the being of dasein, we can simultaneously come into contact with our own modal existence in the mystery of being itself.",
    "So this is starting to take us into the core of how Heidegger is trying to deepen what he thinks was missing.",
    "Ultimately the contact with ourselves, but not with our autobiographical ego, but with our being.",
    "The contact, our participatory knowing of ourself, the connectedness to being, and he's found this, I mean, it's, that's why he's Heidegger.",
    "There's brilliant insight that because we are the being whose being's in question, we can deepen the contact by phenomenologically exploring this (indicates Dasein).",
    "So that's really central to what Heidegger is trying to do.",
    "get the notes!",
    "Reading Heidegger is very hard because it's filled with all this neologisms.",
    "It's filled with all of this constant—qualifying this constant self-criticism, which, of course, is good.",
    "There's constant refinement, but also this constant acknowledgment statement that we're not quite getting it.",
    "We're not getting the answer.",
    "So it's like going on a walk through a really gnarled forest with somebody.",
    "This is a metaphor that Heidegger himself would use.",
    "And you get a sense of progress, but it's not clear if you're actually making progress.",
    "You come into clearings and you get openings and insight, but then there's also, but yet we haven't arrived.",
    "We haven't arrived until you go on again.",
    "And so there's this long process.",
    "So what I want to do instead with you is, instead of trying to do something audacious to try and summarize—I want to try and go in and get some, I'm going to do some exposition with you.",
    "I want to read some key quotes from Heidegger and what I then want to do is try and unpack them, following this idea (indicates Fig. 4e) of how he's trying to deepen contact.",
    "Trying to put into dialogue with the very tremendous help of Dreyfus with the machinery, the theoretical machinery we have developed.",
    "And that will also afford me a critical response to Heidegger.",
    "I'm not going to start those quotes now because we're almost out of time.",
    "What I want to do is just foreshadow what Heidegger is going to argue.",
    "Heidegger is going to argue that the history of metaphysics, that whole philosophical history coming out of the axial revolution is actually the history of nihilism.",
    "This is why he is a prophet of the meaning crisis, that whole historical development, that framework, that cognitive, that cultural cognitive grammar that we've inherited from the actual revolution, that whole metaphysical framework has developed inexorably towards nihilism.",
    "It has driven us into the meaning crisis and, of course, that's already deeply resonant with the historical analysis that we pursued in the first half of this course.",
    "So if we can understand that cultural cognitive grammar of metaphysics, that's a pejorative term for Heidegger, we can link it to this project of the phenomenological investigation of dasein and break free from that grammar.",
    "And deeply re-establish, and this is not just theoretically, this is phenomenologically, existentially our contact with being and that deep participatory knowing and remembering of being and the realization of being and how it's not a being, not a particular being and our status with respect to being, that's the response Heidegger is recommending to the meaning crisis.",
    "And next time, what I would like to do with you is explore what his thinking is by going through the quotes, unpack it more.",
    "What links it to the argument that the history of metaphysics is the history of nihilism.",
    "See what we can glean in a cooperative dialogue between this course and Heidegger about responding to the meaning crisis and draw what important or at least relevant conclusions we can from that.",
    "Thank you very much for your time and attention.",
    "- END - Episode 46 Notes: To keep this site running, we are an Amazon Associate where we earn from qualifying purchases Ludwig Wittgenstein Ludwig Josef Johann Wittgenstein was an Austrian-British philosopher who worked primarily in logic, the philosophy of mathematics, the philosophy of mind, and the philosophy of language.",
    "Alfred North Whitehead Alfred North Whitehead OM FRS FBA was an English mathematician and philosopher.",
    "He is best known as the defining figure of the philosophical school known as process philosophy, which today has found application to a wide variety of disciplines, including ecology, theology, education, physics, biology, economics, and psychology, among other areas.",
    "Process theology Process theology is a type of theology developed from Alfred North Whitehead's process philosophy, most notably by Charles Hartshorne, John B. Cobb and Eugene H. Peters.",
    "Heidegger Martin Heidegger was a German philosopher who is widely regarded as one of the most important philosophers of the 20th century.",
    "He is best known for contributions to phenomenology, hermeneutics, and existentialism.",
    "Edmund Husserl Edmund Gustav Albrecht Husserl was a German philosopher of Jewish origin, who established the school of phenomenology.",
    "Book Mentioned: The Crisis of European Sciences and Transcendental Phenomenology – Buy Here Book Mentioned: Cartesian Meditations – Buy Here John Caputo John David Caputo is an American philosopher who is the Thomas J. Watson Professor of Religion Emeritus at Syracuse University and the David R. Cook Professor of Philosophy Emeritus at Villanova University.",
    "Caputo is a major figure associated with postmodern Christianity and continental philosophy of religion, as well as the founder of the theological movement known as weak theology.",
    "Book Mentioned: The Mystical Element in Heidegger’s Thought – Buy Here Meister Eckhart Eckhart von Hochheim OP, commonly known as Meister Eckhart or Eckehart, was a German theologian, philosopher and mystic, born near Gotha in the Landgraviate of Thuringia (now central Germany) in the Holy Roman Empire.",
    "Immanuel Kant Immanuel Kant was a German philosopher and one of the central Enlightenment thinkers.",
    "Paul Tillich Paul Johannes Tillich was a German-American Christian existentialist philosopher and Lutheran Protestant theologian who is widely regarded as one of the most influential theologians of the twentieth century.",
    "Book Mentioned: The Courage To Be – Buy Here Henry Corbin Henry Corbin was a philosopher, theologian, Iranologist and professor of Islamic Studies at the École pratique des hautes études in Paris, France.",
    "Tom Cheetham Book Mentioned: The World Turned Inside Out – Buy Here Book Mentioned: Imaginal Love – Buy Here Book Mentioned: All The World An Icon – Buy Here John Dourley John P. Dourley was a Jungians analyst, a professor of religious studies, and a Catholic priest.",
    "He taught for many years at Carleton University in Ottawa, his doctorate being from Fordham University.",
    "Book Mentioned: The Psyche as Sacrament – Buy Here Book Mentioned: Paul Tillich, Carl Jung and the Recovery of Religion – Buy Here Nontheism Nontheism or non-theism is a range of both religious and nonreligious attitudes characterized by the absence of espoused belief in a God or gods.",
    "Samuel Taylor Coleridge Samuel Taylor Coleridge was an English poet, literary critic, philosopher and theologian who, with his friend William Wordsworth, was a founder of the Romantic Movement in England and a member of the Lake Poets.",
    "Romantics Romanticism (also known as the Romantic era) was an artistic, literary, musical, and intellectual movement that originated in Europe towards the end of the 18th century, and in most areas was at its peak in the approximate period from 1800 to 1850.",
    "Romanticism was characterized by its emphasis on emotion and individualism as well as glorification of all the past and nature, preferring the medieval rather than the classical.",
    "It was partly a reaction to the Industrial Revolution, the aristocratic social and political norms of the Age of Enlightenment, and the scientific rationalization of nature—all components of modernity.",
    "Owen Barfield Arthur Owen Barfield was a British philosopher, author, poet, critic, and member of the Inklings.",
    "Jacques Derrida Jacques Derriad, born in Algeria, was a French philosopher best known for developing a form of semiotic analysis known as deconstruction, which he analyzed in numerous texts, and developed in the context of phenomenology.",
    "Graham Harman Graham Harman (born May 9, 1968) is an American philosopher.",
    "He is Distinguished Professor of Philosophy at the Southern California Institute of Architecture in Los Angeles.",
    "Book Mentioned: Object-Oriented Ontology: A New Theory Of Everything – Buy Here Timothy Morton Timothy Bloxam Morton is a professor and Rita Shea Guffey Chair in English at Rice University.",
    "Speculative realism Speculative realism is a movement in contemporary Continental-inspired philosophy (also known as post-Continental philosophy) that defines itself loosely in its stance of metaphysical realism against its interpretation of the dominant forms of post-Kantian philosophy (or what it terms \"correlationism\").",
    "Byung-Chul Han Byung-Chul Han is a South Korean-born Swiss-German philosopher and cultural theorist.",
    "Book Mentioned: The Scent Of Time – Buy Here Robert E. Carter Book Mentioned: The Kyoto School: An Introduction – Buy Here Book Mentioned: The Nothingness Beyond God: An Introduction To The Philosophy Of Nishida Kitaro – Buy Here Kitaro Nishida Kitarō Nishida was a prominent Japanese philosopher, founder of what has been called the Kyoto School of philosophy.",
    "Keiji Nishitani Keiji Nishitani was a Japanese university professor, scholar, and Kyoto School philosopher.",
    "He was a disciple of Kitarō Nishida.",
    "Book Mentioned: Religion and Nothingness – Buy Here Phenomenology Phenomenology is the philosophical study of the structures of experience and consciousness.",
    "Robert Sokolowski Monsignor Robert Sokolowski is a philosopher and Roman Catholic priest who serves as the Elizabeth Breckenridge Caldwell Professor of Philosophy at The Catholic University of America.",
    "Book Mentioned: Introduction to Phenomenology – Buy Here",
    "Don Ihde Don Ihde is an American philosopher of science and technology.",
    "Book Mentioned: Experimental Phenomenology – Buy Here Hubert Dreyfus Hubert Lederer Dreyfus was an American philosopher and professor of philosophy at the University of California, Berkeley.",
    "Charles Taylor Charles Margrave Taylor CC GOQ FRSC FBA is a Canadian philosopher from Montreal, Quebec, and professor emeritus at McGill University best known for his contributions to political philosophy, the philosophy of social science, the history of philosophy, and intellectual history.",
    "Book Mentioned:",
    "Retrieving Realism – Buy Here Noesis Nous, sometimes equated to intellect or intelligence, is a term from classical philosophy for the faculty of the human mind necessary for understanding what is true or real.",
    "Noema",
    "The word noema (plural: noemata) derives from the Greek word νόημα meaning \"thought\", or \"what is thought about\".",
    "Maurice Merleau-Ponty Maurice Jean Jacques Merleau-Ponty was a French phenomenological philosopher, strongly influenced by Edmund Husserl and Martin Heidegger.",
    "Book Mentioned: Phenomenology of Perception – Buy Here Speculative Realism Speculative realism is a movement in contemporary Continental-inspired philosophy (also known as post-Continental philosophy) that defines itself loosely in its stance of metaphysical realism against its interpretation of the dominant forms of post-Kantian philosophy (or what it terms \"correlationism\").",
    "Dasein Dasein is a German word that means \"being there\" or \"presence\", and is often translated into English with the word \"existence\".",
    "Aporia",
    "In philosophy, an aporia is a conundrum or state of puzzlement.",
    "Nihilism Nihilism is a philosophy, or family of views within philosophy, expressing negation (i.e., denial of) towards general aspects of life that are widely accepted within humanity as objectively real, such as knowledge, existence, and the meaning of life.",
    "Other helpful resources about this episode: Notes on Bevry Additional Notes on Bevry Ep.",
    "45 - Awakening from the Meaning Crisis - The Nature of Wisdom Welcome back to Awakening from the Meaning Crisis.",
    "So we are continuing, and it deserves this much attention, our long discussion about the nature of wisdom, because since the axial revolution it is just crucially connected to the project of meaning in life.",
    "Last time we finished up [with] a look at Baltes and Staudinger.",
    "I made some criticisms and that that led into important criticisms made by Monika Ardelt.",
    "And then we looked at Ardelt's theory and the way it brought in an important distinction about not just having a good theory of wisdom, but the process of becoming a wise person.",
    "And then the emphasis on what are the features of a wise person as opposed to what are some of the central claims made by a theory of wisdom.",
    "And then we talked about how Monika insightfully brings together the cognitive, the reflective, and the affective.",
    "And I pointed out how within, at least the cognitive directly, because of the invocation of Kekes and understanding, we've got relevance realization, grasping the significance.",
    "I would also point out that I think that's at least implicit in the reflective machinery and there's potential, deep potential connection there with both perspectival knowing and the cultivation of rationality, at least perspectival rationality.",
    "And the affective ties to agape, which I've already argued, has very important connections to relevance realization.",
    "And that affords Ardelt's theory a powerful way of connecting wisdom to meaning in life as something different from connecting wisdom to virtue.",
    "And that's a very important thing to do.",
    "We still noted some criticisms that largely it's still a product theory.",
    "It doesn't have an independent account of foolishness and a processing theory of how one becomes wise.",
    "And in that sense, it's not picking up, as well as it could, the philosophical heritage given to us by people like Socrates and Plato and Aristotle and Marcus Aurelius, et cetera.",
    "We then took a look at the theory of Sternberg just [an] extremely pivotal figure in the psychology, the cognitive science of wisdom.",
    "And we took a look at his theory and I pointed out his ideas about adapting, shaping and selecting are clearly ideas about relevance realization.",
    "He invokes implicit processing, tacit knowledge in order to bring understanding in.",
    "That sort of intuitive grasping of the significance of information, I think is what he's implying.",
    "We talked about how he involves a balancing of interests.",
    "And there's the interpersonal, how you're connected to yourself.",
    "The interpersonal, how you're connected to other people.",
    "The extrapersonal, how you're connected to the world.",
    "And so that's at least important connections to, implicitly at least, I mean, important connections to meaning in life that we've been talking about throughout this course.",
    "He invokes balance throughout.",
    "And I try to make a good case that you should see that as optimization and directly relevant, therefore, to accounts of optimization of processing that we discussed with connection to relevance realization.",
    "There were some issues I had with Sternberg.",
    "The idea that all wise people, all of this machinery is directed towards the common good.",
    "That strikes me as anachronistic.",
    "I think a less contentious claim would be that it's directed towards virtue and meaning in life for oneself and others in some unspecified way.",
    "There was also the invocation of values as affecting or constraining the whole process.",
    "Again, it was unclear to me what this is.",
    "There's an ambiguity here.",
    "It could be the relatively trivial claim that the wise person is being regulated by normativity.",
    "You know, by considerations, what's true and good and beautiful.",
    "And that would be definitional because wisdom is a normative term and therefore relatively trivial, or it could be that specific values are being invoked here.",
    "But if that's the case, they should be specifically stated and then justified for why those ones are chosen and explicitly explained how those specific values make an impact on specific aspects of the machinery.",
    "So that's all sort of missing and needs to be addressed.",
    "It's ultimately a product theory, not a process theory.",
    "Sternberg does have a theory of foolishness, but it's not independently generated.",
    "And it doesn't really pick up on the centrality of seeing through illusion and into reality.",
    "So if you'll allow me to make use of all of that machinery, not only the machinery that we've talked about in the psychology of wisdom, but the machinery that many of these theorists are either explicitly or implicitly invoking all of the philosophical work we already covered in the first half of the course connected to wisdom.",
    "I want to try and humbly draw upon that and talk about a proposal made by myself and Leo Ferraro.",
    "If you remember Leo and I had done work together on flow, which I've talked about.",
    "Work on mindfulness that I've talked about (writes Vervaeke & Ferraro 2013).",
    "This was work from 2013.",
    "get the notes!",
    "Vervaeke and Ferraro Model",
    "So the place to start is to go back to what we saw and what I've argued for.",
    "So I hope I don't have to recapitulate that whole argument that we have these two competencies.",
    "We have sort of an inferential competence (draws a square and writes Inferential inside) that has to do with our propositional knowing.",
    "And we have an insight competence (draws a square and writes Insight) right over here and that has to do with construal.",
    "And that has to be, that's more sort of procedural perspectival.",
    "I'll come back to that point because that's one of my criticisms of Vervaeke and Ferraro.",
    "And then the idea here is that this is enhanced and protected (Fig. 1a) (draws a larger square around Inferential and writes AOM) from undue influence from sort of more S1 processing by active open-mindedness.",
    "And then I argued, following Jacobs and Teasdale, and also arguments derived from the need for an independent competence on construal, et cetera, that while this (indicates AOM and Inferential) is really clearly the case for theoretical context.",
    "A more therapeutic or at least an existentially developmental context, we want this (draws a larger square around Insight) to be foregrounded and we want it protected from that.",
    "And so we want it developed by mindfulness (writes Mindfulness in the larger square).",
    "And you understand that by mindfulness, I mean, a style that coordinates psychotechnologies together, of meditation, contemplation, perhaps flow interaction with the environment.",
    "Fig.",
    "1a That brought up the immediate question of how are these coordinated together?",
    "(draws a connecting line between the left and right box)",
    "Now, one answer might be that they are just opponent processing and they are self-organizing and that's potentially viable, but there's—we—already at this poin (indicates where the the lines converge) —sorry, that sounds so self-congratulatory.",
    "I don't mean it that way.",
    "Okay.",
    "We argued—let's just state it that way.",
    "We argued that, whereas this (indicates Inferential and AOM) is giving priority to propositional knowledge.",
    "This has to do with a procedural knowledge (indicates Insight and Mindfulness), skills of attention, basically with cultivating certain skills of attention.",
    "And then the idea was that active open-mindedness and propositional knowing basically we argued then give you knowledge of facts (Fig. 1b)",
    "(writes Fact under AOM and Inferential).",
    "This gives you knowledge of events or processes (writes Events / Processes under Insight and Mindfulness).",
    "So this (indicates AOM and Inferential) basically tells you about we're understanding what a fact is as cross-contextual patterns.",
    "Events, or processes are things that are unfolding like idiosyncratically in time and space.",
    "I'm not, I mean, that's sort of right.",
    "Perhaps a better way of putting this, that would align it with the stuff we talked about with Schwartz and Sharpe is this is your grasping of principles (writes Principles beside Fact) and this is your grasping of processes.",
    "Fig.",
    "1b",
    "And this (indicates AOM and Inferential) would therefore largely be sort of like what's being talked about in sophia.",
    "And this (indicates Insight and Mindfulness) is largely what perhaps what was being talked about in phronesis.",
    "We suggested that—I'm still open to that suggestion.",
    "I'm not quite sure that it maps as cleanly as that now—but in addition to this clearly propositional and, at least, centrally procedural, we invoked perspectival (Fig. 1c) (draws a square and writes Perspectival inside).",
    "So this is propositional (writes Propositional beside AOM and Inferential).",
    "This is largely procedural (writes Procedural to beside Mindfulness and Insight).",
    "And then this is perspectival (indicates Perspectival at the top).",
    "And then so this has to do with inference, this has to do with insight and we've already got a good sense.",
    "We've seen this.",
    "We didn't—we were not aware cause it hadn't been generated.",
    "We were not aware of Grossman's work at the time, but we knew the Berlin work.",
    "And this is, of course, what's being managed here is internalization (writes Internalization below Perspectival).",
    "How do you learn to take, adopt and take other people's perspectives and internalize them within your own processing so they become metacognitively effective?",
    "Fig.",
    "1c",
    "And then we said, well, what perspectival knowing did—and here's where I want to—one of my, I think first criticisms.",
    "We said, well, what perspectival knowing does is it integrates knowledge of facts with knowledge of events.",
    "It sort of helps you to use, I think, maybe better language, it helps you to put principles (indicates Fact / Principles) into process (indicates Events / Processes) and have processes (indicates Events / Processes) governed by principles (Fact / Principle).",
    "And that's what sort of perspectives are doing.",
    "So this (indicates Insight and Mindfulness) is—we're talking about the epitome of this is a skill and the epitome of this is a theory.",
    "And what a perspective does is put theories and skills together.",
    "I think that's kind of right, still in a sense, but I think the relationship is—and this is what I would argue for here.",
    "The relationship is more like this.",
    "That propositional knowledge (Fig. 2) (writes Propositional knowledge) is grounded in but affected by (draws a double-headed arrow below Propositional knowledge) procedural knowledge (writes Procedural knowledge under Propositional knowledge), your skills, knowing how to interact.",
    "And then (draws a double-headed arrow under Procedural knowledge) that this (indicates Procedural knowledge), your ability to cultivate skills and then apply them to the propositional knowledge is grounded in your perspectival knowing (writes Perspectival knowing under Procedural knowledge).",
    "Because that's going to give you your situational awareness that you need to cultivate the skills and, so that you can apply (indicates Propositional knowledge) your knowledge of principles.",
    "And then I would argue that that's—and you've seen me make this argument before.",
    "This is ultimately grounded in your participatory knowing, (writes Participatory knowing under Perspectival knowing)",
    "the agent arena attunement that affords your being in the world and your ability to go through modal transformation, existential change.",
    "Fig.",
    "2",
    "So that also brings up—I might as well mention it now, another criticism of this theory (indicates Fig. 1c), which is, although it's talking about propositional and procedural knowledge and perspectival knowing, there's no clear discussion here of participatory knowing (indicates Fig. 2).",
    "And that's a significant lacuna in the theory for the following reason.",
    "Without an account of participatory knowing, for all of its claims, the Vervaeke and Ferraro theory of being a process theory rather than a product theory, without talking about the participatory knowing, it really can't incorporate into its account of becoming wise how one goes through transformational experience, how one goes through modal change.",
    "I mean modal in the existential sense, not the logical sense.",
    "So without connecting participatory knowing to this overarching schema (indicates Fig. 1c), the connections between wisdom, transformative experience, altered state of consciousness—all of these things that we've discussed are actually crucially missing from this theory.",
    "And therefore it's claim to being an adequate processing theory can be rather significantly challenged.",
    "I think that, so that needs important development.",
    "We did talk about a cognitive style that you could cultivate.",
    "One more thing.",
    "I think what we were doing is also, we were smuggling in that the perspectival knowing (indicates Perspectival and Internalization) with the process of identity creation, that's central to participatory knowing (indicates Participatory knowing in Fig. 2).",
    "So I think that was also a part of the problem.",
    "get the notes!",
    "Sophrosyne",
    "As An Optimization Of Your Perspectival Knowing Now, what we did argue is that this (draws a larger square around Perspectival and Internalization) is set within a cognitive style that will give you a higher order way of regulating active open-mindedness and mindfulness.",
    "And here we took directly from the philosophical tradition and we talked about internalizing the sage (Fig. 1d)",
    "(writes Internalizing the sage beside Perspectival and Internalization).",
    "Internalizing Socrates, internalizing the Buddha, internalizing Jesus, internalizing the sage.",
    "Fig.",
    "1d",
    "And we talked about what impact that has.",
    "So we, you know what this is.",
    "Internalized.",
    "We talked about this repeatedly.",
    "What the process of internalization is.",
    "What it's like to internalize Socrates, et cetera, et cetera.",
    "And we've already seen how central that is to wisdom.",
    "And see—so while this is overcoming fallacious reasoning (writes Fallacious reasoning beside AOM and Inferential), right?",
    "This is overcoming misframing, misconstrual (writes Misconstrual / Misframing beside Mindfulness and Insight).",
    "What this (indicates Perspectival and Internalization) is doing is it's helping you to overcome egocentrism in a powerful way (writes Egocentrism beside Internalizing the sage).",
    "These are all ways in which we can fall into illusion, self-deception.",
    "But we also talked about what does internalizing the sage do?",
    "What's",
    "—when you get that metacognitive enhancement, you get that perspectival ability, what's it doing?",
    "So here we talked about a virtue that you haven't heard me talk about very much (Fig.",
    "1e) (draws an arrow from Perspectival and Internalization and writes Sophrosyne).",
    "And it's unfortunate because, in some ways, this is...",
    "Okay.",
    "So the ancient Greeks had four cardinal virtues.",
    "Wisdom, which is really kind of a meta-virtue.",
    "Justice, which we talked a lot about.",
    "Courage.",
    "And then the fourth is this word, Sophrosyne.",
    "Fig.",
    "1e Now sophrosyne is often translated as temperance—doesn't capture it well.",
    "Moderation, doesn't capture it well.",
    "So I want to put aside that and try and come back at this.",
    "But you see, if you went to the Delphic Oracle, there were things inscribed on the wall there.",
    "And one was know thyself and that's clearly, you know, connected.",
    "Socrates made it his own.",
    "And we've come to know what that means.",
    "How the knowledge of oneself is, of course, not romantic autobiography, but a deep understanding of the principles by which you're operating.",
    "But the other one was everything in moderation, which was like this.",
    "But that's not—again, moderation is good, but it's not quite right.",
    "And we know this (indicates Sophrosyne) is connected to something like Aristotle's notion of the golden mean.",
    "That all virtue—and remember what that is?",
    "You're trying to create a virtual engine that generates enough options, so you don't suffer vices of deficit, but also generates enough, there's enough governance.",
    "There's enough selective constraints so that it also thwarts vices of excess.",
    "So there's a kind of optimization going on there.",
    "And as I said, there—you get a little bit in the word moderation, but moderation sounds more like averaging and settling.",
    "We argued that there's a better way of trying to understand this by understanding it as something that it was often constrasted with, which is enkratia (Fig.",
    "1f) (writes Enkratia beside Sophrosyne).",
    "So, you know, this word (indicates Enkratia).",
    "This is demos kratia (writes Demos and Kratia above Enkratia).",
    "Power or rule by the people and kratia is sort of exercising power on yourself.",
    "So this is kind of like self-restraint, self-control (writes Self-restraint under Enkratia).",
    "Fig.",
    "1f",
    "And so a way of getting at this is to think about the fact that you could be practicing a virtue—a virtue (indicates Enkratia) enkratically or in a sophrosynic manner.",
    "Let me give you an example.",
    "So here's two people.",
    "There's Tom and there's Susan.",
    "Tom is honest, or at least he's trying to become honest.",
    "Now Tom goes into situations and Tom sees clearly the potential to lie.",
    "And he sees clearly the benefit that would accrue to him if he lies and it comes with a tremendous sort of temptation.",
    "There's a tremendous impulse.",
    "And so he exercises self-control.",
    "And he doesn't lie.",
    "And Tom is to be commended for that.",
    "That is an important kind of honesty.",
    "But consider Susan.",
    "Susan comes into a situation.",
    "She clearly sees the opportunity to lie.",
    "She clearly sees the advantages that would accrue to her if she lies.",
    "But that's it.",
    "It's like when we talked about Frankfurt and whether or not something—it's unthinkable to her, not in the sense that she can't think the thought, I can lie.",
    "Or think or imagine to herself lying.",
    "It's not a viable option to her.",
    "She can't get into the existential mode where that draws on her in any way.",
    "So although she can think it in one sense in a Frankfurtian sense, it is unthinkable to her.",
    "It just—she's not tempted to lie in that sense.",
    "Many of us, myself included would side with the Greeks in saying, Susan is more honest than Tom.",
    "Because honesty is now second nature to Susan in a way it isn't to Tom.",
    "So that's sophrosyne, at least one aspect of it.",
    "Do you remember when we were doing Paul and agape?",
    "And Paul says, now I will show you the most excellent way.",
    "And then he's talking, of course, about agape as the most excellent way.",
    "And then he says, remember, in order to try and get you to understand the transformation, when I was a child, I thought like a child, I spoke like a child and acted like a child, but when I became a man, I put childish things behind me.",
    "And remember we talked about that?",
    "When you're a child, you're deeply tempted by toys, your salience landscape automatically organizes in a certain way.",
    "But when you're an adult, when I'm a man, I come in and I see Spencer's toys.",
    "I know that they're there.",
    "I know that I could play with them, but they have no pull on me.",
    "They do not call me.",
    "They do not tempt me.",
    "And as the child is to the adult, the adult is to the sage, the sage has a salience landscape in which they are not tempted to self-deception in the ways that we so readily are.",
    "That's sophrosyne.",
    "It is to have a salience landscape (draws an arrow pointing towards Sophrosyne) that has gone through a kind of fundamental reversal.",
    "It is not—I mean, these are all differences of emphasis, but like the way our salience landscape is less oriented towards the self-deceptiveness of a child, the sage's salience landscape is less oriented towards our prevalent and pervasive forms of self-deception.",
    "They see through illusion and into reality.",
    "So this (Sophrosyne) is, of course, deeply perspectival.",
    "And I want to add a little bit more to it (draws an arrow from Sophrosyne) because it's not just—sorry, you see this in Daoism, it comes through.",
    "The idea that once you've trained enough, you just have to, you just have to let—the sage can just let things unfold naturally.",
    "You see this even in Augustine.",
    "You know, love God and then do what you want.",
    "Of course you have to love God.",
    "That means if you really, truly love God, if agape is flowing through you, as Paul recommends, then you have sophrosyne",
    "and then you will just, you will—and this is what I want to say.",
    "You will be tempted to the good.",
    "You will be tempted.",
    "Just like you could be tempted.",
    "Your salience landscape, naturally self-organizes towards self-deception.",
    "Your salience landscape, if you're wise, naturally self-organizes towards seeing through illusion, zeroing in on what's relevant and important and how it is relevant to the project of becoming more virtuous and having a more meaningful life.",
    "You're tempted, you're naturally tempted to the good.",
    "That sophrosyne.",
    "And so we argued that what you're doing here is you're cultiv—you're internalizing the sage and what that's doing is helping to overcome egocentrism in this deep sense of helping you to realize sophrosyne.",
    "And so this means we argued that there's deep connections, and I don't think these have been explored enough between wisdom and sophrosyne.",
    "And of course sophrosyne is a kind of optimization of your perspectival knowing.",
    "It's that I've optimized my perspectival knowing.",
    "So it's always in service—and this (indicates Participatory knowing) is what was to some degree missing from this theory—it's in the service of my agent arena relationship and how that is being developed.",
    "Being developed.",
    "That reciprocal realization so that I can go through the important transformations that are needed to become a wise person.",
    "We argued that what the sophrosyne is directed towards were three M's.",
    "Obviously morality (Fig. 1g) (writes Morality below Sophrosyne), more broadly construed as not just knowing the rules, but the capacity for being virtuous.",
    "Realizing meaning in life (writes Meaning in life below Morality).",
    "Now, a deficit there is we only had self-determination theory, Deci and Ryan on this kind of stuff.",
    "And much more work, much more significant work has been done with meaning in life.",
    "Work that I'm doing with Thalia Vrantsidis, Juensung Kim, Philip Rajewicz and we're presenting at APA this year.",
    "And so, this theory needs to be revised and I've tried to show you that in the course to more directly connect this machinery to meaning in life.",
    "So this needs significant improvement (draws an arrow to Meaning in life).",
    "We did argue that meaning in life is irreducible, Wolf, to morality.",
    "And then something we talked about is mastery (writes Mastery below Meaning in life).",
    "Fig.",
    "1g We use the three M's cause they're helpful.",
    "I'm not comfortable with that term anymore because of all of its political connotations.",
    "We were thinking of it more like in almost in the academic sense, like when you get your M.A.",
    "And then when, you know, in the oldest sense, like when you did your masterpiece.",
    "What we meant here was, you know, a terrific capacity for caring and coping with reality.",
    "You had sets of skills, you had sets of psychotechnologies, you had sets of roles that you could take.",
    "So this gives you roles like propositional—sorry, propositional knowing (indicates Inferential and AOM) gives you rules.",
    "And procedural knowing gives you, you know, various routines.",
    "This perspectival knowing gives you various roles.",
    "And being able to use, you know, rules and routines and roles with mastery in coping and caring was central.",
    "Again, always guided under the governance, under the regulation of sophrosyne (draws an arrow from Sophrosyne to the three M's).",
    "So I've already—I mean, so this is a processing account.",
    "It tells you how to become wise.",
    "You cultivate active open-mindedness.",
    "You cultivate mindfulness.",
    "You cultivate internalizing the sage.",
    "We use sports psychology (writes Sports psychology below Internalizing the sage) here as a way of trying to get what that (indicates Internalizing the sage) looks like.",
    "We also use, of course, developmental psychology Vygotsky, but sports psychology talks about very much how people go through a process of internalizing the coach and that's strongly analogous to internalizing the sage.",
    "And so we talked about—you cultivate active open-mindedness.",
    "You cultivate mindfulness.",
    "You cultivate internalizing the sage.",
    "And you're guided overall by trying to become sophrosynic in that.",
    "And so this is a processing theory, as I've mentioned, I think there's a deficit in it.",
    "It does not take into account—it's—what's absent from it is transformational experience, transformational development.",
    "These are all very telling things.",
    "The role or relationship between this and altered states of consciousness was not properly developed.",
    "The participatory knowing, which of course connects to the transformational experience is missing.",
    "So wisdom is not connected to gnosis here in any important way.",
    "get the notes!",
    "So those are some important criticisms I would have.",
    "The relationship between the kinds of knowing wasn't well-developed.",
    "We sort of just argued that while perspectival knowing sort of synthesizes these (indicates AOM and Inferential and Mindfulness and Insight) together, I think that's too simplistic.",
    "A much more complex relationship, you see me argue for in this course, I think needs to, it is being developed and needs to be developed.",
    "Two things that were strongly implicit in other p—",
    "Oh, I should mention.",
    "A core aspect of this theory that I think is still central is that all of this, all of this (indicates the whole board), and we made this very explicit, all of this is, and this came out.",
    "I was so part of this conversation that I forgot to take a moment and explicate it.",
    "That all of this is about enhancing relevance realization (indicates Fig 1g).",
    "Our main argument is that wisdom is some kind of comprehensive optimization of cognition.",
    "And then I would extend that now.",
    "Consciousness, character, et cetera, and that in order to optimize cognition in a comprehensive fashion and in the developmental fashion, that means that what you're doing is enhancing relevance realization.",
    "And we always, we already saw that at work throughout this.",
    "And we saw that relevance realization is central in the explicit psychological theories that we've already examined.",
    "Now in connection with that, there's another serious lacuna in this theory, which is that, although it does something, I think that's very important, it connects wisdom to insight.",
    "Let's start here (indicates Insight).",
    "I mean, it will be odd to say, you know, Sam is very wise, but he's not very insightful.",
    "That seems wrong.",
    "We could say things like, you know, Sam is very wise and he's maybe not very educated.",
    "He might not be sort of super intelligent.",
    "That's fine, right?",
    "But to say that Sam is wise and not insightful, well, that seems to trespass on that McKee and Barber point about seeing through illusion.",
    "Wisdom definitely has to do with, you know, gaining knowledge in the best way, theoretical knowledge.",
    "Obviously gaining procedural knowledge.",
    "So the wise person knows how to believe well, and that seems also deeply, deeply right.",
    "The wise person is overcoming egocentrism, internalizing the sage.",
    "The traditions point to this very clearly.",
    "And they point towards sophrosyne, the most excellent way.",
    "And of course, one way in which this—we could understand this (Sophrosyne) is exactly the Pauline recommendation (Fig. 1h) (writes Agape).",
    "That the best form of sophrosyne is agape.",
    "So, but what's missing?",
    "Fig.",
    "1h",
    "So I've already pointed many things, but something that's central here is a theory of understanding.",
    "To say that, you know, like, oh, Bill is very wise.",
    "He's so insightful.",
    "He's, you know, he's so capable of self-transcendence and overcoming egocentrism.",
    "He believes things really well.",
    "Like he's not easily duped, but he doesn't understand.",
    "He doesn't have deep profound understanding of things.",
    "It's like, no, no, that's not right.",
    "One of the ways people zero in on relevant information is by being more insightful.",
    "Yes.",
    "One of the ways they zero in on a relevant information is like avoiding bias and fallacy in their inferential changes of their beliefs, right?",
    "One way in which they zero in on relevant information and overcome egocentrism is all of the perspectival, internalization, the cultivation of sophrosyne.",
    "But what's missing—and we saw this in Ardelt's work very clearly, we saw it implied in Sternberg's.",
    "And so we should have taken this into account.",
    "Wisdom should also have within it a clear theory or connected to a clear theory of understanding.",
    "And so I think that's also missing.",
    "What is it to enhance understanding?",
    "What is it to develop a profound understanding?",
    "So I want to try and at least discuss that.",
    "I'm not in the place where I have a complete theory of understanding.",
    "I've been doing a lot of work on it.",
    "Work that I'm actually doing with Leo Ferraro.",
    "And because that theory is still very much a work in progress, I'm also not clear quite how it would fit into this (indicates Perspectival Internalization).",
    "What would be the cognitive style for tapping into the participatory knowing and how does that relate to enhanced understanding?",
    "I'm not sure.",
    "I don't know.",
    "I don't know.",
    "So the criticisms have shown me many ways in which there's important lacuna, there's things that are underdeveloped and things of which I'm ignorant.",
    "However, and I'm going to try and address the understanding issue in a moment.",
    "I would like to say, nevertheless, we can see how all of the theories converge, including this one on relevance realization, intelligence, rationality, these different kinds of knowing, and integrating them together, optimization.",
    "They're all zeroing in on this, so that we see—remember?",
    "Back to this old diagram, everything converging (Fig. 3) (draws converging lines) on the RR (writes RR) and then coming out (draws diverging lines) into all these aspects of human spirituality.",
    "And here's one (writes Wisdom)",
    "I've made, I think, a plausible case for that really helps plays a crucial role in helping us to give a naturalistic account of what wisdom is.",
    "That I think I've made a plausible case for.",
    "Now, what about understanding? Fig. 3",
    "Well, we already saw it invoked with this grasping of significance (erases the board).",
    "And it's interesting in a completely independent and convergent manner.",
    "When you look through a lot of the current philosophy of understanding, this is what people are now distinguishing understanding from knowledge, distinguishing understanding from just possessing an explanation, because an explanation is a set of propositions.",
    "So there is the idea that understanding is something beyond possessing an explanation.",
    "It's something above and beyond simply knowing.",
    "We already saw with Kekes, this idea of grasping the significance.",
    "And I pointed out to you that that could be understood in terms of construal and relevance realization.",
    "What I am saying is if you take a look at the philosophy of understanding literature, this idea that understanding goes beyond knowledge and explanation in the grasping of the significance of the knowledge is something to which you can make—you can draw a quite powerful convergence argument.",
    "Many people are converging on this idea.",
    "There's some variation on what they think this grasping the significance is.",
    "I think, to go back to Smedslund that it has to do, like we saw with grasping the relevance of what you know.",
    "Remember that was one of the key features of his account of understanding.",
    "So in addition to all the implication, relations and logical relations, there were relations of relevance, non propositional.",
    "And then I argued that construal plays a central role and that construal can be understood in terms of problem formulation, the relevance realization machinery that's found within problem formulation.",
    "Good Construal",
    "So I would argue that what we're talking about is a really good construal (Fig. 4a) (draws a square and writes construal inside).",
    "And we have a way of talking about that already.",
    "We have the notion of an optimal grip (draws a downward arrow from Good construal and writes Optimal grip).",
    "I have a really good construal",
    "[that] has a structural functional organization.",
    "I've sized up the situation.",
    "You know, featural Gestalt, the right degree of transparency opacity.",
    "I'm getting an optimization on my grip on things.",
    "So this is good contact.",
    "That's the good construal.",
    "And then what it does is it affords me to grasp what's relevant in this situation.",
    "How I sized up the situation and got an optimal grip on it, affords—remember in Madison—good problem formulation?",
    "Fig.",
    "4a",
    "Now, we also saw something else.",
    "If you remember the connection to good problem finding (Fig. 4b) (draws arrows pointing from Good construal).",
    "And that's why I talked about the problem nexus.",
    "And I promise to come back.",
    "I talked about Arlin.",
    "But I also mentioned that point, the work of—very recent work—all my markers are running out—of de Regt (writes De Regt).",
    "And I've never met this person, so I hope I get their name right.",
    "I just want to copy this very carefully (writes Cigsberg).",
    "Cigsberg?",
    "I'm not sure if that's right or not.",
    "This is work from 2017 (writes 2017).",
    "And then there's also de Regt's own book on understanding.",
    "And there's a lot of good work going on about this.",
    "It's very exciting stuff.",
    "They point towards what they call the standard of effectiveness for understanding.",
    "I understand something—what's the contrast here?",
    "Fig.",
    "4b Standard Of Effectiveness For Understanding",
    "Okay.",
    "You don't want to say that somebody has understood something.",
    "And what that means is they've grasped the truth.",
    "Now they have to be trying to grasp the truth.",
    "That's important, but—and that'll come out in a moment, right?",
    "But you can't say, well, if they didn't grasp the truth, they don't understand because then you're your faced to say the following thing that, you know, most people have never understood anything because most people's beliefs in the past are faults.",
    "And most of my beliefs right now are false.",
    "So I'm actually not understanding.",
    "You don't want to tie understanding too tightly to truth in that fashion.",
    "So instead of tying it to truth, you might want to tie it more to something like rationality, where you're trying—you're using the best methods for trying to get out the truth.",
    "That's more plausible.",
    "And this would also help to explain why in the prototypical instances within science, we use things that aren't true in order to generate understanding.",
    "You go and you open a science textbook and they'll show the atom with this little circle and things going around it (Fig. 5) (draws a circle and larger rings around it), right?",
    "And that's all, that's pretty much completely false.",
    "It doesn't matter that it's false.",
    "Fig.",
    "5",
    "It is effective for helping you to grasp the significance of the scientific model of the atom to draw, as Cherniak would say, the right implications, look for the right connections.",
    "It helps you zero in on the relevant information in the right way.",
    "And that's why it's used.",
    "Nobody—you're making a mistake if you think most of the diagrams and the idealizations that are at work in science are attempts to represent the truth accurately.",
    "They are not, they are attempts to effectively get you to zero in on the relevant implications.",
    "Make the relevant connections as Smedslund would say.",
    "This is what is meant by effectiveness.",
    "Effectiveness is exactly doing.",
    "And then they talk about how, what it is to say that somebody understands something, is that they're good at being able to apply their knowledge, find new domains, open up new areas of research.",
    "So of course, it's this multi apt ability to apply what their good problem formulation here.",
    "To transfer it and transform it and specify in many different ways.",
    "And what's implied in here, of course is an important capacity for problem finding (Fig. 4c) (writes Problem finding).",
    "Somebody who has good understanding can facilitate a need for—they can motivate and facilitate a need for cognition because they can use that to go out and find and formulate problems, perhaps zero in on important problem nexus.",
    "Fig.",
    "4c So, and, of course, this optimal grip is giving me something that De Regt—I don't know how he pronounces his name, also talks about what many people talk about.",
    "The idea that understanding is contextually sensitive.",
    "It's contextually relative.",
    "To know that I understand something is relative to the situation at hand and relative to the person at hand.",
    "You and I can both know the same things, but if you're in situation A and situation B, you might understand those things because you can apply them in A. I don't—I couldn't be said to understand them as well, because I can't apply them in situation B.",
    "Also, we could be in the same situation, but I have a different set of skills.",
    "And so I can apply my knowledge better than you can.",
    "I understand better than you can.",
    "So there's very much that this is context relative, and I would then add, of course, context sensitive (Fig. 4d) (writes Context relative and Context sensitive below Optimal grip).",
    "And that, of course, is the context sensitivity, whereas this is the ability to do things in a much more context general way.",
    "And of course, I'm invoking the machinery of relevance realization.",
    "I'm invoking it in a good construal and then the ability to transfer it insightfully (indicates Problem finding).",
    "Fig.",
    "4d I would also argue that one more thing is needed.",
    "Because we've already got the idea that when if I am making these kinds of forward commitments, cognitive commitments, they need to be backed by a lot of convergence (Fig. 4e) (draws arrows converging on Good construal) so that my construct is also trustworthy.",
    "I've done a lot, and of course, to overcome self-deception.",
    "So if basic understanding is to grasp the significance, grasp through relevance realization, the relevant implication, the relevant connections—this is what I'm trying to suggest to you.",
    "That basic understanding becomes profound understanding when basic understanding is used to generate plausibility (writes Generate plausibility above Good construal).",
    "Fig.",
    "4e I don't think that's enough because if you'll allow me a sort of schematic way of putting it, this is very horizontal (draws a horizontal arrow)",
    "it tells you how to bring different domains together into your good construal and then apply them to many domains.",
    "And you're doing the compression.",
    "And then you're doing the variation.",
    "You're doing the relevance realizing.",
    "The compression, variation, good problem formulation, optimal gripping.",
    "This is contextually sensitive (indicates Context sensitive).",
    "This is effectively applied across in a cross-contextual manner, et cetera, et cetera.",
    "But I think understanding also has, if you'll allow me a vertical domain (Fig. 6) (draws a vertical line down the horizontal arrow), because I think part also of what profound understanding does, is it aligns and optimizes the relationship—so if this is plausibility generation (writes Plausibility generation in the X-axis), what's being aligned and optimized here?",
    "I think are the propositional knowing (writes Propositional in the upper right quadrant), the procedural, perspectival and the participatory (writes Procedural, Perspectival, Participatory in the lower right quadrant).",
    "Fig.",
    "6",
    "I mean, this goes back.",
    "Somebody who really knew physics wouldn't just be grasping the propositions of physics, they would be able to—they'd have the skills.",
    "They'd know how to do physics and they'd have the situational awareness.",
    "They would know, you know, which skills to apply and which skills to develop in order to do physics well, There might even be a participatory aspect where they might have come to identify with the physicalist worldview and taken up their agency with respect to that.",
    "Although that might be problematic given arguments from the meaning crisis.",
    "But the more there's—so the more deeply these are aligned and interconnected and mutually facilitating each other the more capable they are, I would say, of understanding the material.",
    "So I think what needs to be developed is a way of theoretically integrating the horizontal that understanding is to generate—well, at least profound understanding is to take basic understanding, grasping the relevance connections and make those relevance connections, convergence and elegance, optimal gripping.",
    "So that profound understanding is to generate plausibility.",
    "That's the horizontal.",
    "But profound understanding is also to align, so you're getting grounding downward and you're getting emergence upward, the relationship between propositional knowing, procedural knowing, perspectival knowing, and participatory knowing.",
    "And then all of that needs to be, of course, integrated into an account of wisdom.",
    "As I said, what also needs to be aligned is transformational experience.",
    "And that means an account of gnosis needs to also be integrated into the account of wisdom.",
    "get the notes!",
    "Transformative Knowing / Experience",
    "So (erases the board) that notion of transformative—of transformation, of knowing through transformation and becoming, so that knowing and becoming, knowing oneself and knowing the world and becoming a different agent in a different arena bound together.",
    "We've talked about this, that transformative knowing (writes Transformative knowing/experience), that transformative experience.",
    "There's, of course, many instances in which it's rather sudden or somewhat sudden (Fig. 7a) (writes Sudden below Transformative knowing/experience), and so it has very much important features of insight and we've taken a look at that.",
    "And that, of course, is again, to recommend it one more time, the seminal and powerful work of LA Paul (writes Insight and LA Paul below Sudden).",
    "Fig.",
    "7a Now, Agnes Callard in her book Aspiration has, I believe it's 2016, has recently argued that there are also instances where people go through this transformative knowing that are much more incremental in nature (Fig.",
    "7b)",
    "(writes Incremental nature under Transformative knowing/experience).",
    "She doesn't deny this (indicates Sudden), but she argues that there are very many instances about this (indicates Incremental nature).",
    "So all of the stuff we talked about here (indicates Sudden) hasn't been dispensed with.",
    "This (indicates Incremental nature) is being added as a compliment and a supplement.",
    "Fig.",
    "7b Aspiring To Proleptic Rationality",
    "So what's an example of this more incremental process?",
    "She gives many examples.",
    "Let's use one.",
    "You joined a music appreciation class.",
    "Okay.",
    "And so we're using the word appreciation here, not in the sense of gratitude, but how it's used when people talk about music appreciation, art appreciation.",
    "So you're joining music appreciation class.",
    "What would make you a good student in the music appreciation class?",
    "If you're there because you want to impress your girlfriend or your boyfriend, or you're there because every time you go, you pass the chocolate store and you buy some chocolate.",
    "Or you're there because you're just trying to get a credit.",
    "The person teaching the music appreciation is not going to regard you as a good student, because why?",
    "Because the goal of music appreciation is to come to value music for its own sake.",
    "It's to come to finding music intrinsically valuable and therefore something that is directly relevant to your meaning in life.",
    "Something that you directly care about.",
    "Now the thing is.",
    "If you were a good—now think of the paradox here, and this is so beautiful the way Callard brings it up.",
    "If I was a good student, I would appreciate music for its own sake.",
    "But if I appreciated music for its own sake, I do not need to take the music appreciation class.",
    "Do you see the paradox here?",
    "And then Callard points that this was the same thing when you decide you're going to undertake a liberal education.",
    "The liberal education is gonna give you values and preferences that you don't currently have.",
    "So the idea is the music appreciation.",
    "So what do you do there?",
    "How do you break through that dilemma?",
    "Now, let's be very clear.",
    "Callard is in agreement with LA Paul, that you can't get through this in an inferential fashion for all of the arguments we've already seen.",
    "Right.",
    "She talks about—she does make something clear that I don't think that's clear in Paul's work.",
    "She talks about the fact that this process, this process of trying to acquire an appreciation for something as intrinsically valuable.",
    "She calls this process aspiration (Fig. 7c) (writes Aspiration).",
    "Where you might call this process more inspiration (indicates Sudden), the sudden insight, right?",
    "Inspiration versus aspiration.",
    "So you're aspiring.",
    "Fig.",
    "7c",
    "And she points out something that I think is really clear that this has to be something that can be seen as a rational process.",
    "Now, of course, there's ways in which we can screw this up.",
    "But what she wants to argue is that there's a form of rationality appropriate to aspiration.",
    "She calls it proleptic rationality.",
    "Like when you gave, when you were doing proleptic things in the ancient world, you were trying to encourage people to cultivate particular virtues or values.",
    "Proleptic rationality (writes Proleptic rationality under Aspiration).",
    "Why?",
    "Because if we were to say that the person who is engaged in aspiration, who is trying to become somebody other than they are to go through the transformative experience, to have a perspectival knowing, a participatory knowing that they do not currently have, if we were to say—because they cannot do that inferentially.",
    "They cannot use decision theory to do that.",
    "If we were to say, oh, therefore they're irrational.",
    "Notice the paradox we fall into.",
    "Because we would have to, we would have to conclude this, that if I am aspiring to rationality (writes Aspiring to rationality), because you have to, that would be an irrational thing to do.",
    "If I'm aspiring to virtue, that would be an irrational thing to do.",
    "If I decide to take up a liberal education to become a better person, a different better person, then that would be an irrational thing to do.",
    "On pain of kind of a not a propositional contradiction, but a performative contradiction—remember we talked about performative contradiction?",
    "This—to call that irrational would be a performative contradiction.",
    "My aspiring to rationality has to be itself a kind of rationality.",
    "That's proleptic rationality, or to use something older.",
    "My loving of wisdom.",
    "My loving of wisdom.",
    "My aspiring to becoming wise cannot itself be an irrational process.",
    "It has to be rational, not inferential rational, for sure.",
    "So first of all, she does that excellent work of saying, look, this is we've got to broaden our notion of rationality to include aspiration.",
    "I would argue we have to broaden our notion of rationality to include inspiration as well.",
    "And that's the way in which I'm being radically sort of reconstructive of romanticism.",
    "So now the issue becomes what's going on here.",
    "Well, I'm going through a process of identity change (Fig. 7d) (writes Identity change below Incremental nature), transformative experience, participatory knowing, right.",
    "Fig.",
    "7d",
    "And here's where Callard's work is a little bit lacking.",
    "Because while she makes a very good case for aspiration, and a very good case for the nature of aspiration that it's proleptically rational, she doesn't give us very much towards a psychology of aspiration.",
    "And that's, of course, perhaps because she's a philosopher, she does offer a couple of cues.",
    "Let's go back to the music appreciation.",
    "So I want to be, I want to—and think about how this connects to sophrosyne trying to tempt yourself into the good, but you got to do it in this tricky way and think about also how it's related to gnosis and trying to get out of existential entrapment.",
    "So what I got to do is I've got to give myself, I got to have a value that will get me currently engaged.",
    "Here's my frame now.",
    "It'll get me currently engaged with music (Fig.",
    "8) (draws 2 intersecting squares), but I will be able to give up that value (draws a northeast arrow from the center of the lower square) when I actually value music for its own sake.",
    "So you see what's going on here?",
    "You need to, she calls it a placeholder, but it's actually in our sense, it's a symbol.",
    "It's something that connects the future you and its way of life or your way of life to the current you.",
    "And it does it by having this double-faced—not duplicitous, cause you're aware of this.",
    "That's what makes it a rational process.",
    "This double-faced thing.",
    "Fig.",
    "8",
    "So I may go to the music class because I currently have the value of sort of making myself do things that I find difficult.",
    "Now that's not the same value as appreciating music.",
    "But I do that with the understanding that that is temporary.",
    "That is to try and get me into a liminal place where I can start to play with what it's like to value music for its own sake.",
    "To enter that world.",
    "You can see the connections to gnosis here.",
    "You can see the connections to symbolic enactment here.",
    "Aspiration is deeply bound up, I would argue, with gnosis.",
    "And then something that Callard, doesn't talk at all about, but we've already talked about.",
    "I think aspiration is deeply connected to wonder (Fig. 7e) (writes Wonder).",
    "Wonder gets you to question almost like Socratic aporia your world view, your sense of self.",
    "It opens up and it motivates you.",
    "It opens you up and motivates you to go through aspirational change.",
    "I think if you have a wonderful kind of gnosis that's got the appropriate placeholder in place, that's the beginnings of a psychological account of how we can go through aspiration.",
    "Fig.",
    "7e",
    "So.",
    "I think we can bring what was needed for a theory of wisdom, because, of course, Philosophia.",
    "We aspire to wisdom and we always aspire to wisdom because to claim—and this is a deep point—that we've achieved wisdom is kind of a mistake.",
    "So we need an account of understanding, an account of gnosis.",
    "And these are all related and an account of aspiration.",
    "We need them to be further explicated, integrated and then integrated with the accounts of wisdom that I've been arguing for already.",
    "Okay.",
    "I want to try and draw this all together now.",
    "So I'll point to what's going on.",
    "Cause I'm going to need more time.",
    "I'm going to need time from the next episode to try and draw this all together.",
    "What I want to do in the drawing together is I want to try and draw this all together into an account of what wisdom is.",
    "I'll say what this is now so I don't just leave you completely hanging, but I want to come back and develop it.",
    "And then I want to try and connect this notion of wisdom back to enlightenment and back to responding, awakening from the meaning crisis.",
    "get the notes!",
    "An Account Of Wisdom Here's the account of wisdom I'm going to leave you with.",
    "And then I'm going to come back and try and at least develop and defend a bit.",
    "Wisdom is an ecology of psychotechnologies.",
    "An ecology of styles that dynamically—and that means reciprocally, in a reciprocal fashion—constrain and optimize each other, such that there is an overall optimization, enhancement of relevance realization.",
    "Relevance realization within inference, within insight and intuition, the connection to implicit processing, internalization, understanding, gnosis, transformation and aspiration.",
    "Wisdom is an ecology of psychotechnologies and cognitive styles that dynamically enhance relevance realization in inference, insight and intuition, internalization, understanding, and gnosis, transformation, and aspiration.",
    "In that sense, what's happening is something that's already overlapping with the machinery of enlightenment.",
    "We're seeing that wisdom is a dynamical system.",
    "A dynamical system that is counteractive to the machinery of self-deception.",
    "And that helps to afford the self-organized transformation into the life of flourishing.",
    "A life that is deeply meaningful.",
    "Thank you very much for your time and attention.",
    "- END - Episode 45 Notes To keep this site running, we are an Amazon Associate where we earn from qualifying purchases Leo Ferraro Book Mentioned: The Scientific Study of Personal Wisdom: From Contemplative Traditions To Neuroscience – Buy Here Sophrosyne Sophrosyne is an ancient Greek concept of an ideal of excellence of character and soundness of mind, which when combined in one well-balanced individual leads to other qualities, such as temperance, moderation, prudence, purity, decorum, and self-control.",
    "Enkratia Enkrateia comes from the adjective which means possession, power over something or someone else.",
    "Self-determination theory Self-determination theory (SDT) is a macro theory of human motivation and personality that concerns people's inherent growth tendencies and innate psychological needs.",
    "Henk de Regt Book Mentioned: Scientific Understanding: Philosophical Perspectives – Buy Here Book Mentioned: Understanding Scientific Understanding – Buy Here Book Mentioned: Explaining Understanding: New Perspective From Epistemology And Philosophy Of Science – Buy Here L. A. Paul Laurie Ann Paul is a professor of philosophy and cognitive science at Yale University.",
    "She previously taught at the University of North Carolina at Chapel Hill and the University of Arizona.",
    "She is best known for her research on the counterfactual analysis of causation and the concept of “transformative experience.”",
    "Agnes Callard Agnes Callard is associate professor of philosophy at the University of Chicago.",
    "Her primary areas of specialization are ancient philosophy and ethics.",
    "Book Mentioned: Aspiration: The Agency of Becoming – Buy Here Other helpful resources about this episode: Notes on Bevry Additional Notes on Bevry Ep.",
    "44 - Awakening from the Meaning Crisis - Theories of Wisdom Welcome back to Awakening from the Meaning Crisis.",
    "So last time, we took a look at the theory of Schwartz and Sharpe.",
    "And that was an important theory for linking wisdom to virtue and positive psychology.",
    "And we saw the deep connections between wisdom and the cultivation and practice of virtue.",
    "We made some criticisms of Schwartz and Sharpe.",
    "I argued that they should include sophia and not just phronesis.",
    "If you remember those, they were invoking Aristotle's two notions of wisdom and giving priority to phronesis.",
    "I think you need both as Aristotle argued: sophia and phronesis in order to be virtuous.",
    "I also argued that their attempt to explain phronesis with expertise, I think it was confused and we should put that aside.",
    "And then we took from that some ideas about the, you know, the developmental aspect that's, of course, central to Aristotle.",
    "Remember, he brought the developmental dimension to wisdom and how much wisdom is becoming the virtuous person.",
    "Other things I think are lacking in the theory—and we'll come back to this, that there wasn't much discussion about the connection between wisdom and meaning in life.",
    "But we then passed to taking a look at a theory that took very seriously the connection between wisdom and virtue.",
    "And this is the seminal theory of work of Baltes and Staudinger.",
    "We took a look at the idea of the metaheuristics, pragmatics for orchestrating mind and virtue and excellence.",
    "And they talked about the fundamental pragmatics of life.",
    "I pointed out to you that a way of making sense, conjointly of the invocation of metaheuristics and both senses of pragmatics is this idea that its core to wisdom is this capacity for relevance realization, obviously improved in some fundamental way, which I think means that there is integral connections between wisdom and intelligence via the notion of rationality that we've already been developing together.",
    "And then we took a look at the five criteria where there was clear indications of propositional knowledge, procedural knowledge.",
    "The contextualism, I argued can be best, be seen as perspectival knowing.",
    "I argued against their notion of relativism and argued instead for humility and fallibilism.",
    "And then, obviously, I have the—I think that's—I'm sorry that was maybe too harsh to them.",
    "I think it's pretty clear that many of the seminal figures of wisdom figures from the past were not moral relativist or relativist in any significant way.",
    "So we—I strongly recommend replacing relativism with fallibilism and humility.",
    "And then finally the fifth criteria is that wisdom is applicable to domains in which there is uncertainty.",
    "And of course, as I've already argued that a huge aspect of our life and our cognition, because ill-definedness, combinatorial explosion, et cetera, et cetera, which again means relevance realization is being strongly presupposed.",
    "I want to give you some further quotes to indicate what's going on about that.",
    "So they talk about a wisdom having to do with generativity, flexibility and efficiency in application.",
    "And of course the, the generativity and, but the flexibility, but also the efficiency are bringing together many of the ideas that we've been talking about with relevance realization.",
    "They talk about it taking place—listen to this language—within the frame of bounded rationality.",
    "They're invoking the notion here from Simon.",
    "Bounded rationality.",
    "This is rationality that is taking place within a framing, you know, it's taking place within the constraints of a combinatorial explosion, within the constraints of working with ill-definedness.",
    "It's a notion of rationality deeply enmeshed with relevance realization.",
    "They talk about a highly comp—these are all quotes.",
    "I'm giving you, by the way—highly complex sets of information about the meaning and conduct of life are—highly complex, okay?—are reduced quickly—listen to this!—to their essentials without being lost in the never ending process of information search—notice that combinatorial explosion directly being invoked here—that were to occur if wisdom was treated as a case of unbounded rationality.",
    "So I think it's pretty clear from all of those quotes that they are directly invoking the machinery of relevance realization.",
    "I pointed out that they—with their criteria, they were able to begin the operationalization of wisdom and in some of the first experimental work, they found that cognitive style in which one had excellent skills of judgment, seemed to be predictive of the ability to do well in the experiments.",
    "They also found that discussing with another person or imagining discussing with another person enhance your capacity for wisdom, challenging an individualistic notion of rationality going back to Descartes and assumed, if you remember, by Cohen in his arguments.",
    "But what's really interesting about that is, you know, of course, this brings up the relevance of platonic dialogue, but also the Stoic idea of internalizing the sage because the case where people imagined talking to the significant other was just as efficacious as actually talking to a significant other.",
    "And both of those were more efficacious than just giving more time to think and reflect on your own.",
    "I pointed out that one plausible explanation for why this works is that you're getting something like what Igor Grossman has shown with the Solomon effect that the move to take the perspective of other people—move to a third person perspective really enhances...",
    "I'm sorry, I'm going to have to get a drink.",
    "I seem to have something caught in my throat.",
    "Really enhances one's ability to overcome the transparency and the bias induction of one's own perspective and that it can be facilitated, of course, if you not only are taking the perspective of somebody who's present, but you can do that by simulating them internally.",
    "And eventually you could internalize that perspective the way we've talked about with the Stoics, internalizing Socrates, and one can learn to—remember this?—one can learn to converse with oneself dialogue, with oneself, do something like, and link us with oneself and provoke aporia, provoke the challenge and the demand for transformative change.",
    "I think all of this is really excellent work on their part.",
    "I noted that they also, like Schwartz and Sharpe, I think, make the mistake of trying to understand wisdom in terms of expertise.",
    "And I'm not gonna repeat that argument.",
    "And I think that it is a kind of fundamental mistake.",
    "I think, understanding wisdom on the lines of something like rationality is a much more fruitful connection.",
    "get the notes!",
    "Further Criticism Of Baltes And Staudinger Now, what are some further criticisms of Baltes and Staudinger?",
    "I think one of the most important criticisms I would make, other than the ones I've made about expertise and not being very clear on explicating the role of perspectival knowing—that's kind of an unfair criticism, cause it's anachronistic, but we should at least endeavor to bring that into our account of wisdom, given their own experiment and given the further work of Igor Grossman.",
    "I would also say that one of my criticisms, and this is a criticism that I made with Leo Ferraro and the 2013 article we published on wisdom.",
    "I would argue that there's a mistake here at a more theoretical and conceptual level.",
    "It's a mistake of omission, not commission.",
    "What we're getting here is a product theory of wisdom (writes Product theory of wisdom) in which what you're trying to do is to come up with sort of the account of what wisdom is, and that's a valuable thing you should definitely do that you should try and come up with, not just a feature list, of course, that's a little bit of a problem with just the list of criteria, of course, but what you do is you want to come up with, you know, what am I going to do?",
    "I'm going to find wise people.",
    "I'm going to get, sort of, you know, widely shared definitions of wisdom and try and come up with an account of what wisdom is based on that.",
    "And that's a legitimate thing to do.",
    "It omits something that's very important though.",
    "It omits what we saw as so central to the ancient theories of wisdom that we were looking at.",
    "If you look at Plato, even Socrates and Plato and Aristotle and the Stoics, and if you look at the Buddha, and the centrality of transformational change in wisdom is being lost in a product theory.",
    "What you also need (Fig. 1a) (draws a double headed arrow beside Product theory of wisdom)—and these two should talk to each other—you need a process theory.",
    "You need a theory.",
    "Fig.",
    "1a",
    "And this is what I think the ancient theories of wisdom are.",
    "You basically—what you're getting is (taps Process theory) an account that works something like this in a process theory.",
    "You have an independent account of what is foolishness (Fig. 1b)",
    "(writes Foolishness) and what is flourishing (writes Flourishing below Foolishness).",
    "And then what a process theory tells me is how I can overcome foolishness (draws an arrow pointing from Process theory to Foolishness) and how I can afford (draws arrow from Foolishness to Flourishing) flourishing.",
    "And then from that (draws an arrow from the arrow between Foolishness and Flourishing to Product theory of Wisdom), it tries to derive what wisdom is.",
    "Fig.",
    "1b",
    "And I think this is a very important thing to do, to try and figure out what wisdom is from an account of how one becomes wise, because first of all, it taps into this longstanding and important philosophical heritage so there's a tremendous legacy that we've explored in this course that we should be making use of.",
    "And secondly, it does something that I think is much clearer than what a product theory does, which is it picks up on the central insight that McKee and Barber talked about, which is seeing through illusion and into reality.",
    "The process theory gives you an account of what self-deception is and how you can see through it and into reality so that you can be better connected to yourself, to other people, to the world and thereby flourish.",
    "So I would advocate that in addition, not in competition, but complimentary to a product theory, we should be developing a process theory.",
    "Tells us what foolishness is, how foolishness develops, how we can overcome foolishness, how we can afford flourishing.",
    "And then on the basis of that, get into a dialogue with accounts of what wisdom is.",
    "So I think that's important.",
    "I think, I mean, the fact that Aristotle's being invoked really says we need to bring in the developmental and transformative aspects of wisdom (underlines Process theory) in a more serious manner.",
    "So that's one of my most central criticisms of Baltes and Staudinger.",
    "The other one, like I said, is they seem to make a mistake around expertise.",
    "I would now like to share with you some central criticisms made by a really seminal thinker.",
    "Somebody I've had the chance to meet a couple times and interact with.",
    "And this is Monika Ardelt (writes Monika Ardelt 2004).",
    "I think she did something really important, which is the critique she published of Baltes and Staudinger in 2004 and her ongoing work she's generated one of the wisdom scales that is used experimentally.",
    "She's generated a lot of experimental work.",
    "Monika's work is really important and central.",
    "Modal Confusion",
    "Now, I'm going to use some of my language that we've developed in this course to talk about Monika and I hope it's not imposing on her, but I think it's plausible that it's not.",
    "Her main criticism of the Baltes and Staudinger paradigm or approach is they confuse—this is her criticism.",
    "And I agree with it.",
    "They're confusing having theoretical knowledge about wisdom with being a wise person.",
    "So there's a sense in which there's a bit of a modal confusion going on in the Baltes and Staudinger theory.",
    "It's like, I have a lot of knowledge about wisdom, that doesn't make me wise.",
    "So I'm an excellent case in point of that, I would think, I mean, I've read a lot of the wisdom literature, about the philosophical, and the psychological, I teach on it.",
    "I, you know, I'm involved with this currently some experimental work, et cetera.",
    "So I think I have quite a bit of theoretical knowledge about wisdom.",
    "I would not in any way dare to equate that with the claim that I'm a wise person.",
    "While that knowledge, I think, could be argued to be necessary.",
    "I think what Ardelt is clearly pointing to is that it's nowhere near sufficient.",
    "It's nowhere near sufficient.",
    "And so what, I think, she's pointing to is the fact that a wise person—and we can think of the Socratic example here about the truth and the transformative relevance needing to be kept together.",
    "The wise person must realize these theoretical truths within a process of self- transformation, must realize this within a process of self-transformation.",
    "So people that are wise have gone through a process of self-transformation.",
    "They have achieved a significant kind of self-transcendence that allows them to embody and enact these truths rather than just having them in a propositional fashion.",
    "So, Ardelt is clearly pointing towards something missing (indicates Process theory): the importance of the process of development.",
    "She's also importantly pointing to the way in which wisdom is a project of, you know, becoming a particular kind of person.",
    "And so she—a particular kind of person living in a particular kind of world.",
    "And so she's directly pointing towards what I've articulated in this course as participatory knowing.",
    "Distinction Between Descriptive Knowledge And Interpretive Knowledge",
    "So she then brings out something very important about this distinction.",
    "She makes use of this distinction between having knowledge and being wise.",
    "And she makes use of a distinction from a really important philosopher, John Kekes (writes Kekes).",
    "I've mentioned him before that",
    "[he has] done some important work on moral wisdom and its relationship to virtue.",
    "And Kekes makes a very important distinction between descriptive knowledge (Fig. 2a) (writes Descriptive knowledge beside Kekes) and interpretive knowledge (writes Interpretive knowledge below Descriptive knowledge), which Monica points to.",
    "Fig.",
    "2a",
    "And what Ardelt is pointing to is, of course, that wisdom has to do more with this (draws an arrow indicating Interpretive knowledge).",
    "So descriptive knowledge is basically your knowledge that, you know, the cat is a mammal.",
    "Your knowledge that the cat is a predator or something like that.",
    "Interpretive knowledge is your ability to grasp the significance of your descriptive knowledge (Fig. 2b)",
    "(writes Grasping the significance).",
    "Fig.",
    "2b",
    "Now that's going to be important because that actually turns out to be a central feature in most of the theories that are now being developed about what understanding is and how it differs from knowledge.",
    "And I think what Ardelt is pointing to here by invoking Keke's notion of interpretive knowledge is the centrality of understanding (writes Understanding) to wisdom.",
    "And, of course, grasping the significance is putting us—I'm going to argue later, that this (encircles Grasping) of course is going to be, have to do with construal.",
    "And this (encircles Significance) is going to have to do about, of course, with relevance.",
    "Relevance realization.",
    "Now, I think that's very important work",
    "and I think the fact that—there's so much happening in these theories and the thing is it's happening in a very sort of concise fashion.",
    "So let's gather what we've got here.",
    "We've got the realization that we shouldn't be just talking about having knowledge about wisdom (indicates Monika Ardelt 2004), we should be going through the process of self-transcendence (indicates Fig. 1) by which we become wise.",
    "That involves, as I said, a process theory.",
    "We're going to need a process theory, not just a product theory and that what the wise person has above and beyond knowledge (indicates Descriptive knowledge), I would call descriptive knowledge, knowledge, and Interpretive knowledge, whether that's knowledge, I don't know, but that's at least what many people are considering understanding.",
    "And this has to do, as I'm going to argue that—I'm going to argue that grasping the significance of your knowledge is, you know, good construal that realizes what's relevant.",
    "get the notes!",
    "Personality Characteristics Of A Wise Person So Ardelt then talks about the personality characteristics that you should see in the wise person.",
    "So she thinks that if we want to talk about what it is to be a wise person, we shouldn't just be talking about the knowledge.",
    "Obviously.",
    "We should be talking about the kind of characteristics that the wise person has and that's important because we've already seen the deep connections between wisdom and virtue.",
    "So she thinks that there are three ways in which we can judge the relative value.",
    "We can grasp the significance of info—of our knowledge.",
    "There's three domains—no, that's not right.",
    "I don't want to say domain.",
    "There's three dimensions of our personhood that are crucial for being a wise person (erases the board).",
    "She thinks there, and up until now, we've been clearly giving these emphasis.",
    "There are cognitive factors (writes Cognitive).",
    "So what are the cognitive factors here?",
    "This, of course, is your ability to gr—to have a comprehension of the significance and meaning of information.",
    "And not just theoretically.",
    "The significance of the information for your development, for your going through a process of self-transcendence and becoming a wise person.",
    "Then there are reflective (Fig. 3a) (writes Reflective beside Cognitive).",
    "And I think the distinction here, this is picking up a bit on the way rationality has this reflective component into it, as opposed to intelligence, which is just your ability to grasp what's relevant.",
    "So I think that's something that can be integrated quite well with the discussion we had on rationality.",
    "So the reflective factors a person who has been cultivating wisdom is multi-perspectival.",
    "They're capable—excuse me, I don't know what's wrong with my throat.",
    "They're capable of engaging in multiple perspectives.",
    "They're capable of self-examination, self-awareness, self-insight.",
    "So notice again that the reflection here has an existential import.",
    "You're taking these perspectives, but you can ultimately internalize this perspectival ability onto yourself in self-reflection, self-examination, self-insight.",
    "So I think that's very valuable (draws a line connecting Cognitive and Reflective).",
    "Fig.",
    "3a",
    "And then there's an affective component (writes Affective below Cognitive and Reflective).",
    "And I'm doing this because Ardelt clearly thinks that these three are mutually influencing, mutually constraining, causally interacting with each other.",
    "So the affective dimension, she talks about in terms of compassion, I've made a case that, so—because of some of the models she invokes from Buddhism, I don't think this is inappropriate.",
    "I've made a case that the capacity for agape (Fig. 3b) (writes Agape below Affective) is the best way of understanding compassion.",
    "Fig.",
    "3b The main idea here is, and this is why I'm invoking agape, as opposed to either philia or eros is because it helps to overcome egocentrism and you can see how these two are working together (draws a double-headed arrow pointing to Agape and Reflective), right?",
    "How this (indicates Agape) is helping to overcome egocentrism (Fig.",
    "3c) (writes Overcome egocentrism below Agape) in kind of a powerful way.",
    "So this is sort of overcoming egocentrism (indicates Reflective) perspectivally.",
    "And this (indicates Affective) is overcoming egocentrism attitudinally in terms of your core motivations, your core set of priorities and preferences.",
    "Basically the way in which you're caring about the world and that you're caring is directed towards the flourishing of other persons.",
    "Fig.",
    "3c",
    "So I think that also is an important improvement in the theorizing.",
    "The fact that Ardelt is clearly giving equal priority to cognitive, I would call, you know, these are perspectival (indicates Reflective) and the perspectival aspects of rationality and, of course, the important affective things.",
    "And if you remember, this does something also, at least implicitly that we don't have in Baltes and Staudinger.",
    "And so this is one other criticism I would make on behalf of Monika Ardelt of the Baltes and Staudinger theory, which is agape starts to (draws an arrow down from Agape)—I've argued, so I'm not attributing this argument to Ardelt but I'm supplementing it to her theory because what agape gives is, it at least gives a way of trying to talk about meaning in life (Fig. 3d) (writes Meaning in life below Agape), the way Wolf talked about it.",
    "And I argued that agape helps us to get that kind of connectedness and caring that is so central to grounding meaning in life.",
    "We don't have to ground meaning in life, in being subjectively attracted to something that's completely objectively attractive.",
    "We just have to be transjectively coupled to those things we find inherently valuable because of their connection to meaning making coherence and caring.",
    "Fig.",
    "3d",
    "And so this (indicates Agape and Meaning in life) allows, well, what it affords and I would argue enables Ardelt's theory to do is to connect her account of wisdom to meaning in life in a much more direct fashion.",
    "That connection to meaning in life is not clear in Baltes and Staudinger.",
    "And if you remember Wolf argues that you can't reduce meaning in life to morality.",
    "So the connection between wisdom and virtue while central, and being discussed well by Schwartz and Sharpe, and by Baltes and Staudinger, is not reducible to, it's not sufficient for the connection between wisdom and meaning in life.",
    "By bringing in this affective component, and the idea that we are becoming wise, not just thinking about wisdom, and so the process of identification, the modal existential aspects, Ardelt is doing a lot more with, about connecting wisdom to meaning in life.",
    "And I think that's very, very important.",
    "Okay.",
    "So, you know, this is sort of understanding (Fig. 3e) (writes Understanding below Cognitive) in a sense I'm going to help develop later.",
    "This ability to pick up on what to realize the significance of your knowledge.",
    "And allow you, and again, in an existential fashion, not just abstract, theoretical, but in a way that allows you to, I would argue, ultimately see through illusion and into reality so that you can afford your development.",
    "Fig.",
    "3e Then you—that is connected to your reflective capacity, your perspectival knowing which ultimately could be linked to the stuff we saw with Grossman's work and in the Berlin paradigm of Baltes and Staudinger.",
    "The ability to take the perspective of others, being multiple perspectival and ultimately turn that back on oneself, in self-examination, internalize it in self-insight.",
    "And that those two (indicates Cognitive and Reflective) are also linked to something like a capacity for agape.",
    "And I would argue that you need these two (indicates Agape and Reflective) for overcoming egocentrism and it also—I haven't seen Monica develop this very explicitly in connection, especially with Wolf's work, but there's the real potential here of connecting wisdom to something other than virtue.",
    "It should be connected to virtue, but it should also be importantly connected to meaning life.",
    "What are some of my criticisms of Ardelt's work?",
    "Well, it clearly invokes transformational processes.",
    "It doesn't really incorporate a theory of transformative experience or an account of it.",
    "You know, like we have in LA Paul's work.",
    "It doesn't really give us a processing theory.",
    "It points towards the—and this is important—it points towards the need for a processing theory, but it doesn't give us an account of what that process looks very much like.",
    "get the notes!",
    "And it doesn't have an independent account of foolishness.",
    "It doesn't really tell us very much what foolishness is and that's something needed if we're going to, I would argue, to have a good process theory of wisdom.",
    "There is an untapped potential.",
    "This isn't so much a criticism as encouragement that there's an untapped potential, given this theoretical machinery of connecting wisdom to meaning in life in a way that compliments the connections between wisdom and virtue.",
    "So I think these are all some very important features about this.",
    "I've already indicated, you know, grasping the significance.",
    "I think this is points towards RR (Fig.",
    "3f)",
    "(writes RR below Understanding).",
    "We've already talked a lot about agape and the relationships I tried to drive between it and relevance realization.",
    "And, of course, I've already indicated this (indicates Reflective) is bringing in the recursive aspects.",
    "It's bringing a perspectival knowing.",
    "And there's also, at least in the background sense, the participatory knowing because one is coming to know oneself differently.",
    "One is coming to know the world differently, because one is going through this inherent transformational process.",
    "Fig.",
    "3f",
    "So (erases the board)",
    "we're seeing how the theory of wisdom is sort of building up.",
    "It's becoming more complex.",
    "It's bringing in different aspects of, you know, the different kinds of knowing, there is, you know, quite important aspects of relevance realization in Schwartz and Sharpe clearly in Baltes and Staudinger.",
    "Quite strongly implied in the central role, at least in the cognitive aspects—although I would argue also in the reflective and the affective aspects of Ardelt's theory.",
    "Relevance realization is playing an important role.",
    "And we can see how its getting a—we see how it's getting sort of developed and specified into and connected to other important cognitive processes.",
    "Alright.",
    "I think the next important theory we should turn to is Sternberg.",
    "So, and this is because you can see Sternberg's work (writes Sternberg) running throughout the history.",
    "Robert Sternberg's work—running through the history of the psychology of wisdom.",
    "Robert Sternberg basically really gets this off the ground with his pivotal work, not only in his own theories.",
    "But, the title this work he's done in putting together anthologies, various anthologies on wisdom or related anthologies on foolishness, Why Smart People Can Be So Stupid as one of the best anthologies on the psychology of foolishness I've seen right.",
    "He is also been tireless in trying to connect the psychology of wisdom to pedagogy.",
    "He gets—again, something that is important and central, and we see it crucially in the ancient theories of wisdom.",
    "It's not so clearly present in the current psychological theories, but the deep connection between wisdom and teaching that's, of course, clearly the case in Socrates, it's clearly the case in Jesus of Nazareth.",
    "It's clearly the case in Buddhism that they're, like in the work of the Buddha, that there's a deep connection between wisdom and education.",
    "And so he's been very tireless, not only sort of theoretically pointing out that connection, but trying to work out, if you'll allow me, I mean, this word in a complimentary sense, trying to engineer some psychotechnology, sets of practices for how to bring the cultivation of wisdom into the educational domain.",
    "So, and so for many reasons, Sternberg is a pivotal figure in the wisdom ind—the wisdom.",
    "I was going to call it the wisdom industry.",
    "That sounds like an industry for producing wisdom.",
    "In the community of people who are endeavoring to come up with a psychological cognitive scientific, even neuroscientific theory of wisdom.",
    "There are some people who are pursuing this.",
    "Meeks and Jeste are one.",
    "Other people are pursuing the neuroscientific aspects, but I'm not going to go into that in great detail right now.",
    "I'm trying to concentrate on the psychological theories, precisely because they are the ones that are most directly accessible and relevant to trying to respond to the meaning crisis.",
    "A Balance Theory Of Wisdom So in 1998, Sternberg, he had an earlier theory where he was talking and he's come back to this many times.",
    "He has a book where he tries to talk about the relationships between intelligence, wisdom, and creativity.",
    "I'm trying to follow that up in my own way, trying to show you the relationships between the intelligence, rationality, wisdom, insight, et cetera.",
    "So definitely influenced by Sternberg in that way, but he then came up with a more—I don't—I want to be complimentary of the newer theory without being completely dismissive of the older.",
    "I would say the newer theory is sort of more coherent, more tightly integrated, more explicated.",
    "So in 1998, he came up with what he's called and maintained a balance theory of wisdom.",
    "That's actually the title of the 1998 article, A Balance Theory Of Wisdom.",
    "So the notion of balance (writes Balance beside Sternberg) is going to play a crucial role in this theory.",
    "Now Sternberg starts by talking about sophia and phronesis and episteme, which is at the core of epistemological—that's what I would call propositional knowledge.",
    "He calls it scientific understanding.",
    "He quickly drops that as not being particularly relevant to wisdom, not because knowledge is unimportant.",
    "Because everybody knows the centrality of knowledge to wisdom.",
    "That's why we're spending so much time on getting knowledge in this course, but everybody pretty much also understands that wisdom is something above and beyond scientific, theoretical propositional knowledge.",
    "Okay.",
    "So that's sort of easily understandable.",
    "He also, like I said, he invokes sophia and talks about it sort of in connection with contemplation, phronesis and practical wisdom.",
    "He zeroes in like Schwartz and Sharpe on phronesis.",
    "I think that's, again, a little bit too much of a neglect of the important role of sophia.",
    "He also draws on Polanyi's idea of tacit knowledge.",
    "Remember we talked about that when we talked about subsidiary awareness?",
    "And he talks about the tacit knowledge having to do with our procedural abilities.",
    "It's relevant to attaining goals.",
    "It's not explicitly taught, we experience it intuitively, so he's invoking all kinds of important aspects about implicit learning, intuition and procedural knowing.",
    "I think that's good.",
    "I think he needs to also add to that techne.",
    "What are some important psychotechnologies?",
    "Especially since I can, I think, in fairness, say that he is trying to engineer such psychotechnologies in his pedagogical endeavors.",
    "I also think he needs to try to do what Aristotle did, which is integrated how phronesis, which is largely happening in an implicit intuitive sort of S1 kind of fashion is integrated with sophia, which can have a more reflective, perhaps a more S2 aspect to it.",
    "All right.",
    "That being said, what's the core theory?",
    "Well, the idea is we, and this is important because it's just sort of directly invoked, right?",
    "He invoked—the tacit knowledge is also...",
    "It's used as a way of— he starts talking about it as tacit understanding.",
    "So the tacit knowing, the tacit—the implicit learning, it is also a way in which Sternberg tries to invoke the capacity for understanding.",
    "Now that isn't very well explicated in the account.",
    "I'm thinking.",
    "I'm trying, I'm speculating that what I think he's trying to get at is that the machinery by which we sort of grasp the significance of our knowledge and use it to directly and intuitively cope with the world is a significant component of understanding.",
    "So I think that's why the shift is there.",
    "He then talks and the way, the reason why I want to invoke that is because the, what the tacit understanding is doing it's again, so pregnant with aspects of relevance realization, because he talks about the understanding, guiding our ability to adapt to—listen to that language, to adapt to situations, to shape them.",
    "And that doesn't necessarily mean physical.",
    "It can mean how we construe them, how we formulate our problems and the selection of environments, the selection, the moving around the exploring or exploiting.",
    "This is all very much doing with relevance realization.",
    "He talks about these abilities are needed to deal with practical problems.",
    "This is why he's emphasizing wisdom.",
    "Not as the same thing as theoretical knowledge, he's picking up on the same point that Monika Ardelt did.",
    "But he said these practical problems, quote, are unformalized or in need of—sorry, I misquoted.",
    "And that misquotation was an important mistake.",
    "These problems are unformulated, much more relevant to problem formulation or unformulated or in need of reformulation.",
    "And so he's invoking problem formulation here.",
    "He's, of course, with the notion of reformulation, he's invoking the notion of insight.",
    "And so we can see, again, the relevance realization machinery being invoked.",
    "So his theory quote involves—sorry, his theory involves quote views.",
    "Let me start that again.",
    "His theory, quote, views wisdom as inherent in the interaction between an individual at a situational context.",
    "It's inherently transjective.",
    "It's about fitting.",
    "In fact, he then goes on to say wisdom depends on the fit of a wise solution to its context.",
    "So notice the fittedness.",
    "All of the language that we've developed is being directly invoked, it's being put at the center of the theory.",
    "And I'm going to argue that what he is invoking when he's invoking balance is he's invoking optimization (Fig. 4) (writes Balance - Optimization).",
    "And I, of course, argued that relevance realization is an optimization theory.",
    "Fig.",
    "4",
    "So what's sort of beautiful about the theory is he gives a schematic diagram of it and I'll draw this for you.",
    "So here's our tacit understanding (draws a box and writes Tacit understanding inside) that's that implicit processing, et cetera.",
    "Remember, we talked about that so much in connection with flow?",
    "And what's happening is it is dealing with trying to balance your interests (Fig. 5a) (draws a triangle above Tacit understanding).",
    "Balance your interest.",
    "What you're interested in, what you find salient, important.",
    "And he talks about three kinds.",
    "There's the intrapersonal, the interpersonal and the extrapersonal.",
    "So this of course is interest between people (writes Interpersonal at the right corner of the triangle).",
    "This is interest within a person (writes Intrapersonal at the left corner of the triangle).",
    "So you can think of Plato here trying to get your various centers and what they're interested in coordinated together.",
    "And extrapersonal (writes Extrapersonal at the top corner of the triangle), having to do...",
    "I think this is ultimately how you're connected to yourself (indicates Intrapersonal), how you're connected to other people (indicates Interpersonal) and how you're connected to the world (indicates Extrapersonal).",
    "I think he's invoking, at least implicitly, the three dimensions of connectedness that go into meaning in life.",
    "And so, I think, at least implicitly, we could make a clear argument here that he's trying to connect wisdom to meaning in life (draws a bracket on the left and writes Meaning in life).",
    "Fig.",
    "5a",
    "And that this capacity (indicates Tacit understanding) for sort of zeroing in on relevant information is playing an important role in that.",
    "And the reason why I say that is because (Fig. 5b) (draws an arrow pointing up from Extrapersonal).",
    "It's not clear to me why the arrows only go one way.",
    "It seems to me that I—well, this is a criticism I would make.",
    "I think the arrows should go both ways because there's also going to be feedback.",
    "But anyways, they feed forward to the three things (draws a triangle above the second arrow) that are clearly I would argue are, which is the adapting (writes Adapting at the top of the second triangle), the shaping (writes Shaping at the right corner of the second triangle), and the selecting (writes Selecting at the left corner of the second triangle).",
    "Fig.",
    "5b I'll come back to that (indicates Balance - Optimization and erases it on the board) notion.",
    "The triangles are meant to indicate balance.",
    "And I've tried to indicate to you that,I think, I'll put that down here and we'll come back to it.",
    "That balance has to do with optimization (writes Balance - Optimization).",
    "get the notes!",
    "Okay.",
    "So what's going on here is this is a balance of your response to the environmental context.",
    "So this is balancing your interests (indicates the lower triangle and writes Interests) and this is balancing your response (writes Response beside the second triangle).",
    "Like I said, that's (indicates the upper triangle) pretty clear of our relevance realization stuff.",
    "This then is directed towards, he argues the common good (Fig. 5c) (draws an arrow above the upper triangle).",
    "Here again is where I have to step aside (writes Common good) from the value he's articulating and the theory he is generating.",
    "I agree, especially within a liberal democratic framework, a framework deeply influenced by Christianity, that the common good is an overarching value for us.",
    "I'm not sure that that is going to be universally shared by all people who I think can reasonably be deemed wise individuals.",
    "Fig.",
    "5c",
    "So if you're in a culture, that's—what this means, maybe you have to bend this (indicates Common good) so much, then it's then trivial.",
    "It doesn't mean—maybe it doesn't mean common in the sense of shared by everyone.",
    "It might be, you know, you're in some sort of hierarchical, feudal society and the common good isn't that everybody equally benefits from it.",
    "It might be more that, you know, everybody is working well together, some sort of justification like that.",
    "I'm not trying to justify a hierarchical society or anything like that.",
    "I want you clearly to understand.",
    "I think this (indicates Common good) is an important value.",
    "I'm just concerned here that Sternberg is being anachronistic here and he's using one of our central values and attributing it to every person who has been wise?",
    "I'm not, I'm not sure.",
    "I mean, I think you could make a case that that's clearly the case in Jesus or the Buddha.",
    "I'm not sure that it's the case for Marcus Aurelius and all of the Stoics.",
    "It's clearly not the case for Epictetus.",
    "So I'm hesitant about putting that up there.",
    "I'm going to advise that there's something I think more, more broadly comprehen—I think the things that we should put up here (erases Common good) are things that there's already a consensus on a broader notion of virtue.",
    "Oh, sorry.",
    "That really makes it sound like I'm making an emphasis here that we're really concerned with is virtue (Fig.",
    "5d) (writes Virtue above the upper triangle).",
    "Right?",
    "And because of this (draws an arrow from Meaning in life to Virtue), I think, and meaning in life (writes + Meaning in life).",
    "Fig.",
    "5d",
    "Okay.",
    "I have changed (indicates Virtue + Meaning in life) that is not in his model.",
    "I have changed it because of, again, I'm worried that his particular term is, anachronistic and idiosyncratic to our particular historical cultural context.",
    "It's important to also note that, running alongside of this (draws a rectangle to the right) is values (Fig. 5e)",
    "(writes Values inside the rectangle) and then arrows that are kind of indeterminately pointing (draws two arrows from Values pointing to the left).",
    "Fig.",
    "5e",
    "So trying to interpret what that means is something like, the wise person is being constrained by values.",
    "What those values are are not clear.",
    "Again, is it that we're talking about things like meaning in life and the virtues (Fig. 5f) (draws an arrow from Meaning in life to Values and another arrow from Virtues to Values)?",
    "Perhaps.",
    "Fig.",
    "5f",
    "So another criticism I have is that this part of the theory (indicates the part where Values are located), this part of the diagram, the way it's operating is unclear.",
    "It might be something as basic as the wise person is working normatively.",
    "They're trying to get the best.",
    "And then this (indicates the Values) is not that interesting a claim because wisdom is itself a normative notion.",
    "And therefore the fact that the wise person is being governed by normativity is almost definitional.",
    "If it means something specific like the wise person has a specific set of values that needs to be explicated and defended.",
    "So I don't know quite what to do with that.",
    "So I have to sort of leave it somewhat inert.",
    "What I would argue, and I think what these triangles indicate and what the language indicates is that the balance is used to adapt to shape and select environments.",
    "And what is this balance?",
    "He argues that something like Piaget's equilibration between assimilation and accommodation.",
    "He argues that it's a balance between coping with novelty and proceduralization.",
    "You see what he's doing, what the balance here is?",
    "It's balance that is directly invoking a lot of the machinery we've talked about relevance realization, and he's clearly not invoking sort of a spatial balancing.",
    "He's invoking optimization.",
    "He's invoking optimization.",
    "So I think this theory is also a theory that is deeply integratable with a lot of the material we've been discussing.",
    "Relevance realization is playing a key role in terms of the balancing and this (indicates the upper triangle) process here also down here (indicates Tacit understanding) at the level of grasping the significance, at least intuitively of our knowledge and the patterns we picked up from the environment.",
    "There's deep connections to, or at least there's a real potential in Sternberg's theory for making deep connections to meaning in life (taps the lower triangle).",
    "Because as I said, I think this has played clearly how you connect to yourself (indicates Intrapersonal), how do you connect to others (indicates Interpersonal), how do you connect to the world (indicates Extrapersonal).",
    "Right?",
    "There's important possible connections, possibly (indicates Values)—I'm unsure between this (indicates Values) and virtue and meaning in life.",
    "So this is all very good.",
    "And like I said, overall, this is clearly a dynamic optimization and it's invoking something very much like relevance realization.",
    "All right.",
    "What are some of (the) criticism?",
    "Well, I'll leave that on the board for now.",
    "What are some of the criticisms of Sternberg?",
    "This is still in the end, a product theory.",
    "It's not a process theory.",
    "It doesn't make the sort of, I mean, this (indicates Tacit understanding) avoids equating wisdom with expertise and that's a powerful advantage of this theory, but there, like I said, this domain, the values unexplicated, it could either be somewhat trivial, because it's almost tautological that the wise person is acting according to normative standards, or it could be a much more controversial claim that specific values are involved.",
    "I don't know.",
    "I've already told you (indicates Virtue + Meaning in life)",
    "I don't think that the common good is the goal of all wise people that see seems anachronistic to me in an important way.",
    "The theory could clearly be improved by getting clear about the nature of tacit implicit learning.",
    "Getting clearer about optimization, getting clearer about relevance realization, getting clearer about how relevance realization connects to meaning in life.",
    "So all of those things would be developed if one started to develop a process theory and not just a product theory.",
    "So Sternberg needs an independent theory of foolishness.",
    "Now he has a theory of foolishness, but it's not independent.",
    "His theory of foolishness is basically foolishness is an imbalance in these things.",
    "So what foolishness is, is basically a lack of wisdom.",
    "And again, while that's definitionally the case, what you need is an independently constructed theory of wisdom—sorry, a theory of foolishness.",
    "You need a theory of foolishness that takes in hand the centrality of seeing through illusion and into reality, that has an independent account of how we're self-deceptive, how we're self-destructive, how that operates, how that unfolds, because we see that in many of the ancient wisdom theories and how we can ameliorate that.",
    "So I think that that is missing in a central way from Sternberg's work.",
    "get the notes!",
    "Okay.",
    "So I think in the end, what I want to do is try and draw all of these together.",
    "I've tried to indicate important points of improvement.",
    "One theory, with respect to the other relative strengths and weaknesses.",
    "I've also tried to indicate powerful points of convergence and connection to the ideas of relevance realization that we've been developing in this course because that's part of the big possibility diagram.",
    "To show how the machinery of relevance realization can help to explain what it is to be wise and help to explain, as I'm going to argue, what it is to become wise, to give a processing theory.",
    "So we want to take that up in the next account.",
    "And I'm somewhat hesitant about that.",
    "Because you know, I'm sort of okay with, you know, offering you proposals of—well, I'm not, I mean, I'm very cautious about proposals, about consciousness and other things like that, but, you know, we've got proposals about higher states of consciousness and insight and that.",
    "And I've tried to make arguments for that, and I'm going to make an argument that I have made with Leo Ferraro about wisdom.",
    "But there—it's something —it borders on hubristic to talk about this in that way, but I'm going to try and draw on this and at least show you a plausible way in which we can draw all of this material together with the theory of relevance realization and come up with an account of wisdom.",
    "And then, I think, also subject that theory, at least the version that was published in 2013 to some important criticisms.",
    "And then hopefully then be in a place where we can fold it back into the connections between wisdom, the cultivation of wisdom, the cultivation of meaning in life, the cultivation, the pursuit—are these the right verbs?—of enlightenment.",
    "How these are all connected and how that can be situated into the larger project of awakening from the meaning crisis.",
    "Thank you very much for your time and attention.",
    "- END - Episode 44 Notes To keep this site running, we are an Amazon Associate where we earn from qualifying purchases",
    "Herbert Simon Herbert Alexander Simon was an American economist, political scientist and cognitive psychologist, whose primary research interest was decision-making within organizations and is best known for the theories of \"bounded rationality\" and \"satisficing\".",
    "Bounded Rationality Bounded rationality is the idea that rationality is limited when individuals make decisions.",
    "In other words, humans' \"...preferences are determined by changes in outcomes relative to a certain reference level...\" as stated by Esther-Mirjam Sent (2018).",
    "Igor Grossman Leo Ferraro Publication: Relevance, Meaning and the Cognitive Science of Wisdom Monika Ardelt Susan Wolf Susan Rose Wolf is an American moral philosopher and philosopher of action who is currently the Edna J. Koury Professor of Philosophy at the University of North Carolina at Chapel Hill.",
    "L. A. Paul Laurie Ann Paul is a professor of philosophy and cognitive science at Yale University.",
    "She previously taught at the University of North Carolina at Chapel Hill and the University of Arizona.",
    "She is best known for her research on the counterfactual analysis of causation and the concept of “transformative experience.”",
    "Robert J Sternberg Robert J. Sternberg (born December 8, 1949) is an American psychologist and psychometrician.",
    "He is Professor of Human Development at Cornell University.",
    "Book Mentioned:",
    "Wisdom: Its Nature, Origins, and Development - Buy Here A Handbook Of Wisdom: Psychological Perspectives - Buy Here The Cambridge Handbook of Wisdom - Buy",
    "Here Why Smart People Can Be So Stupid - Buy",
    "Here Applying Wisdom To Contemporary World Problems - Buy",
    "Here Teaching For Wisdom, Intelligence, Creativity, and Success - Buy",
    "Here Wisdom, Intelligence, And Creativity Synthesized - Buy",
    "Here Article Mentioned: A Balance Theory Of Wisdom Thadeus Meeks Dilip V. Jeste Dilip V. Jeste, M.D. is an American geriatric neuropsychiatrist, who specializes in successful aging as well as schizophrenia and other psychotic disorders in older adults.",
    "Sophia Sophia is a central idea in Hellenistic philosophy and religion, Platonism, Gnosticism and Christian theology.",
    "Phronesis Phronesis is an ancient Greek word for a type of wisdom or intelligence relevant to practical action, implying both good judgement and excellence of character and habits.",
    "Episteme Episteme is a philosophical term that refers to a principled system of understanding; scientific knowledge.",
    "Michael Polanyi Michael Polanyi was a Hungarian-British polymath, who made important theoretical contributions to physical chemistry, economics, and philosophy.",
    "Tacit knowledge Tacit knowledge or implicit knowledge—as opposed to formal, codified or explicit knowledge—is knowledge that is difficult to express or extract, and thus more difficult to transfer to others by means of writing it down or verbalizing it.",
    "This can include personal wisdom, experience, insight, and intuition.",
    "Other helpful resources about this episode: Notes on Bevry Additional Notes on Bevry Ep.",
    "43 - Awakening from the Meaning Crisis - Wisdom and Virtue Welcome back to Awakening from the Meaning Crisis.",
    "Last time, we took a look at the work of Stanovich and sort of culminating ideas coming out of the rationality debate.",
    "Tried to expand the notion for you the need for cognition, talked a little bit more about problem finding and the generation of a problem nexus, and then also the affective component of that wonder and curiosity, and sort of balancing them off together.",
    "And then more specifically, looked at Stanovich's theory of foolishness, which he calls dysrationalia.",
    "And we looked at the idea of dual processing S1 and S2.",
    "And the idea that what makes you foolish is S1's functioning that makes you leap to conclusions, interferes with the inferential processing of S2.",
    "You leap to conclusions inappropriately, and that's what causes you to be biased in your processing, self-deceptive, foolish, et cetera.",
    "And then what active open-mindedness does is it foregrounds S2 and protects it from undue interference from S1 and that's all very good in a theoretical context, but we took a look at the work of Jacobs and Teasdale and said, but in a medic—sorry, in a therapeutic context, the opposite is the case.",
    "What you need is you need that machinery of leaping to work well.",
    "And we took a look at the work of Baker-Sennett, and Ceci showing that that ability to leap, cognitive leaping is actually very powerfully predictive of insight.",
    "And that's what you need in therapy.",
    "You need insight.",
    "Powerful kinds of insight to break you out of the ways in which you're confronting, you know, existential entrapment and inertia ignorance, and you cannot infer your way through a transformative qualitative change.",
    "So I proposed and Teasdale also has independently proposed this, that we need a cognitive style that foregrounds S1, puts us into a state for triggering insight, tends to background and constrain S2 processing, that inferential, argumentative processing.",
    "And that's mindfulness.",
    "We know—we have evidence that mindfulness facilitates insight and mindfulness is also increasingly being incorporated into therapeutic settings, precisely for its capacity to generate cognitive flexibility and afford insight.",
    "So we're noticing that what we're needing is—because the relationship between S1 and S2 is opponent and not adversarial, we're going to need some higher order way of coordinating these two cognitive styles, active open-mindedness and mindfulness, so that we can optimize the enhancement in rationality of the relevance realization that is at the core of our intelligence.",
    "And then took time before we passed to explicit theories, psychological theories of wisdoms to note this idea.",
    "That how you are relating to your intelligence and applying your intelligence to itself (Fig. 1) (writes Intelligence and draws an arrow from it pointing back to itself).",
    "The degree to which you problematize your own intelligence and try and improve it, we can see that as rationality (writes Rationality).",
    "And then I suggested to you, I proposed to you the possibility that when I do this (draws an arrow from Rationality pointing to Rationality), when I recursively and reflectively use my rationality to enhance and optimize my rationalities by enhancing perhaps the relationship between the component styles of mindfulness and active open-mindedness, then I am moving towards wisdom (writes Wisdom).",
    "We took a look at that.",
    "Fig.",
    "1",
    "And in connection with this (emphasizes the circular arrow for Intelligence), we took a look at the work of Dweck (writes Dweck beside Intelligence) and again, making the argument that the relay—the way you relate to your higher cognitive processes, your meaning making problem solving capacity is not just intellectual or information processing, it's deeply existential.",
    "And we saw the work on mindsetting (writes Mindset below Dweck) and that the way you identify with your intelligence, the way you're framing, how you're identifying with your intelligence has a tremendous impact on your need for cognition, your problem solving, your behavior, your proclivity towards deception, self-deception, et cetera.",
    "Okay (erases the board).",
    "So we've learned a lot along the way that I think has given us a good framework with which we can critically and constructively engage with some of the, I think, representative theories of wisdom.",
    "Let's remember earlier on that we have already took a look at a central review of some of those theories—the work of McKee and Barber.",
    "Showing us that they were not trying to give a comprehensive theory of wisdom, they were just trying to find a central feature.",
    "And the central feature was seeing through illusion and into reality.",
    "And then we took that up as how does one get comprehensively reliably, systematically better at dealing with self deception.",
    "And that's how we got into the rationality debate.",
    "That's how we're here.",
    "So we've done a lot to unpack that intuition.",
    "Well, it's more than an intuition.",
    "It's a conclusion of the argument.",
    "The very careful argument made by McKee & Barber that at the core of wisdom is, what I would argue, is rationality the systematic and reliable ability to overcome self-deception.",
    "get the notes!",
    "The Connection Between Wisdom And Virtue Now let's take all of this.",
    "And like, as I said, let's put it into dialogue with some existing theories.",
    "The first theory I want to take a look at isn't a comprehensive theory of wisdom.",
    "But nevertheless, it's instructive, because it brings up some core components of the theory of wisdom and that's something that's exemplary.",
    "Something we need to consider.",
    "It discusses the relationship between wisdom and virtue, which is an idea that's taken up explicitly by one of the core theories of wisdom, which is the work of Baltes & Staudinger known as the Berlin paradigm.",
    "But before we do that, in order to examine the connection between wisdom and virtue, I want to take a look at the work of Schwartz and Sharpe (writes Schwartz and Sharp 2006).",
    "Schwartz and Sharpe and it's 2006 is the main article.",
    "Later on, there was a book written—I think 2010?—called Practical Wisdom, which is much more extensive, but I'm relying on this article because I think that was a sort of clear and concise presentation of the argument.",
    "The article is called Practical Wisdom: Aristotle Meets Positive Psychology.",
    "Of Aristotle, you know, about, and he's been invoked and discussed repeatedly.",
    "The positive psychology.",
    "You remember, we talked about this when we were talking about 4E cognitive science.",
    "Remember what positive psychology is about?",
    "Positive psychology is the idea that we should study the mind, not only how it breaks down into its parts, we should also study it in terms of how it excels as an integrated system as a whole, because that excellence, that excelling beyond can often reveal powers and principles that work within our mind that normal cognition and pathological cognition do not reveal.",
    "So positive psychology studies states that are considered excellent.",
    "Now, what Schwartz and Sharpe are interested in is they're interested in some work done by Peterson, not Jordan Peterson, another Peterson.",
    "Peterson and Seligman, where they're discussing virtue and they're discussing virtue of course, as a form of human excellence.",
    "And so they study, they list a bunch of virtues and Schwartz and Sharpe, sort of, stand aside from that.",
    "And they note some difficulties with this idea.",
    "This list of virtues, you know, that you should be honest, you should be courageous, right?",
    "Things like that.",
    "Notice what we have here, the presentation of the virtues carries with it the strong implication that they're logically independent from each other, or to use language you're familiar with, what we're given is a feature list of virtues.",
    "We're given a feature list of virtues (Fig. 2) (draws several horizontal lines parallel to each other) without any indication of how they relate to each other.",
    "And see, in fact, there seems to be the assumption that they're logically independent from each other.",
    "A very questionable assumption, right?",
    "Instead, what we should be looking for as a feature schema (draws a bracket beside the horizontal lines), we should be looking for a structural functional organization that helps to explicate and explain how virtues relate to each other.",
    "Fig.",
    "2",
    "So it's important to note that the feature list carries with it the implication that what you should simply do is maximize each virtue in a—and right away that tells you an inadequacy of the feature list.",
    "I mean, if I maximize honesty, if I'm always as honest as I can possibly be, I will at times be cruel.",
    "I will have given up on kindness, right?",
    "If I meet people and just say, \"Oh, I need to tell you, you're looking uglier than you did yesterday.\"",
    "I need to tell you that because it's being honest, maximally honest, right?",
    "We don't think of that person as being excellent.",
    "We think of that person as being an asshole, right?",
    "And so that's important.",
    "That's important right away to notice that we're not trying to maximize the virtues.",
    "We're trying to get some optimal relationship between them and the ancients had a, at least these ancient Greeks had a very stronger version of this.",
    "They had the idea that the virtues were actually significantly interdependent with each other.",
    "And there's two ways in which they could be interdependent.",
    "They could form an interdependent system or they could all be different, different versions of some core ability.",
    "I might come back to that if I have time, but I want to get into the core argument.",
    "So the core argument is we should talk about the relationship between the virtues.",
    "And as soon as we do that, we can see some important issues coming to bear.",
    "So what they do is they talk about a couple of situations in which we can see virtues in conflict with each other.",
    "So one example they give is, well, an example that relates to something I just said, they give the example of you're a bridesmaid",
    "and like time is running out and you're with the bride or at least the intended bride.",
    "What's the metaphysical status before the wedding?",
    "Are you a bride?",
    "Like a potential bride?",
    "I don't know.",
    "Anyways, they're with that person and they're trying on wedding dresses and time is running out and they're asking you, \"Well, how do I look?\"",
    "And if you like, so you're caught between, you know, being honest, being kind and being helpful, right?",
    "You could, you could just be totally kind, \"you look wonderful.",
    "Oh, you're beautiful.\"",
    "That might—that's maybe not the right thing to do, right?",
    "You could be honest, \"you look ugly.",
    "It's hideous,\" right?",
    "\"Such a mistake.\"",
    "Or you could try to be helpful, like, \"we should—we're running out of time.",
    "Um...\"",
    "But then what do you say?",
    "And how do you balance them off?",
    "Do you just give up honesty?",
    "Do you just lie?",
    "No.",
    "Do you give up honesty?",
    "Kindness?",
    "Are you just brutal?",
    "No.",
    "Do you forget that you're trying to be helpful and you're under time constraints.",
    "No.",
    "What do you do?",
    "Another example they give is—this is one that's pertinent to me.",
    "You're grading an assignment for a student.",
    "Now the student has, you know, they've made terrific progress (Fig. 3) (draws a northeast pointing arrow).",
    "They've really overcome some barriers.",
    "They've gone from like, you know, a low C and they've been improving and they're getting into a high B.",
    "Now, if I grade this, if I try to grade this paper as completely objectively as I possibly can.",
    "There's a good chance that that feedback will stop (draws an arrow pointing at the tip of the first arrow) that arc, will stop that growth and the person will remain a B student.",
    "But if I just give them a little bit of encouragement, if I extend it (writes A) and is this lying, because what am I doing with marking?",
    "Am I marking what they've done?",
    "Or am I also simultaneously indicating what they can do?",
    "So if I give them a little bit more, if I push them into the A range (writes - beside A) that might actually, like in a self-fulfilling prophecy, lift them into an A student.",
    "And what's my moral obligation here?",
    "Is my moral obligation to give them brutally objective truth, or is my brutal—is my moral obligation to make them and afford them to be the best student they can possibly be?",
    "What do I do?",
    "What do I do?",
    "Fig.",
    "3",
    "Okay.",
    "What these dilemmas make clear is that the virtues are not independent from each other.",
    "And we're not trying to maximize between them.",
    "We're trying to optimize between them in an important way.",
    "Wisdom",
    "As The Higher Order Ability Dealing With Relevance",
    "Now this brings up some very important issues, right?",
    "So what it brings up is it brings up some, when we take a look at the dilemmas, we start to see some important issues of conflict.",
    "We start to see some important things about our relationship to the virtues.",
    "Let me read a quote from you.",
    "Real life situations do not come labeled with the needed virtues or strengths attached.",
    "Notice how this is the categorization, the demonstrative reference and all the stuff we talked about.",
    "And notice how they zero right in on it, because notice what they say next.",
    "There is thus the problem of, here it is, and this word is emphasized in the original, there's the problem of relevance.",
    "Which is the relevant virtue to bring to bear?",
    "And then, of course, not only is that—so, you see that?",
    "Do I bring honesty—is honesty the relevant virtue in these examples?",
    "Is mentorship the relevant virtue?",
    "Guidance, good guidance?",
    "Is kindness?",
    "Is being helpful?",
    "What's the—what are the relevant virtues?",
    "And, of course, what's also shown is the virtues can conflict with each other.",
    "They often pull you into different kinds of behavior.",
    "Different kinds of behavior.",
    "Now let's bring another thing back we often, and this is something that Schwartz and Sharpe are gonna make a lot out of.",
    "We often represent virtues with rules, and we've talked about this when we talked about rules.",
    "Remember, and—remember this rule and this is a virtue rule.",
    "Be kind (writes Be kind).",
    "Do you remember the problem with that?",
    "That rule doesn't specify its conditions.",
    "It doesn't specify— this is the problem with specifications—it doesn't specify it's conditions of application.",
    "Being kind to my son is not the same thing as being kind to my partner.",
    "It's not the same thing as being kind to my students.",
    "It's not the same thing as being kind to my friends.",
    "Not the same thing as being kind to a stranger.",
    "Not the same thing as being kind to a stranger on the street and being kind to a stranger, somebody you've just met at a funeral.",
    "These are all different.",
    "Remember that?",
    "That's why you can't capture relevance, your cognitive commitment in a rule because you just have, you'd have to just get an ever-expanding penumbra of rules for how to apply and specify that rule.",
    "Rule application specification depends on relevance realization.",
    "In fact, unlike Schwartz and Sharpe, I think all the problems they list, the problem of relevance, it's clearly a problem of relevance realization, the problem of conflict is a problem of determining which is more important, right?",
    "And the problem, as I've just argued, of specification is also a problem of determining relevance.",
    "get the notes!",
    "I would add—so they specified these three interconnected problems as I've argued, relevance, conflict and specificity.",
    "I would add a fourth that they don't talk about.",
    "And this has to do with the fact that sometimes the best response to a situation is to realize that I need to develop a virtue that I do not have.",
    "It's an aspirational response.",
    "Rather than a select—which of my virtues should I apply?",
    "Or there—how do I specify—this might be, \"Oh, geez, I'm lacking a virtue that I need.",
    "I need to cultivate a virtue that I do not have.\"",
    "So I would add in there, in addition to the problem of relevance, conflict and specificity, there's the problem of development.",
    "The need to aspire to acquire virtues you do not have, and I've already shown you how much that developmental process is dependent on capacity for insight and qualitative transformative experience, et cetera.",
    "Alright, so what are they proposing?",
    "They're proposing that we need a higher order.",
    "So here's the virtues (erases the bracket beside the horizontal lines), right?",
    "We need a higher order ability that deals with relevance (Fig. 4a) (places a bracket above the horizontal lines and right Relevance).",
    "They had put it as a list, but I've tried to show you how they're related.",
    "Conflict.",
    "Specificity—specification, sorry.",
    "Development (writes Conflict, Specification, Development under Relevance).",
    "Well, what would that be?",
    "Well, they argued that's wisdom.",
    "They argued that that's wisdom (writes Wisdom).",
    "Fig.",
    "4a Wisdom is what you need.",
    "Notice what the argument they're making here.",
    "Given the fact that they are not logically independent (indicates the horizontal lines), given that in very many situations, all of these issues (indicates Relevance, Conflict, Specification, Development) are brought to bear and I'm arguing.",
    "And I think it's fair that it centers on the ability to determine relevance.",
    "You need wisdom in order to be wise (Text overlay appears saying 'John means \"You need wisdom to be virtuous\" instead of \"You need wisdom to be wise\").",
    "In fact, one of the interpretations of the Greek, the ancient Greek idea of the interdependence of the virtues is not that the virtues are all constraining on each other, but that each virtue is just a particular way in which you're wise in a situation (Fig. 4b) (draws many arrows from wisdom pointing to each of the horizontal lines).",
    "So to be kind is how to be most wise in this situation, to be honest is how to be most wise in that situation.",
    "So that version of the interdependence of the virtues really, really tightly ties the virtues to wisdom.",
    "Either way, there is a deep connection between the cultivation and the pursuit of a virtuous way of life and the cultivation of wisdom.",
    "Fig.",
    "4b Sophia Vs.",
    "Phronesis Now, this is where Schwartz and Sharpe, and this is why their book is entitled Practical Wisdom.",
    "And that's why the title of the article is Practical Wisdom because they call back to Aristotle's distinction (Fig. 5a) (writes Aristotle's distinction).",
    "This is the distinction between sophia, which is in philosophia, and phronesis (writes Sophia and Phronesis beside Aristotle's distinction).",
    "Both of these words can be translated as wisdom.",
    "This (Sophia) is often translated as theoretical wisdom and then that becomes problematic because that's often assimilated to our idea of theoretical knowledge.",
    "And then we lose a lot of what sophia is.",
    "And then Phronesis is often translated as practical wisdom.",
    "Fig.",
    "5a",
    "So what Schwartz and Sharpe want to argue is that Phronesis is what you need for virtue (Fig. 5b) (writes Virtue below Phronesis).",
    "Phronesis is the ability to be very contextually sensitive to exercise good judgment, to know what to do in this situation.",
    "So it overlaps very considerably with, you know, the relationship between procedural knowledge, knowing how to do various things, knowing how to be honest, knowing how to be kind and perspectival knowing, a situational awareness of what is best fitted here, what is most appropriate for here.",
    "Fig.",
    "5b",
    "And so, you can see clearly why phronesis is relevant.",
    "And one of the things they argue, which is very interesting, is they really resist (indicates Phronesis), and I think appropriately, trying to understand Phronesis as having rules (Fig. 5c) (writes Having rules below Phronesis).",
    "So here they're very sort of critical of a Kantian idea of being virtuous as sort of specifying as your beha—whether or not this is Kant's view, it's not something I'm going to get into.",
    "This is certainly a view that many people have that the point, the way in which you are virtuous is to have a set of rules, moral commandments, and that you follow those rules as best—you can't.",
    "And then what that can lead to and Schwartz has been critical of this elsewhere in some talks you can find on YouTube, for example, this has been sort of—this can lead to the attempt to try (writes Legislation) and legislate everything, to try and specify everything, in term—of how we should behave in terms of rules.",
    "Fig.",
    "5c",
    "And they're critical of that because, first of all, it's impossible.",
    "Notice the example of, be kind.",
    "If I try to make a law that we should be kind then I have to make laws about all these different ways in which I specify being kind, I'd have to make laws that tell me when I should give a preference to kindness over honesty across all possible—like, it's just, it's impossible.",
    "But you can get into an illusion.",
    "This (indicates Legislation and Having rules) is part of Schwartz's—that you can somehow replace people becoming wise with people having laws.",
    "Now, obviously I am not proposing anarchy that we shouldn't have laws or et cetera like that.",
    "That's not Schwartz's point.",
    "He's not proposing that.",
    "That's absurd.",
    "What he's proposing is to step back and realize that that we should have this balance between proposing legislation and requiring from people that they cultivate wisdom.",
    "Okay.",
    "So he's making that argument",
    "and I think that's something that we should take into account.",
    "We should ask ourselves not just, will this legislation reduce harm?",
    "That's a really important question.",
    "Okay.",
    "For sure.",
    "But we should also, and I think this is also an important question.",
    "Will this legislation tend to make people less likely to pursue the cultivation of wisdom and virtue?",
    "Okay.",
    "So you have to, you have to think about that is Schwartz's argument.",
    "I think that's an argument that should be taken seriously.",
    "And that's why, of course, he keeps making it",
    "and he's getting a considerable audience around it.",
    "get the notes!",
    "The Need For Both Sophia and Phronesis Okay.",
    "Let's go back to the main point.",
    "They tend to leave this (draws an arrow from Sophia) out because they tend to associate sophia, I think, unfairly, with having rules.",
    "They assimilated, I think too much to theoretical knowledge and the possession of propositions (writes Proposition below Having rules), of course, rules are propositions you're proposing what people should do.",
    "Proposing very strongly.",
    "And they see sophia as theoretical knowledge, largely propositional.",
    "I think that's an unfair representation of sophia and other people have pointed this out.",
    "So, whereas I think, look, this (indicates Phronesis) is about being very contextually sensitive and that's very important because that allows me to generate the process needed in this situation.",
    "I need to start behaving, you know, in this sort of balance between being kind and honest, right.",
    "But I also need this (indicates Sophia).",
    "And I think instead of thinking of this as rules and the possession of propositions, or are sort of analogous to the Kantian model, let's think of this instead as the awareness of principles (Fig.",
    "5d) (writes Principles beside Sophia).",
    "So phronesis is about getting you into a process (writes Process beside Phronesis), the contextual sensitivity the perspectival situational awareness, activating the right procedures in the appropriate way so that I fit the situation.",
    "Fig.",
    "5d",
    "That's great.",
    "But I also need a cross contextual sensitivity.",
    "I need to pick up on things that are generalizable across different contexts.",
    "And, of course, that is partially what we're trying to do with our laws (indicates Legislation).",
    "Hence the connection, but to reduce this (indicates Principles) to just—the ability to generate propositional knowledge, I think is a mistake.",
    "That's not what sophia is.",
    "Sophia is something like a deep kind of ontological depth perception.",
    "It's to be able to see deep underlying principles, because what I need to know really, and this was Aristotle's point, right?",
    "I need both of them.",
    "I need to know how to put principles into processes (Fig. 5e)",
    "(draws arrow from Principles to Process).",
    "And I need to know how to regulate processes with principles (draws an arrow from Process to Principles).",
    "That's what it is to put a principle into practice and to practice in a principled manner.",
    "Fig.",
    "5e",
    "So, I would argue against Schwartz and Sharpe that you need both sophia and phronesis.",
    "You need something that is trying to pick up on cross contextual invariants, and you need something that is designing, helping you to, and of course, this is in line with the relevance realization model, I've argued something that is the aspect of wisdom that is about contextual sensitivity.",
    "What's different here?",
    "What's special here?",
    "How do I fit myself to this specific situation, as opposed to, how do I generalize across these many situations?",
    "And what I want is an opponent relationship between them so that I can discover powerful principles and put them into effective practice.",
    "And so that I can regulate my practices with well-justified principles.",
    "So I think that that's a very crucial issue.",
    "Phronesis In Terms Of Expertise There's one other issue about Schwartz and Sharpe that I want to come back to.",
    "I think they're right in saying that phronesis is a kind of know-how, procedural knowledge, I think it's more.",
    "It's also perspectival and potentially participatory, but at least perspectival.",
    "And one of the things they do is they talk about this in terms of the language of expertise (Fig. 5f) (writes Expertise below Phronesis) of being an expert, which is different.",
    "And what they're trying to do with that contrast is an expert doesn't necessarily possess the best theory they, but they don't have the knowledge that—the expert has the best know-how.",
    "Expertise is a kind of excellence in know-how.",
    "I think because they've—focusing on phronesis is separate from sophia, and they've thought of know-how without thinking also of the perspectival knowing.",
    "I think this is a mistake.",
    "Fig.",
    "5f",
    "Here's why I think that.",
    "Expertise.",
    "Well, I'm trying to be careful here.",
    "There's a way which this—we can equivocate with this word.",
    "We can just mean that we can—what we sometimes use this to mean, just good (Fig. 5g) (writes Good beside Expertise).",
    "Like, you know, excellent.",
    "You know, that's what expertise is, and that's a very loose way of talking.",
    "But if you're trying to use it within psychology in a more precise manner, expertise is a domain specific thing (writes Domain specific).",
    "And we've talked about this before, right?",
    "Fig.",
    "5g So I can become a tennis expert.",
    "My know-how can rise to a level of authority and notice that my being an expert in tennis, you've done this before, but let's do it again.",
    "My being an expert in tennis, doesn't give me any special authority over squash.",
    "In fact, my expertise in tennis can dramatically interfere with my playing squash.",
    "So typically what happens in expertise is it tends to be very domain specific, which is precisely why you can get very focused training on it and become very good at, like, tennis.",
    "Here's my problem with understanding Phronesis and therefore also the relationship with virtue on the model of expertise.",
    "The domain specificity of expertise, if we're using the current term carefully, is not what I need here.",
    "It's not what I need.",
    "And you're saying, \"Oh, but phronesis is context sensitive.\"",
    "Yes, it is.",
    "And perhaps that's the source of the confusion.",
    "Being context sensitive isn't the same thing as having expertise.",
    "And you say, but that sounds similar.",
    "Well, let's pull it apart, right?",
    "What phronesis is.",
    "And so let's do this very carefully.",
    "Phronesis is not like expertise in tennis, which I can only apply here.",
    "And in fact, if I try to transfer it to something even similar, it will interfere.",
    "I would argue that what phronesis is, is my ability to be sensitive in this context, and sensitive in this context ,and sensitive in this context.",
    "And that is very, very different, that is very, very different from expertise.",
    "So what we need is a domain general ability.",
    "This is not a contradiction.",
    "Your ability to be contextually sensitive is itself a domain general ability.",
    "I have to be able to be contextually sensitive in many different domains.",
    "And so I'm arguing that there's a bit of confusion here.",
    "And if you pull it apart, what we need is an ability to be contextually sensitive, but in a domain general way, across many domains.",
    "So, you know, I think things like, well, intelligence and rationality, or I would argue your ability to realize relevance, which always has a contextually sensitive component to it, are much better ways of understanding phronesis than expertise because this, those ways of talking are domain general.",
    "They have—each one of them has an aspect that is the domain general ability to be contextually sensitive here and here and here.",
    "And that's important because you know what, you're not foolish generally in a domain specific way.",
    "Specific domains may make you more foolish, but we all wonderfully have the ability to be foolish in almost every domain of our life.",
    "Often many domains simultaneously in a disastrous chaos.",
    "So I would argue that we shouldn't confuse that phronesis is about context sensitivity with expertise, which is locked to a particular domain.",
    "We should think of something much more like.",
    "Intelligence, rationality, relevance realization, which can apply across multiple domains, make you a general problem solver and deal with the domain generality of your capacity for foolishness (erases the board).",
    "And so I think my two main response.",
    "So let's try this together.",
    "The argument for the connection between wisdom and virtue, I think is very powerful, solid argument.",
    "The argument that that should make us more hesitant to trying to capture wisdom just with virtue, just with rules.",
    "I think that's an argument I'm sympathetic with.",
    "I think that's going in the right direction.",
    "The argument that phronesis is all we need for virtue.",
    "I question, I think following Aristotle that phronesis and sophia should be in a very powerful opponent relationship, you know, trying to get principles into processes and processes regulated by principles, et cetera.",
    "And the idea of trying to capture the procedurality of phronesis with the notion of expertise, I think, is a confusion, as I've argued.",
    "And we should put that aside.",
    "Okay.",
    "I now want to pick up on one of the—I mean, I think this is a fair way of saying it.",
    "One of the seminal theories, psychological theories of wisdom.",
    "In many ways, this theory turned the investigation of the psychological investigation of wisdom into an experimental, empirical process.",
    "And so this is the important work of Baltes & Staudinger (writes Baltes and Staudinger).",
    "Okay.",
    "It's called the Berlin wisdom paradigm.",
    "They're both working in Berlin.",
    "Obviously, they're German.",
    "And so what I want to do is go to their—it's, you know, it's always hard to tell you what to refer to cause they show—their work shows up in multiple articles, multiple handbooks on wisdom.",
    "But the article, I think that many people regard as sort of the Seminole one is an article entitled, Wisdom as a meta-heuristic yielding—sorry.",
    "I'm getting the wrong quote here.",
    "Sorry, here I just want to get into it.",
    "Wisdom, a meta-heuristic pragmatic to orchestrate mind and virtue towards excellence.",
    "Okay.",
    "So sorry for that little delay.",
    "Wisdom, a metaheuristic, and then in brackets, pragmatic to orchestrate mind and virtue towards excellence.",
    "So notice right here, the title tells you that they've accepted, deeply accepted the point by—made by Schwartz and Sharpe that there's a deep connection, between wisdom and virtue.",
    "Orchestrating mind and virtue towards excellence.",
    "There's already the deep connection to positive psychology.",
    "But also notice something the invocation of the term metaheuristic and that notion of pragmatic tells us that relevance realization is playing a very significant role in this theory, at least I will argue that.",
    "Okay.",
    "get the notes!",
    "Defining The Notion Of Pragmatic So let's first of all, deal with this notion that they put in brackets of pragmatic (Fig. 6a) (writes Pragmatic), cause they're sort of picking up on a couple different related, but not identical meanings associated with that term.",
    "One is having to do with, I think like the pragmatic aspects of language pragmatics (writes Pragmatic).",
    "So there's syntax, semantics and pragmatics.",
    "And we talked about this when we talked about Grice and conversational implicature that you always are conveying much more than you're saying and how that depends on capacity for relevance realization and you can see—and so there's that sense of dealing with how much our communication and more broadly our cognition goes beyond what we can directly propositionally represent.",
    "Fig.",
    "6a",
    "That's definitely there.",
    "There's another meaning of pragmatics and that has to do with pragmatism (Fig.",
    "6b) (writes Pragmaticism), which I haven't talked about.",
    "I'm going to talk about it later when I talk briefly about James.",
    "And so the idea behind pragmatism is—sorry, like I said, there's so much there, but the idea about pragmatism, I would argue a way of understanding it, at least the way of understanding James.",
    "James was one of my heroes.",
    "James was both a great psychologist and a great philosopher and he was interested...",
    "so he's kind of a proto-cognitive scientist, but he isn't interested just in cognition.",
    "He's interested very much in, you know, what it is to live a good life.",
    "He starts some of the earliest work on, you know, the study of mystical experiences and religion, psychological investigation.",
    "So he's just a really pivotal figure for me.",
    "And for many people, Fig.",
    "6b",
    "But one way, I would argue, is what James was on about is that you should evaluate your knowledge claims, ultimately in terms of their efficaciousness, how much they can be viably used in your life in order to adapt you to the world.",
    "And so, one way of thinking about this is your propositional claims ultimately have to be grounded in your procedural abilities.",
    "James doesn't use this language, but I could find passages in James that clearly point to it, I would argue, that your, you know, your propositional knowing has to be grounded in your procedural abilities, which have to be grounded in your perspectival, which has to be grounded ultimately in your participatory.",
    "James was very interested in the phenomena of conversion when people go through these massive identity changes and how that changes the world that they can live in.",
    "Now, I think that's deeply right, but there are also some problems with pragmatism and I'll come back to this.",
    "So I'll just mention it now.",
    "I think there's a confusion or at least a potential confusion between truth and relevance.",
    "And that can be problematic.",
    "Now why does all of that matter?",
    "Well, because as I've just tried to show you, pragmatism tries to situate what James would call sort of your intellectual claims into this deeper lived, experienced, viable ability to fit your world, to develop your connectedness, to develop yourself.",
    "Both of those (indicates Pragmatics and Pragmatism), I think, can plausibly be brought back together in the notion [of] what we're talking about.",
    "And then, this just goes so well with the invocation of the term metaheuristic, a heuristic for managing your heuristics.",
    "We can draw this (indicates Pragmatic) together and this term together metaheuristic (Fig. 6c) (writes Metaheuristic beside Pragmatic), right.",
    "We can draw this all together with while having to do with realizing relevance (draws an arrow from Metaheuristic and Pragmatic and writes Realizing relevance below).",
    "This is invoked, not in terms of the theoretical account I've given, but the idea that zeroing in on relevant information is crucial to wisdom.",
    "This is invoked throughout the article by Baltes and Staudinger.",
    "Let's be clear.",
    "I don't think they are explicitly making a case the way I am.",
    "What I'm saying is they're invoking ideas and making use of them that ultimately deeply presuppose (indicates Realizing relevance) the ability for relevance realization.",
    "Fig.",
    "6c Berlin Wisdom Paradigm Now, they have an account of the five criteria you need in order to be wise.",
    "And the point about this is to try and specify what these metaheuristics are that bring about an excellence in our life, an excellent orchestration of mind and virtue together so that we become excellent human beings.",
    "Excellent persons.",
    "They try to specify this in terms of five criteria.",
    "The point of the criteria are these are the features that are needed to judge someone wise.",
    "And also these are features that could be empirically investigated.",
    "Okay.",
    "So what are these criteria?",
    "So rich factual knowledge about the fundamental pragmatics of life.",
    "So this is in some sense, like sophia.",
    "This person has a deep grasp of the facts, the principles of the fundamental pragmatics of life.",
    "They also need rich procedural knowledge about the fundamental pragmatics of life.",
    "And this goes back to the McKee and Barber point, right?",
    "That wisdom is not so much what you know, but how you know, it's very much about knowing how to put these principles into practice into process.",
    "Now they, of course, have now done propositional and procedural knowledge.",
    "I think they should have gone deeper.",
    "They obviously are going to need a participatory knowledge because they have to explain how we go through dramatic developmental change, because presumably qualitative change is what's needed for wisdom, hence the term excellence, right?",
    "And, of course, they are missing the perspectival knowing that connects the procedural knowing to specific contexts, situational awareness.",
    "Okay, so that I think is important.",
    "I think they're pointing towards this perspectival knowing and how it ultimately plugs into participatory when they invoke the next criteria, they call it lifespan contextualism.",
    "Lifespan contextualism.",
    "Like I say, this is a kind of perspectival knowing.",
    "This is the way in which you are, you know, you're taking the big picture, your ability to zoom out and then from that big picture, zoom in as needed, right?",
    "So it's this perspectival knowing, and that I think is very crucial.",
    "I think it has a lot to do with our capacities for self-regulation.",
    "We've talked about that.",
    "Now, the next one I want to state it",
    "and then I want to challenge it.",
    "This is—they call it relativism of values and priorities, right?",
    "I find that a hard criterion to be sort of tethered to.",
    "If they're using this term carefully, I don't think that many of the people that I would regard as quintessentially wise were moral relativists.",
    "I do not think Socrates or Plato were moral relativists.",
    "I think they're clearly the opposite.",
    "I think it's unlikely that the Buddha is a moral—was a moral relativist or Jesus of Nazareth was a moral relativist.",
    "I think we are falling prey to thinking that our liberal democratic values are constitutive of wisdom.",
    "I'm not arguing against these values.",
    "That is not what I am doing here.",
    "I'm arguing against tying the notion of wisdom to those values.",
    "I think what might be on offer here, what they're actually talking about is a capacity for tolerance.",
    "And perhaps the way we can understand that then is instead of a kind of relativism.",
    "We can understand it in terms that we can apply to Socrates of a fallibilism, which is a claim that you should never assert certainty.",
    "All right.",
    "We can easily attribute that to Socrates and it sings analogous—Jesus is a regular condemnation of self-righteousness, seems to be appropriate here.",
    "So.",
    "a kind of fallibilism.",
    "And then linked to something that you've heard me mentioned multiple times, humility, a recognition and appreciation of your status, your limits, et cetera.",
    "So if we bring in fallibilism and humility, rather than requiring wise people to demonstrate moral relativism, I think we can plausibly apply this criteria to many exemplars of wisdom from the past.",
    "The fifth one.",
    "And I think this is very crucial—is recognition and management of uncertainty.",
    "Recognition and management of uncertainty.",
    "So this is to say, we're in the finitary predicament.",
    "Most of the time, we can't do algorithmic processing.",
    "We cannot pursue certainty.",
    "We have to act as best we can within unavoidable context of uncertainty.",
    "So you can see why I see why I think this theory is sort of dripping in the machinery of relevance realization.",
    "I think the term metaheuristic is very good.",
    "I think a metaheuristic is something that coordinates between heuristics.",
    "It might be something like an optimization within a dynamical system, like, I've argued, you know, trade off between compression and particularization.",
    "Things like that.",
    "They, at times though, tend to talk about this metaheuristic as a form of expertise and I've already made the criticism.",
    "I think that's a mistake.",
    "I think that understanding wisdom as expertise is to mislead us.",
    "Again, it causes us to over-focus on the important procedural knowledge to the exclusion of the perspectival and the participatory.",
    "It also confuses, you know, the context sensitivity with, you know, being domain specific and we shouldn't do that.",
    "And I've made that argument.",
    "I'm not going to make it again.",
    "Instead, I want to point out that what they tend to be arguing for is a very comprehensive kind of cognitive flexibility and adaptability that your cognition is flexible enough that it can adapt itself to different situations in a very efficacious manner.",
    "What's important is that they started to generate a semi-empirical work.",
    "How do you do this?",
    "Well, you basically train independent judges to be able to evaluate these criteria and people's behavior, their spoken behavior.",
    "Things like that.",
    "And then what you do is you put people into various situations, often situations that might involve moral dilemmas or other more practical challenges.",
    "And you get those people to relate on how they would deal with those difficult little situations.",
    "You try to find some situations that we would prototypically do this for.",
    "We would say for somebody who handled themselves well in that situation, we would be quite happy with attributing wisdom to them.",
    "They'd say, \"yeah, somebody managed this situation really well, that would be good evidence for me for calling them wise.\"",
    "Now what you do is reverse engineer that take those situations that if solved successfully would generally lead to the attribution of wisdom, give them to a bunch of people, evaluate how in the answers that people are giving, not just sort of vaguely how well they answer it, but do they answer it in a way that exemplifies these five criteria?",
    "And then you can judge how well people are doing in solving these problems.",
    "And so what you got was some of the first attempts to start to empirically measure wisdom by putting people—I mean, see what they're doing.",
    "This is analogous on how we test intelligence and rationality.",
    "We give people a bunch of tests across situations and we try to see how they do and that's—and then we start to generate from that a measure of how wise they are.",
    "I think this is, as I said, this is just quintessentially important.",
    "So, what are the things I want to bring out?",
    "They talked about the cognitive styles that are important for being wise.",
    "And that is important, sort of a judicial style.",
    "Somebody who's good at making judgements.",
    "The reason why I'm not going into that detail is that relying on sort of notions from Sternberg and others about particular kinds of styles—I don't really have time to go into that in depth.",
    "So that would be a large chunk on itself.",
    "What it shows is how important the capacity for good judgment is for wisdom.",
    "And we sort of knew that, but, as I'm trying to argue, we're getting a sense of how, like, in terms of relevance realization, and the ability to zero in on relevant information.",
    "We're getting a sense of what that good judgment means.",
    "get the notes!",
    "The Solomon Effect Now, one of the things I want to draw from Baltes and Staudinger, one of the experimental results—because this points to more recent and important work by Igor Grossman—is they gave people this experimental task in which they have to try and solve these problems.",
    "And they put them into three conditions.",
    "In one condition, they could discuss the problem with a significant other before responding.",
    "In another condition, they could imagine a virtual or internal dialogue.",
    "Notice that.",
    "Imagine a virtual and internal dialogue.",
    "Remember the Stoics and internalizing Socrates, right?",
    "Third condition, they were just given more time to think about it.",
    "And what they found is that the first and second group clearly outperformed group three.",
    "You're wiser, if you talk to other people and that's sort of like, \"duh!\"",
    "Yeah,",
    "but if it's \"duh,\" why do we carry around this bullshit mythology of complete individualism?",
    "So that's one interesting finding.",
    "This goes back to the platonic dialogue that in discussion with others, we get to a level of wisdom that we cannot get to on our own.",
    "Now, what was interesting.",
    "That's in itself interesting.",
    "What was interesting is also there was no important difference between group one and group two.",
    "Talking to another person and imagining, simulating in your mind, talking to another person, which is—that was just as good.",
    "If you can internalize other people.",
    "They can give you the metacognitive ability to overcome your biases.",
    "Now, why this, I think is important, is I think this points to more recent work done by a colleague of mine, Igor Grossman, and I've mentioned his work already, the Solomon effect.",
    "Solomon, of course, the biblical figure of wisdom, which is if you have—what's going on with that talking with other people?",
    "Well, part of it, I think is the Solomon effect, right?",
    "If I describe a problem to you from the first person perspective, which I'm liable to do, especially in an individualistic culture like ours, right.",
    "I will tend to be very locked in because again, remember?",
    "Remember the whole thing about internalization when I'm in a perspective it's biasing me.",
    "And one of the things I can't see—my framing is often transparent to me.",
    "I can't see it.",
    "I'm seeing through it.",
    "And when I'm in the first person perspective, I'm sort of locked here in my perspective, because it is my problem.",
    "\"Ah!\"",
    "If you get people to redescribe the same problem from the third person perspective and notice the word I'm going to have, they often have an insight.",
    "They often notice something they hadn't noticed before.",
    "They pick up and make something salient or relevant that wasn't salient or relevant from within their first person perspective.",
    "So moving to—moving outside and looking back through somebody's eyes from a third person perspective on your cognition can enhance your capacity for these wisdom task.",
    "This is what I mean why Baltes and Staudinger, although they're not invoking it or theoretically discussing it, they are relying on perspectival knowing in their experimental work.",
    "So we're starting to make our way through these theories of wisdom.",
    "We've taken a look at Schwartz and Sharpe.",
    "We've seen how the connection between wisdom and virtue is being established; Baltes and Staudinger are picking up on that.",
    "And they're starting to get us into some of the fundamental machinery of what it is to be a wise person.",
    "I want to continue that next time and also, bring up some important criticisms of the work of Baltes and Staudinger.",
    "You've seen me already make one.",
    "I don't think that this ability (indicates Fig. 6c) should be understood as expertise.",
    "I'll make some other ones.",
    "And those criticisms will take us into the important seminal work on wisdom by Monika Ardelt.",
    "And then we'll also take a look at the work of Sternberg.",
    "And then I will return and propose, or at least explain to you an account, a proposal made by myself and Leo Ferraro in 2013, about how to try and draw this (indicates Fig. 6) together in terms of this machinery (indicates Realizing relevance) that I've been advocating.",
    "And then I want to subject that theory, my own theory to, I think, some pretty significant criticisms and then point—and I hope that will point us towards how we can then reintegrate the account of wisdom with the account of enlightenment.",
    "And ultimately we situate us back with awakening from the meaning crisis.",
    "Thank you very much for your time and attention.",
    "- END - Episode 43 Notes To keep this site running, we are an Amazon Associate where we earn from qualifying purchases Paul Baltes Paul B. Baltes was a German psychologist whose broad scientific agenda was devoted to establishing and promoting the life-span orientation of human development.",
    "Ursula Staudinger Ursula M. Staudinger is a German psychologist and researcher of aging.",
    "Article Mentioned: Wisdom: A Metaheuristic (Pragmatic) to Orchestrate Mind and Virtue Toward Excellence Barry Scwartz Barry Schwartz is an American psychologist.",
    "Schwartz is the Dorwin Cartwright Professor of Social Theory and Social Action at Swarthmore College and since 2016 has been visiting professor at the University of California, Berkeley.",
    "Kenneth Sharpe Article Mentioned: Practical Wisdom: Aristotle meets Positive Psychology Book Mentioned: Practical Wisdom: The Right Way To Do The Right Thing - Buy",
    "Here Christopher Peterson Christopher Peterson was the Arthur F. Thurnau professor of psychology and organizational studies at the University of Michigan in Ann Arbor, Michigan and the former chair of the clinical psychology area.",
    "Martin Seligman Martin Elias Peter Seligman (/ˈsɛlɪɡmən/; born August 12, 1942) is an American psychologist, educator, and author of self-help books.",
    "Phronesis Phronesis is an ancient Greek word for a type of wisdom or intelligence relevant to practical action, implying both good judgement and excellence of character and habits.",
    "Sophia Sophia is a central idea in Hellenistic philosophy and religion, Platonism, Gnosticism and Christian theology.",
    "Fallibilism Broadly speaking, fallibilism is the philosophical claim that no belief can have justification which guarantees the truth of the belief, or that no beliefs are certain.",
    "Igor Grossman Solomon Effect (Solomon Paradox) Grossmann and Kross have identified a phenomenon they called \"the Solomon's paradox\" - wiser reflections on other people's problems as compared to one's own Other helpful resources about this episode: Notes on Bevry Additional Notes on Bevry Ep.",
    "42 - Awakening from the Meaning Crisis - Intelligence, Rationality, and Wisdom Welcome back to Awakening from the Meaning Crisis.",
    "So last time, we were taking an in-depth look at the work of Stanovich and rationality, 'cause we are building towards an account of wisdom.",
    "Because that is deeply intertwined with the cultivation of enlightenment and, of course, with the cultivation of meaning.",
    "We noted that rationality is an existential issue.",
    "It's not just a matter of how we're processing information.",
    "It's something that is constitutive of our identity in important ways and our mode of being in the world.",
    "And we'll come back to that again.",
    "One of the core things we saw as we took a look at the rationality debate in which Stanovich's work is situated, is that debate showed us a couple of important things.",
    "It showed us if I can put this as a formula that rationality does not equal logicality and it does not equal intelligence (Fig. 1) (Writes Intelligence ≠ rationality ≠ logicality).",
    "That debate also showed us that we need multiple competencies when we're talking about rationality.",
    "We need an inferential competency, but we need an independent competency of construal.",
    "And then I proposed to you how we could understand what that competency is and what the normative theory is acting upon it.",
    "Namely insight, good problem formulation.",
    "Fig.",
    "1",
    "We then moved into what Stanovich saw as the missing piece.",
    "If intelligence doesn't give us rationality, what's the missing piece?",
    "There's two missing pieces.",
    "They overlap in some important ways.",
    "One is the notion he calls it mindware, what I've called psychotechnology (Fig. 2a) (writes Psychotechnology) and the other is a cognitive style (writes Cognitive below Psychotechnology).",
    "And the cognitive style that he talked about was active open-mindedness (writes AOM beside Cognitive), which he gets from Jonathan Baron.",
    "And this is the idea that what you should do is cultivate a sensitivity and an ongoing awareness of the presence and effect of cognitive biases in your cognitive behavior and your cognitive life, and to actively counteract them.",
    "Fig.",
    "2a I pointed out that, unlike Stanovich, who doesn't emphasize this as much, Jonathan Baron, who's the originator of this idea (AOM) as a constitutive of feature of rationality points out that you can't do that too much.",
    "'Cause if you try to override too many of your cognitive biases, you, of course, will be also overriding them in their functioning as heuristics, that help you avoid combinatorial explosion.",
    "So getting an optimal form of active open-mindedness rather than a maximum form of it is crucial to rationality.",
    "Defining Psychotechnology I want to just briefly stop here and be a little bit more precise about how I want to use this term (draws an arrow pointing to Psychotechnology).",
    "I've been using it throughout, and I basically defined it by example, and then through exemplification.",
    "But I want to be a little bit clear about it because it's going to be relevant as we go forward and talk about wisdom.",
    "So here's the definition I want to offer to try and clarify what I mean and how I'm using the term psychotechnology.",
    "As I said, I don't claim to be the originator of this idea, but I am claiming that this is the particular slant I'm taking on this idea of psychotechnology.",
    "The psychotechnology is a socially generated and standardized way of formatting, manipulating, and enhancing information processing that's readily internalizable into human cognition, and that can be applied in a domain general manner.",
    "That's crucial.",
    "It must extend and empower cognition in some reliable and extensive manner and be highly generalizable among people.",
    "Prototypical instances are literacy, numeracy, and graphing.",
    "So I want to just make it very clear.",
    "It's not just that anything we use mentally will count as a psychotechnology.",
    "So the cognitive style of active open-mindedness will probably make use of psychotechnologies in order to help track bias.",
    "But obviously, Stanovich means something much more comprehensive.",
    "He means a set of skills, psychotechnologies, sensibilities, and sensitivities that will help you in a domain general manner, note and actively respond to the presence of cognitive bias.",
    "We can then ask, what is it about people that if intelligence is insufficient for this (indicates Fig. 1), what is it about people that is predictive of them acquiring this (draws an arrow pointing to AOM).",
    "Now, this is learnable (indicates Psychotechnology), right?",
    "Need For Cognition And Problem Finding",
    "And we talked about the need for cognition as being an important predictor (Fig.",
    "2b) (writes Need for cognition at the end of the arrow below AOM).",
    "So this is the degree to which you are motivated to go out and look for problems.",
    "You're trying to find, formulate, and solve problems.",
    "And so in that sense, you are generating your own instances of learning and problem solving in a quite directed and comprehensive manner.",
    "I suggested to you that there's two ways in which we can think about this curiosity and wisdom (Text overlay appears saying \"John means \"Curiosity and Wonder\" instead of \"Curiosity and Wisdom\").",
    "Fig.",
    "2b I want to stop here and note something about this need for cognition (draws an arrow from Need for cognition) that I'm now going to be making use of in today's lecture.",
    "Which is the connection between the need for cognition and what Arlin calls (writes Arlin), problem finding (Fig. 2c) (writes Problem finding below Arlin).",
    "Because that's a very central feature, I would think.",
    "I would say an essential feature of what it is to have a high need for cognition.",
    "Arlin argues that problem finders are very good at finding problems.",
    "As the name indicates there, they are able to realize problems and connect things together in ways that other people have not previously done.",
    "Some people have argued that this is central to creativity.",
    "But important for our purposes is that Arlin—and this is kind of prescient of this whole argument.",
    "She made this argument in 1990 (writes 1990 above Arlin).",
    "She argues that this is central to wisdom (draws an arrow pointing from Problem finding and writes Wisdom).",
    "That one of the crucial features of being a wise person is the capacity to find problems that other people have not yet found.",
    "Fig.",
    "2c I want to take a moment here and offer a suggestion of how we can think about what makes somebody a good problem finder.",
    "I want to—this is not in Arlin, this is my attempt to extend, to develop Arlin and make it a little bit more concrete and practicable.",
    "So I want to propose to you this idea (erases the board).",
    "get the notes!",
    "Becoming A Good Problem Finder We don't find problems typically in a vacuum.",
    "We don't do anything in a vacuum.",
    "There's always, you know, already a background of existing issues we're dealing with, other people are dealing with in our culture.",
    "I would suggest to you that a good problem finder can do this.",
    "Here's some existing problems in the space in which human beings are operating (Fig. 3a) (draws 4 circles).",
    "And what a good problem finder does is, I think, not just simply add to that (Fig.",
    "3b) (draws a 5th circle).",
    "I mean, that would be kind of a basic skill in problem finding (erases the 5th arrow).",
    "But I think people that we regard as being exemplary in this and doing it very well and therefore demonstrating an aspect of creativity, it helps to explain why problem finding sort of overlaps between wisdom and creativity.",
    "They can do the following thing.",
    "They can find a problem (Fig. 3c) (draws a 5th circle) that if solved would make a significant impact on these existing problems (draws arrows from the 5th circle).",
    "So what I'm suggesting to you is that good problem finding is the ability to generate a problem nexus (writes Problem nexus).",
    "So then if you go in and say, \"Here's some problems in various domains (indicates the circles) and then they are all centered on this core problem (indicates the circle in the middle).",
    "And if we can address that core problem, then we can go back and make a significant impact (indicates the arrows) on this.",
    "I think many of the people—I don't take credit for this.",
    "I think the person who should be given credit for this, and I'll talk about that later today is Dreyfus.",
    "The idea that many of the central problems of cognition are centered on this ability to realize relevance.",
    "I think that's a very powerful kind of problem finding it's the generation of a real problem nexus.",
    "And I've tried to show you how it can be very generative of theoretical and empirical research.",
    "So I think that's part of what it is to be a good problem finder.",
    "It is to generate the problem nexus.",
    "I also want to point out something that I'm going to come back to, which is this is going to overlap with an important aspect in some current theories of the nature of understanding that have to do with the effectiveness of how we are relating to knowledge.",
    "Now, that sounds very vague and I will come back to that more carefully, but I need you to understand right now is that this problem finding, the ability to generate a problem nexus will also make a significant impact and interact with some of the best accounts, I think, that are emerging about the nature of understanding.",
    "And that's going to be important because we want understanding to be part of our theory, our account of wisdom (erases the board).",
    "I want to come back to the affective side of this.",
    "So I've suggested to you.",
    "That one part of good problem finding—sorry, one part of need for cognition (Fig. 4) (writes Need for cognition) is good problem finding (writes Good problem finding below Need for cognition).",
    "And then good problem finding is the ability to generate a problem nexus (draws a downward arrow below Good problem finding and writes Problem nexus).",
    "The need for cognition.",
    "Look at this word here (underlines Need) also points to obviously an affective, a motivational component.",
    "And this takes us into the few things I talked about before.",
    "Wonder (writes Wonder below Need) and curiosity (writes Curiosity beside Wonder).",
    "And I propose to you that, although these terms (indicates Wonder and Curiosity) are slippery, one way in which we can pick up sort of polar opposites is that curiosity is much more in the having mode.",
    "It's much more about manipulating and controlling things and wonder is much more in the being mode.",
    "It's much more about encountering mystery and calling into question one's worldview, one's identity, etc.",
    "So that's why wonder can shade into awe or potentially even into horror.",
    "Fig.",
    "4 Two Different Interpretations",
    "To Wonder Now I want to pick up on something here, because this again has some very deep connections to wisdom.",
    "So I actually have this on a, like a plaque in my apartment.",
    "It's a famous quote from Socrates, which is, \"Wisdom begins in wonder.\"",
    "Wisdom begins in wonder.",
    "And like everything about Socrates, it is simultaneously provocative and enigmatic as to what did Socrates actually mean by that?",
    "And there's two different interpretations.",
    "And you can see this in the different ways in which wonder (writes Wonder) is treated by Plato (writes Plato below Wonder) and, of course, by Aristotle (writes Aristotle below Wonder).",
    "You can see this sort of distinction to some of the current work on wonder.",
    "But for Plato, the point of philosophy is to develop and extend that sense of wonder so that what you're actually trying to do, for Plato, is you're trying to deepen wonder into awe (Fig. 5a) (writes Wonder and Awe below Plato).",
    "Because he feels that this awe will have the greatest capacity for transforming us for getting us deeply involved in the anagogic ascent.",
    "That makes sense.",
    "Fig.",
    "5a Aristotle also thinks that philosophy begins in wonder, but, I think you could make a good case that—many people have—that Aristotle sees this more in line with curiosity (writes curiosity below Aristotle), trying to figure things out.",
    "And then what you're ultimately doing (erases Curiosity), I would say for Aristotle, is this.",
    "You're trying to basically shape wonder into curiosity (writes Wonder and an arrow and writes Curiosity) in philosophy and then resolve the curiosity (draws an arrow from Curiosity and writes Resolve) in some answer to some question.",
    "So for Plato wonder sets you on a quest of anagoge, but for Aristotle wonder gets you to formulate questions that you then answer.",
    "And that's a fundamental difference between them.",
    "And it's interesting because you see Plato is here sort of pushing for meta-accommodation (Fig. 5b)",
    "(Writes Meta-accommodation beside Awe) as we've seen before.",
    "Well, we talked about this, when we talked about the numinous.",
    "And Aristotle is, of course, putting for Meta-assimilation (writes Meta-assimilation beside Resolve).",
    "Of course, when I answer questions that may force me to do conceptual accommodation, but overall, this is trying to stabilize and assimilate and sort of home things for you.",
    "Now the kind of stuff we saw in Geertz.",
    "Fig.",
    "5b So philosophy is working within that whole structure.",
    "And why am I pointing this out?",
    "Because of course, again, we're invoking this higher order relevance realization that's at work within this need for cognition within wonder and curiosity (indicates Wonder and Curiosity in Fig. 4).",
    "Alright.",
    "So we saw that Stanovich was able to respond to many of the defenders of human rationality in the rationality debate (erases the board).",
    "He was able to respond to Cohen by crucially noting that we have to challenge Cohen's assumptions.",
    "We do not have a single competence.",
    "Well, I also added in that we shouldn't think of it as static or completely individual.",
    "He was able to respond to Cherniak by pointing out that Cherniak was quite right about the centrality of dealing with computational limitations, but that what Cherniak is really giving is not a theory—because Cherniak's theory is a theory of relevance realization—it's not a theory of rationality, but a theory of intelligence.",
    "Something that's Stanovich also agrees with, and then to Smedslund, Stanovich acknowledges that we need an independent normativity on construal, and we've already seen that we can answer that.",
    "Well, at least I'm proposing that we can answer that by a different area in psychology, which is the work on good problem formulation and insight of problem formulation that avoids the combinatorial explosion ill-definedness and the way in which we can be misdirected by salience to misjudge what is relevant.",
    "get the notes!",
    "Dual Processing Theory I now want to to return to Stanovich's theory properly.",
    "What's his positive account of what rationality is?",
    "So the way Stanovich does this —and I don't want, sorry—the way he's doing this overlaps with a lot of other work, and this is a point that he himself made.",
    "There's a lot of convergence in psychology on the idea of a dual processing theory that we have multiple competencies.",
    "Now, what this dual processing is itself very controversial.",
    "Initially, people talked about two systems and then they talked about two styles.",
    "And because of critiques, it was hard to maintain those terms.",
    "And I'm not even convinced that they are, you know, that there are distinct things.",
    "They might lie on a continuum, but the basic idea—so to avoid all that controversy, they're simply called S1 and S2 (Fig. 6a) (writes S2 and S1).",
    "And like I say, I'm not claiming that they're discrete systems or even discrete styles, it's quite possible that they are polar positions on a continuum of processing.",
    "I'm going to put all of those aside because it's not relevant for what we're talking about here.",
    "Right?",
    "Fig.",
    "6a",
    "So the main idea here is that these are different ways in which you process information.",
    "And this process (S1) works largely intuitively (writes intuitively below S1).",
    "It works very much in a highly associational fashion (writes association below S1).",
    "It makes use of a lot of implicit processing (writes implicit processing below S1) and it's very fast, right (writes fast below S1)?",
    "It's very fast.",
    "So this is the kind of processing that you're using all the time in what we've, you know, when we talked about this, when Varella talked about your ability to cope (writes Coping beside S1).",
    "This is your coping, right?",
    "So when I'm moving around the environment, I'm relying a lot on my intuitive knowledge, my capacity for implicit learning, the way I can quickly associate things together, right?",
    "And so I would add to this, of course, as I argued before, that this is also (writes Caring below Coping) sort of how we're primarily caring for things, being involved with them, finding them salient, et cetera.",
    "But nevertheless, this (indicates S1) is the part of your cognition that is operating a lot of the time in the background.",
    "In fact, I want to, I'm going to step aside from Stanovich for a moment and propose to you that instead of thinking that these are discrete systems, we can think of different states where you're in, where one style or other is more foregrounded, and the other is more backgrounded.",
    "So I'll come back to that.",
    "Let's do the—what's S2?, Well, S2 operates more deliberately (Fig. 6b) (writes Deliberately below S2) in both senses of the word, like deliberation, where I'm engaging in reflection and deliberate in the sense that I am aware of it and intentionally directed in it.",
    "So it's deliberately, right?",
    "So it tends to not work associationally, it tends to work inferentially, argumentatively (writes Inferentially / Argumentation below S2).",
    "Right?",
    "Argumentation in the philosophical sense, not in the sense of having an emotional conflict with somebody.",
    "The processing, of course, all processing has—and this is an important point—some aspects of it that are implicit.",
    "So this is more of a contrast of emphasis, but this processing is much more explicit (writes Explicit below S2) and it tends to be very slow (writes Slow below S2).",
    "Fig.",
    "6b",
    "So Kahneman has a book out right now, Thinking fast and thinking slow that is a very good sort of discussion of this dual processing model.",
    "Because, as I said, there's a lot of theoretical argumentation and evidence that is converging on this.",
    "So this is a very highly plausible thing.",
    "And you see it showing up in many, many different domains within psychology.",
    "Alright, so one way of thinking about this, and this is a way in which Stanovich and Evans have tried to get a clearer, more precise way of distinguishing the two is the degree to which they're tapping, making use of working memory.",
    "So the idea is S2 really relies on working memory (Fig. 6c) (writes WM and draws an arrow pointing to S2), whereas S1 (draws an arrow from WM to S1) relies much less on it (writes + beside the arrow from WM to S2 and writes - beside the arrow from WM to S1).",
    "And so it's much more automatic (writes automatic beside the arrow from WM to S1) in that sense and its operation.",
    "Fig.",
    "6c So let's, let's take an example where you use the two systems, right?",
    "So you're grocery shopping and you come up to the cashier and they're ringing you in.",
    "And you've got a normal sort of basket full of normal groceries.",
    "And the cashier says to you, well, \"It'll be $1,000 please.\"",
    "And you go, \"What?\"",
    "Now, where did that \"what\" come from?",
    "Where did that \"what\" come from?",
    "Well, you have associations between sort of these objects, their prices, sort of the amount you've picked up implicit patterns, right?",
    "Notice how intuitively associationally, implicit and quickly you do, \"What?",
    "That's wrong.",
    "It can't be a thousand dollars.",
    "That makes no sense.\"",
    "So you call the cashier in the question by using your S1 processing.",
    "Now what the cashier has to do.",
    "She can't just—she or he can't just respond this way (indicates S1) to you.",
    "The cashier can't go, \"Nah, it's a thousand.",
    "I can sense it.\"",
    "What do they have to do?",
    "They have to deliberately—no, they have to take out each thing.",
    "They have to get out the bill, they have to deliberately, right—Notice that—to concentrate, they have to pay attention step-by-step, make the argument to you explicitly and slowly (indicates Explicit).",
    "\"No, no look, look, look.",
    "Look at—this matches this...\"",
    "They're using S2 processing.",
    "Now, these, of course, are in a tradeoff relationship because the problem with—and this, of course, is, and Stanovich acknowledges this but part of the problem with this (indicates S2) is how much demand it puts on your working memory (indicates WM), how slow it is.",
    "So you cannot rely on it.",
    "Yet another argument why you can't Descartes your way through your whole of your existence, right?",
    "You can't rely on it (indicates S2) for most of your behavior because you will just head into the ocean of combinatorial explosion.",
    "You will get so slowed down and so overwhelmed that you won't be able to live your life.",
    "You'll commit cognitive suicide.",
    "But, of course, we have this (indicates S2) for a reason.",
    "We have this because it is supposed to override to a degree this (indicates S1).",
    "So notice that these two systems are in a, basically, an opponent relationship.",
    "They are both working towards the same goal of making you adaptive but they tend to work in opposite fashions and Stanovich sees S2 as largely having, and there's deep truth to this, having a corrective function for S1 (Fig. 6d)",
    "(draws an arrow from S2 to S1).",
    "Fig.",
    "6d Active Open-Mindedness Alright, so now I can first give you his theory of foolishness, which he understands as dysrationalia (writes Dysrationalia), like dyslexia.",
    "And then, by implication, his theory of rationality, which, because, it's a comprehensive kind of rationality it deals with a comprehensive kind of foolishness, it's now bordering on an account of what wisdom is.",
    "So here's the idea: what is active open-mindedness doing?",
    "Well, what's happening is this (indicates S1) is the place where, you know, all the heuristics and biases are.",
    "This is where they're operating.",
    "And what happens here is they make you leap to conclusions (Fig. 6e) (writes Leap to conclusions).",
    "Fig.",
    "6e Remember when I did the problem with you, you know where you've got the pond and on day one, there's so many lily pads and it doubles every day.",
    "And on day 20, it's done.",
    "On what day was the pond half-covered?",
    "And your S1 shouts at you, \"10 days.",
    "10 days in 'cause it's half and half.",
    "And that's how it works.\"",
    "And that's wrong because on day 19 half, the pond was covered, right?",
    "And what you have to do is S2 has to basically override (draws an arrow from S2 to Leap to conclusions) how you're leaping to conclusions.",
    "How you leap to the conclusion that the people at the airport are in danger because of the representative heuristic or the availability heuristic.",
    "So S1 is constantly giving, but I need this—that's what makes me fast.",
    "If I'm not leaping, I'm not fast.",
    "I'm not coping.",
    "Leaping and coping are deeply interdependent.",
    "Okay?",
    "But the thing is sometimes, and again, it's unclear what the degree is, that's one of the ways in which I think Baron is a little bit more clear than Stanovich, but sometimes we need to override this leaping to conclusion.",
    "So he sees that what active open-mindedness (Fig. 6f) (writes AOM) is basically doing is teaching you to protect this (indicates S2) processing from being overwritten by the way S1 makes you leap to conclusions.",
    "You are foolish.",
    "You have dysrationalia if you are highly intelligent and yet you do not—you have not trained S2 (indicates S2 and AOM) to be properly protected from the interference (indicates Leap to conclusions) from S1.",
    "Do you see what's going on here?",
    "This is how he's ultimately responding to Cohen.",
    "You have these two competencies (indicates S1 and S2) you need both of them.",
    "They are constitutive of your cognitive agency.",
    "So they're both (indicates S1 and S2) constitutive competences, but this one (indicates S1) can interfere with this one (indicates S2), and that can cause you to behave irrationally.",
    "What active open-mindedness does is to protect this kind of processing from that interference.",
    "Fig.",
    "6f",
    "So that's Stanovich's central model.",
    "Now, I think that's definitely a good account of active open-mindedness (encircles AOM).",
    "My central criticism of this is I think it's an insufficient account of rationality (erases the arrow between S2 and Leap to conclusions).",
    "It's insufficient.",
    "I grant a lot of work has been done here.",
    "Rationality is not being equated to intelligence is not being equated to logicality.",
    "It's centered on overcoming self-deception.",
    "There's an account of self-deception.",
    "It's so platonic, eh?",
    "It's so platonic.",
    "Here's the monster (indicates S1) and it's interfering with the man, right?",
    "So platonic.",
    "And so we get that interference effect.",
    "That's, what's causing us to be foolish.",
    "That makes us self-deceptive.",
    "If we cultivate active open-mindedness, then we can reduce the interference.",
    "And that makes us more comprehensively rational.",
    "I think that's all, well, it's all elegant.",
    "It's beautiful.",
    "And the fact that this kind of work keeps getting replicated and massively convergent it's so highly plausible and profound.",
    "That's all, I think, that's all worthy of being noted.",
    "get the notes!",
    "Cognitive Leaping However, let's look to this (underlines Leap to conclusions).",
    "Because is leaping always a bad thing?",
    "Well, I already implied you need to leap to cope, but let's take a look at the work of Baker-Sennett and Ceci from 1996.",
    "Okay.",
    "So they investigated a thing they called inductive leaping (Fig. 7a) (writes Inductive leaping).",
    "I think calling it inductive leaping is a mistake because I understand induction (encircles inductive) as an inferential procedure and what they're explicitly doing is not inferential in nature.",
    "So I'm going to suggest that we don't use that term (crosses out inductive).",
    "I'm instead going to use the more neutral term of cognitive leaping (writes Cognitive leaping below Inductive leaping).",
    "Fig.",
    "7a What's cognitive leaping?",
    "Well, cognitive leaping—how did they test it?",
    "They tested it in the following ways.",
    "I give you various patterns that are unfolding across time.",
    "And that at various times, I stop and ask you, you can tell me what it's going to be.",
    "One version of it is there's, like—we talked about this before—a bunch of dots and what's it going to be?",
    "And more dots get filled in.",
    "Eventually you're able to leap and say, \"Oh, it's going to turn out to be a sofa.",
    "I notice how you're doing —you're going from features to Gestalt.",
    "You're doing that leap.",
    "And you're going from looking at the dots to looking through them.",
    "You're doing an opacity transparency shift, all that stuff we talked about, right?",
    "Mindfulness is involved.",
    "Now, why does that matter?",
    "Well, what they found was something very important.",
    "This (indicates Cognitive leaping) allowed them to operationalize kind of the, at least an aspect of the inevitability of insight, because often you don't know what's going on in an insight there's this leap.",
    "And what they—how they operationalized it was this.",
    "You're a good leaper.",
    "You're a good leaper if you can use fewer cues (Fig.",
    "8) (draws three lines) and accurately say (draws an arc that intersects with a circle) what the final pattern is going to be.",
    "So if you use fewer cues and you're making lots of mistakes, you're not very good; if you're largely accurate, but you have to get a whole bunch of cues that you're not a very good leaper, but if you use few cues and get to the full Gestalt reliably, then you're a very good cognitive leaper.",
    "Okay, you're doing this.",
    "You have this skill, this facility with pattern detection, pattern completion.",
    "Fig. 8",
    "Why is that so important?",
    "Well, because that (draws an arrow from Cognitive leaping), and that's what the experiments show.",
    "This is directly predictive of insight (Fig. 7b) (writes Predictive of insight below Cognitive leaping).",
    "The better you are at leaping, the better you are at insight.",
    "Fig.",
    "7b So do you see the tension here?",
    "If I try to shut off too much leaping to conclusions (indicates Leap to conclusions), I'm also shutting off the machinery (indicates Cognitive leaping) that makes me more insightful.",
    "It's—we have, look, we have to give up simplistic notions, naive, simplistic notions of rationality.",
    "Yeah, I'm not accusing Stanovich of this, not at all.",
    "But what I'm saying is we have to—being rational is a very complex process in which there are trade off relationships and a very complicated kind of optimization needs to be trained.",
    "So we want active open-mindedness to pro— (erases the arrow between S2 and S1) Let's make this the error of interference.",
    "So sometimes I leap to conclusions (Fig. 9a)",
    "(draws an arrow from S1 to S2) and that causes a lot of mistake in inference (indicates S2), the kind of stuff Stanovich— And what I need is active open-mindedness to moderate that (draws an arrow from AOM to the arrow between S1 and S2) and ameliorate it in a significant degree, right?",
    "But I want to leap to insight (writes Leap to insight under S1).",
    "And I need that for good construal and good construal is central to being a problem solver.",
    "Central to being rational.",
    "Fig.",
    "9a Two Different Contexts: Theorizing And Therapy",
    "So what do I need?",
    "Well, it's interesting because if you turn to a domain outside of the academic domain and we have to, because rationality is existential, we have to pay attention to the context, even in which we're theorizing about rationality.",
    "And if we're in a largely academic context, we're going to think of rationality as primarily about theorizing.",
    "And so this (indicates S2 and S1) is a great danger to theorizing.",
    "And so I would argue, if the rationality, if the project you're engaging in rationality is the project of theorizing.",
    "And I mean, that broadly in the sense of generating scientific, you know, or historical theory, then active open-mindedness is crucial.",
    "But there are domains where it goes the other way there's domains in which what you do not—what you need is you need to be able (indicates Leap to insight) to come up with a transformative insight where you need a radical reconstrual of the problem or the issue where that's crucial because you're somehow locked in.",
    "Where is the domain in where that's crucial?",
    "Well, we've talked about it.",
    "So this is good for theorizing (Fig. 9b)",
    "(adds Good for theorizing above the arrow between S1 and S2).",
    "But Jacob's in his book, and you can see some related work by Teasdale, points out there's an opposite context.",
    "There's the context, not a theory, but there's the context of therapy (writes Therapy).",
    "I think it's broader than this (indicates Therapy), but I'm using this because it's a good contrast, there's an alliterative relation (indicates Theorizing and Therapy) to help for mnemonic purposes.",
    "Fig.",
    "9b",
    "So in therapy very often what's needed is and we've talked about this, remember?",
    "You're existentially trapped and you need this fundamental kind of transformative insight and you cannot infer your way through it.",
    "We've already tackled that argument in detail.",
    "You cannot infer your way through it.",
    "And the problem in therapy is people try to think their way through it.",
    "That's Jacob's main point.",
    "And I say it related in a book called The Ancestral Mind.",
    "And the same point is being made I think by Teasdale, when he talks about metacognitive insight being central to therapy.",
    "Often, what you need is to try and shut this down (indicates S2), try to trigger this.",
    "(indicates S1).",
    "I mean, think of Freud and free association.",
    "You have to try and shut this down (indicates S2), prevent it from interfering (Fig. 9c)",
    "(draws an arrow from S2 to S1 and crosses it out and erases the cross).",
    "Bring this (indicates S1) into the foreground.",
    "Keep this (indicates S2) in the background.",
    "Fig.",
    "9c",
    "So when we're theorizing, we need this (indicates S2) in the foreground and we need it protected.",
    "We need it to seriously background and constrain this (indicates S1).",
    "Active open-mindedness in doing this.",
    "But in the therapeutic situation, this (indicates S1) needs to be much more foregrounded this (indicates S2) needs to be backgrounded and it (indicates S2) needs where it needs to be constrained so it doesn't unduly interfere with this.",
    "get the notes!",
    "Mindfulness Now, what we can ask ourselves is, well, what's a cognitive style that makes this (indicates S1) much more focal, tries to constrain this (indicates S2) and really improves insight (underlines Leap to insight)?",
    "Well, we know what that is.",
    "And Teasdale in fact, argues for it explicitly as to what triggers meta-cognitive insight.",
    "That's mindfulness (Fig. 9d)",
    "(writes Mindfulness below S1).",
    "So you think about how much in a mindfulness practice, I've tried to argue that there we need an ecology of psychotechnologies to cultivate the cognitive style of mindfulness.",
    "Think of one, think of meditation.",
    "Think of how much in meditation you are trying to really constrain this (indicates S2), shut this down, reduce all that inner speech, all that inferential processing that deliberate direction.",
    "And you're trying to open this (indicates S1) up in a very powerful way.",
    "So notice that I now have (encircles Mindfulness) a cognitive style that is (indicates AOM and Mindfulness), in a very important sense, opponent, not adversarial, but opponent to active open-mindedness.",
    "Fig.",
    "9d Notice that they are both sharing the training of attention and what you're paying attention to and how you're paying attention.",
    "So, this is great for planning (writes Planning below S2).",
    "And this is, I said, this (indicates S1) is great for coping.",
    "And especially when the planning is epistemic, when we're trying to theorize, when we're planning for truth.",
    "This (indicates S1) is very good for coping, especially when we're doing a kind of coping that's therapeutic in a broad sense in which we are needing to transform and undergo important qualitative development.",
    "So I think what's missing from Stanovich is a broader account of our competences and how they can be—how they are played off against each other, how there's a tradeoff relationship with them.",
    "And part of what, I would argue, and I'm going to come back to this more directly later, what goes into wisdom is a cultivation of both active open-mindedness for inference (indicates Good for theories) and mindfulness for insight (indicates S1), and then what we're going to need is—well, what coordinates them together (indicates S1 and S2)?",
    "How are they coordinated together?",
    "How do I optimize the opponent—not adversarial—how do I optimize the opponent processing between them?",
    "I want to come back to that and explore that in detail with you, because I can't push this any farther because as we push this farther, we're getting farther and farther away from Stanovich's theory and moving in towards a theory that I myself am going to propose to you.",
    "The work I've done with Leo Ferraro and then, sort of, critically reflect on that.",
    "So I need you to remember this for when we come back to more explicitly talk about a theory of wisdom and to be fair to Stanovich, he's ultimately not offering a theory of wisdom.",
    "And I think when he talks about rationality, He really means theoretical rationality, as opposed to what we might call practical or therapeutic rationality (indicates Therapy).",
    "Okay.",
    "I want to stop here though and note it and continue on with our investigation now of explicit theories of wisdom, but I want to sort of pull one thing out of our discussion from Stanovich before we leave Stanovich's good company (erases the board).",
    "I want to use somebody else's work to extend that a bit and to just add more teeth to this claim that rationality is ultimately an existential issue.",
    "Intelligence, Rationality, Wisdom A way of understanding what Stanovich says is the following (Fig. 10a) (writes Intelligence).",
    "Right?",
    "When I use intelligence to learn the psychotech and the cognitive syle, I can use intelligence to actually improve (draws a circular arrow from Intelligence to intelligence) how I'm using my intelligence.",
    "I can use my intelligence to improve how the competencies are optimizing and I can therefore overall enhance my capacity for relevance realization.",
    "We can think of that as rationality (writes Rationality beside the circular arrow).",
    "Fig.10a But something else is emerging right out of this, which, I'm only suggesting now, but we may perhaps be able to use our rationality to improve our rationality (Fig. 10b)",
    "(draws a circular arrow from Rationality to Rationality), to make it more optimal overall.",
    "And I would suggest to you, that's going to be crucial to wisdom (writes Wisdom beside the second circular arrow).",
    "That's going to be an essential feature, I would argue—not everybody agrees with me on this I'm trying to be really clear about this—but I would argue that that (indicates Fig. 10b) is a place in which we can find the locus for understanding the nature of wisdom, how it relates to rationality and how it relates to intelligence.",
    "What the nature of this (indicates the circular arrow pointing to Rationality) is we're going to have to come back to one more time (taps Rationality).",
    "Please remember when you see this word (Rationality) that I am not equating it to logicality or intelligence, I've given you long arguments, but I've tried to expand this notion, right?",
    "It's as much more about the reliable and systematic ability to overcome self-deception and to afford the enhancement of development and meaning in life.",
    "That's what I mean by that.",
    "Okay.",
    "So (taps Wisdom).",
    "Let's keep this in mind (indicates Fig. 10b).",
    "Fig.",
    "10b",
    "Let's take some time now (erases the board) to take a look at some more explicit theories of wisdom.",
    "I can't look at them all.",
    "This is a growth industry.",
    "I'm going to a conference tomorrow that has a lot of the people that are working in this.",
    "And it's going to be a discussion to try and see if we can come to some consensus about this precisely because it is such an important topic.",
    "A lot of good work has been done on it, but again, because there is such a multitude of viewpoints, getting a clear consensus on this is going to be a theoretical challenge.",
    "The Existential Aspect Of Rationality Okay.",
    "I want to, as I said, do one more thing before I do that before I move to the explicit theories.",
    "And I want to show you, I want to show you a bit about this (Fig. 11) (writes Intelligence with a circular arrow pointing to itself).",
    "And the point about this is to bring out something that Stanovich is not addressing, which is the existential aspect of rationality, the degree to which we identify with our higher cognitive processes.",
    "Fig.",
    "11",
    "Okay (erases the board).",
    "So this is the work of Dweck (writes Dweck).",
    "Carol Dweck, and its work on mindset (writes Mindset beside Dweck).",
    "She has a book entitled that, and this is, of course, is again, an ongoing thing it's been taking up in a lot of different research.",
    "So I want to describe an experiment to you.",
    "And then I want to sort of challenge a little bit a potential ambiguity or confusion in Dweck that I think we can clarify by making use of this work from Stanovich.",
    "So Dweck did the following thing.",
    "She brought—she has a whole bunch of experiments, but let's talk about one.",
    "She brought in a bunch of school children and she randomly assigned them to three groups (Fig. 12a)",
    "(draws three squares vertically): group A, group B, and group C (writes A, B, or C inside each box respectively).",
    "Fig.",
    "12a get the notes!",
    "Mindset: Fixed And Malleable View Now, Dweck talks about two different ways in which you can set your mind towards your traits.",
    "And the trait, it's really crucial, huh, and, this is relevant, is intelligence.",
    "So she talks about two views you can have.",
    "Two ways in which you can set your mind (underlines Mindset).",
    "Now it's a mindset because it's not just a belief, it's the way in which you identify with.",
    "It's the way in which you feel you are embodying the traits we're talking about.",
    "So you can have a fixed view (Fig. 13a)",
    "(writes Fixed beside Mindset) of your intelligence or a malleable view (writes Malleable beside Mindset).",
    "Okay.",
    "And so in the fixed view, you think intelligence is, like fixed, basically at sort of birth or early on.",
    "And then once it's locked in, there's not much you can do with it.",
    "So, for example, my height is a fixed trait.",
    "There's not a lot I can do to modify it.",
    "It's a fixed trait, okay?",
    "My weight is a much more malleable trait.",
    "I can change.",
    "It can—it's quite variable.",
    "It can change quite a bit.",
    "So you may think that intelligence is more like my height.",
    "You're sort of born with it.",
    "You're fixed.",
    "You got this number assigned and that's it.",
    "Or intelligence is malleable.",
    "It can develop and change.",
    "Fig.",
    "13a Now notice your behavior is going to be different if you think intelligence is fixed.",
    "If you think intelligence is fixed (writes Fixed intelligence), your attitude towards error (Fig. 14a) (writes Error under Fixed intelligence) is that error will reveal that you have a defect in a non-changeable trait.",
    "It'll reveal—it'll permanently disclose that you are not smart.",
    "So fixed intelligence tends to turn error into permanent revelation (draws an arrow from Error and writes Permanent revelation).",
    "If I make mistakes, that will show that I have low intelligence.",
    "And once everybody knows that, including myself, there's nothing I can do about it.",
    "And then I'm doomed to being a stupid person.",
    "Fig.",
    "14a If you have the malleable view of intelligence (writes Malleable view beside Fixed intelligence, adds View to Fixed intelligence), error doesn't do that for you (Fig. 14b)",
    "(writes Error below Malleable view).",
    "Error doesn't see—error is now—oh wait, error points towards the skills I'm using.",
    "I need better skills or the effort I'm putting in (writes Skills and Effort below Error).",
    "I need to put in more effort because if it's malleable, I can do things to change it.",
    "So the error says the error is basically pointing, you need to make some changes.",
    "You need to cultivate more skills.",
    "You need to cultivate more effort.",
    "Now, notice something right away, please.",
    "Notice how the fixed view (draws an arrow from Fixed view to Error) focuses you on the product?",
    "It focuses you on—you just get fixated on the error.",
    "\"Oh no.\"",
    "This (indicates Error under Malleable view) focuses you—remember the key of rationality?",
    "It focuses you on the process (draws an arrow from Error to Skills effort).",
    "The process.",
    "Fig.",
    "14b",
    "Okay.",
    "So she has a lot of experiments showing that the fixed view and the malleable view have a huge impact on your behavior, but interesting thing, the way—how can you trigger people into this modal?",
    "If you were in sort of an authority position, like being a teacher at a school, the one way you can trigger the mode—and this is really a mode, right?",
    "The mode that people are in— is how you praise them, the kind of feedback you give them (Fig. 13b)",
    "(writes Praise above Dweck).",
    "If I praise you using trait language, like \"You're so smart, you're so bright.",
    "That's going to tend to trigger this orientation (indicates Fixed view).",
    "If I praise you for the process, \"Wow.",
    "You're really, you're using a really good skill.",
    "You're putting in a lot of effort for that.\"",
    "That's going to make the process salient to you.",
    "The more I make this (indicates Fixed view) salient, the more you're going to be in that mode.",
    "The more I make this (indicates Malleable view) salient, the more you're going to be in that mode.",
    "Okay.",
    "So I can praise the trait (writes Trait beside Praise), right.",
    "Or I can praise the process (writes Process beside Praise).",
    "I think about how important this is to parenting or schooling.",
    "Fig. 13b",
    "Okay.",
    "So let's go back to the experiment.",
    "We have these three groups (Fig. 12b), the C group is the control group.",
    "They're all given a set of problems that have been pre-tested for, I think they were grade 4.",
    "They could all solve these problems.",
    "They're challenging, but they're all solvable.",
    "They all solved them.",
    "So all groups solved them (checks each box).",
    "But group A is praised for its trait (writes Trait beside group A).",
    "Group B is praised for the process (writes Process beside group B).",
    "And then group C has given the neutral, just acknowledgement that, \"Oh, the praises we used succeeded so well in that problem.\"",
    "Okay?",
    "So neutral (writes Neutral beside group C).",
    "Fig.",
    "12b",
    "So the idea is this (indicates Trait) is going to trigger these people into the fixed view (indicates Fixed view), this (indicates Process) is going to trigger people into the malleable view (indicates Malleable view).",
    "So now what your do is, you give the kids a bunch of tests.",
    "You ask, which ones want to take on some more challenging problems.",
    "Notice what I'm doing here.",
    "I'm looking for need for cognition.",
    "Looking for need for cognition.",
    "What I find is this group (Group A).",
    "Yeah (checks Group A but erases it again).",
    "Sorry, that was completely wrong.",
    "This group (Group A) says \"No. I don't want to try harder problems (Fig. 12c) (draws a southeast pointing arrow and a line pointing northeast).\"",
    "\"Why?\"",
    "Because if they try harder problems, there's a very good chance that they will generate error and then error will generate the recognition that they're less intelligent.",
    "And then they're permanently stained with that, permanently marked.",
    "The process people say, \"Yeah, I'd like to try harder problems.\"",
    "(draws a northeast pointing arrow beside Group B).",
    "They have a need for cognition.",
    "C's to be triggered, of course, neutral (draws a horizontal arrow beside Group C).",
    "Right?",
    "Fig.",
    "12c",
    "Then you give them some harder problems and ask, do they enjoy them?",
    "And the, \"Oh, do not enjoy this.\"",
    "(Fig. 12d)",
    "(draws a southeast pointing arrow beside Group A)",
    "\"Yeah, I enjoy this.\"",
    "Neutral (draws a horizontal arrow beside Group C).",
    "Now, here's the crucial thing.",
    "Fig.",
    "12d",
    "Now you give them a set of problems that were equal in difficulty to the first set of problems.",
    "You give them a set of problems that were equal in difficulty to the first set of problem.",
    "This has all been massively pre-tested so it's safe.",
    "And what you find is this group does about the same (Fig. 12e) (draws a horizontal arrow beside Group C), this group does much worse, and this group does better than it did before.",
    "(Text overlay appears saying, \"John means \"Group A does much worse, Group B does much better!).",
    "Fig.",
    "12e",
    "And now I want to, I want to extend this and notice how this is starting to fold into a kind of self-deception.",
    "Right?",
    "You ask these kids to write to—I believe the experiment was done in America.",
    "You ask these people to write to a student in Germany that they will never meet by the way and report how they did on the experiment.",
    "So these two groups (Fig. 12f) (draws a + sign beside Group B and C) largely tell the truth.",
    "They just took, you know.",
    "40% of these people (writes 40% beside Group A) lie about their performance.",
    "Fig. 12f",
    "Okay.",
    "What am I trying to show you?",
    "I'm trying to show you the way you frame yourself, the way you identify with your processing has a huge impact on your problem-solving ability, your proclivity for self-deception and your need for cognition.",
    "Rationality is an existential thing.",
    "It is not just an informational processing thing.",
    "Now.",
    "One thing that comes out of this is the question.",
    "Yeah, but is intelligence fixed or malleable?",
    "And Dweck is not quite clear about that.",
    "The evidence is pretty clear that there's a few things you can do to modify your intelligence.",
    "There's some suggestion that long-term mindfulness practice, by enhancing attention and working memory, can improve your measures of general intelligence.",
    "But by and large, intelligence is fixed.",
    "It's not that malleable.",
    "And then you may say, \"Oh, then this whole thing is based on, you know, lying to the kids.\"",
    "Right?",
    "Basically getting them to relate to intelligence and something malleable.",
    "Not really, not really at all because what we're actually talking about here, and that's what I've been continually alluding to here is that something that is terrifically, there's a way in which intelligence is terrifically malleable.",
    "And this is exactly what mindsetting is (Fig. 10a) (writes Intelligence and draws a circular arrow pointing to Intelligence) the way in which intelligence recursively relates to itself is a way in which we can think about it being rational, but that might—sorry, a way in which we can think about it being malleable.",
    "And my word slip shows you what my thinking is, a better way of talking about this is that intelligence is fixed (writes Rationality beside the circular arrow of Intelligence) and this is what Stanovich argues, but rationality is highly malleable.",
    "And then here's Stanovich's main point about this.",
    "We care too much for intelligence and not enough for rationality.",
    "Because, yes, intelligence is highly predictive of all these things.",
    "And that's why measurements of g are such powerful predictors.",
    "But if I wanted to know something about you now, following Stanovich's argument, I want to know not how intelligent you are (indicates Intelligence), I want to know how rational you are (indicates Rationality) and that is highly malleable.",
    "Fig.",
    "10a",
    "Okay.",
    "So what I'm trying to show you is that rationality is an existential issue.",
    "It's about how you're identifying with your own cognitive processing and the way in which that identification process (indicates Intelligence) can impede how you're applying and using an intelligence, or it can enhance it.",
    "And then there's the possibility (indicates the circular arrow around Intelligence) of cultivating the right kind of recursion and identity, the right kinds of cognitive styles.",
    "So somehow we have to put processes of identification, processes of coordinating cognitive styles together, and we can get back a clear path for becoming much more rational.",
    "And as I'm suggesting to you therefore much more wise, because if we use rationality to better learn how to use rationality and identify with our rationality (Fig.10b) (draws a circular arrow around Rationality), then, of course, I'm suggesting to you that's wisdom (writes Wisdom).",
    "And as I promised, we should now next—we should now turn to some of the explicit, psychological theories of wisdom.",
    "Fig.",
    "10b I can't, as I've already mentioned, I can't do all of them.",
    "I'm going to zero in on four theories that I think are quite representative of central ideas in the psychology of wisdom.",
    "And then I will then propose, I'll propose my theory the work I've done with Leo Ferraro.",
    "And I'll put that into sort of dialogue with these existing theories, as well as critiquing the theory the work that I did with Leo, and then ultimately what we want to do is resituate that account of wisdom into its connection with the cultivation of meaning and the pursuit of enlightenment.",
    "We will take a look at that next time.",
    "Thank you very much for your time and attention.",
    "Episode 42 Notes Jonathan Baron Jonathan Miller Baron is a Professor Emeritus of Psychology at the University of Pennsylvania in the science of decision-making.",
    "Dual processing theory In psychology, a dual process theory provides an account of how thought can arise in two different ways, or as a result of two different processes.",
    "Daniel Kahneman Daniel Kahneman is an Israeli psychologist and economist notable for his work on the psychology of judgment and decision-making, as well as behavioral economics, for which he was awarded the 2002 Nobel Memorial Prize in Economic Sciences.",
    "Book Mentioned: Thinking Fast and Slow - Buy Here Jacquelyn Baker-Sennett Stephen J. Ceci Stephen J. Ceci is an American psychologist at Cornell University.",
    "He studies the accuracy of children's courtroom testimony (as it applies to allegations of physical abuse, sexual abuse, and neglect), and he is an expert in the development of intelligence and memory.",
    "Publication mentioned: Clue‐Efficiency and Insight: Unveiling the Mystery of Inductive Leaps Gregg D. Jacobs, Ph.D Book Mentioned: The Ancestral Mind: Reclaim the Power - Buy",
    "Here John D. Teasdale John D. Teasdale was a leading researcher at Oxford University, and then in the Cognition and Brain Sciences Unit in Cambridge.",
    "Carol Dweck Carol Susan Dweck is an American psychologist.",
    "She is the Lewis and Virginia Eaton Professor of Psychology at Stanford University.",
    "Dweck is known for her work on mindset.",
    "Book Mentioned: Mindset - Buy",
    "Here Robert J Sternberg Robert J. Sternberg (born December 8, 1949) is an American psychologist and psychometrician.",
    "He is Professor of Human Development at Cornell University.",
    "Book Mentioned: Why Smart People Can Be So Stupid - Buy",
    "Here Keith E. Stanovich Keith E. Stanovich is Emeritus Professor of Applied Psychology and Human Development, University of Toronto and former Canada Research Chair of Applied Cognitive Science.",
    "Book Mentioned: What Intelligence Tests Miss - Buy Here Other helpful resources about this episode: Notes on Bevry Additional Notes on Bevry Ep.",
    "41 - Awakening from the Meaning Crisis - What is Rationality?",
    "Welcome back to Awakening from the Meaning Crisis.",
    "So we are pursuing the cognitive science of wisdom because wisdom has always been associated with meaning from the axial revolution onward, so that's a deep reason.",
    "Wisdom, of course, is also important for the cultivation of enlightenment, the response to the perennial problems.",
    "It's also playing a central role in being able to interpret our scientific worldview in a way that allows us to respond to the historical forces.",
    "And so wisdom is very important.",
    "We took a look at—we continued to look at McKee and Barber and we saw their convergence argument that at the core of wisdom is the systematic seeing through of illusion and into what's real.",
    "And this is very much like as the child is to the adult, the adult is to the sage.",
    "And then two other important aspects of it: that wisdom is much more with how you know than what you know, which means how you come to know it, and also how you interpret the knowledge.",
    "And that wisdom is, therefore, in a related fashion, deeply perspectival and participatory.",
    "And that's why wisdom can be associated with important forms of pragmatic self-contradiction.",
    "We then noted the connection with overcoming self-deception in a systematic fashion, and the emphasis on wisdom on the process rather than the products of knowing.",
    "And that both of those took us into the work of Stanovich.",
    "And because he famously argues that one of the hallmarks of rationality is valuing the process rather than—sorry, not rather than—valuing the process in addition to valuing the products of our cognition.",
    "And that took us also into the notion, the discussion of rationality and Stanovich is a good bridge because for him, the notions of rationality and ameliorating foolishness overlap very strongly.",
    "And we got into this notion of, at which I've been sort of comprehensively arguing throughout this course, that rationality has to do with the reliable and systematic overcoming of self-deception.",
    "And the potential affording of flourishing by some process of optimization of achieving our goals, with the caveat that as we try to optimize, we often change the goals that we are pursuing.",
    "One reason being that we come to more and more appreciate the value of the process as opposed to just the end result of the process.",
    "So in order to pursue that and to deepen our notion of rationality and thereby deepen our notion of wisdom—and, of course, wisdom has been associated with rationality from the beginning: Socrates and Plato and Aristotle, right?—we took a look at the rationality debate.",
    "We saw—I gave you three examples of many possible examples of experimental results that seem to show reliably this kind of thing.",
    "Very reliably, no replication crisis on this material.",
    "So, very reliably, two things: that people acquiesce, they acknowledge and accept the authority of certain standards, principles of how they should reason.",
    "And yet they reliably failed to meet those standards.",
    "And so one way to—one possible interpretation of that, not the only interpretation—one possible interpretation is that most people are irrational in nature.",
    "As I pointed out, because rationality is existential and not just, sort of, abstract, theoretical, concluding that people are irrational, has important implications for their moral status, their political status, their legal status, even their developmental status.",
    "Okay?",
    "So this is what I keep meaning when I'm saying rationality is deeply existential, it is not just theoretical.",
    "Okay.",
    "We took a look at one—the beginning of what's called the rationality debate and good science always has good debate in it.",
    "The rationality debate and the argument was made by Cohen, that human beings can't be comprehensively irrational because we have to ask this very pertinent question: where do the standards come from?",
    "And the argument is the standards have to come from us.",
    "And how do we come up with that normative theory?",
    "We come up with that normative theory that acknowledges, the way we come up with our normative theory is consonant with that we're the source of our standards.",
    "This is the—and this is the idea that at the level of my competence, my competence contains all the standards.",
    "And that what I do in order to get my normative theory is I idealize away all my performance errors.",
    "And this takes time.",
    "It takes a lot of reflection, until I get at the underlying competence.",
    "And then what I'm doing when I'm proposing a normative theory is I'm basically taking—I'm giving people this now explicated, excavated account of the competence that they possess and then demanding from them that they do their best to reduce the performance errors and meet that competence.",
    "So we—at the level of our competence, we are fundamentally rational and all of the mistakes that people are making in these experiments, according to Cohen are just performance errors.",
    "They're just performance errors.",
    "Okay.",
    "And just like you do not think I've lost English because of all of my performance errors, you basically dismiss them and read through them and attribute to me the underlying competence, Cohen is arguing, we should read through and dismiss these experimental results.",
    "And because the argument shows that people must have the underlying competence.",
    "get the notes!",
    "Competence Performance Distinctions",
    "Okay.",
    "So how does Stanovich reply to this?",
    "Well, in many places, but I think the best is Stanovich and West 2000 (writes Stanovich and West 2000) in behavioral and brain sciences.",
    "Because the argument—this is a really, for me, a gold standard of how you do really good cognitive science, the way they integrate a philosophical, psychological argumentation together, for example, is really, really impressive.",
    "So Stanovich and West say, well, if Cohen is right, then all of the errors that people are making are performance errors (Fig. 1a) (writes Performance errors).",
    "And Cohen is invoking the competence performance distinction, (writes Competence vs. Performance beside Performance Error) which, of course, goes back through, you know, to Chomsky and, ultimately to, Piaget.",
    "Fig.",
    "1a Now let's remember Piaget 'cause we've talked about this.",
    "Well, how do you distinguish the child's speech deficits from the drunkards, right?",
    "The drunkard is giving performance error, but we think the child, actually, her competence, for example, is not sufficiently developed.",
    "Why do we think that?",
    "Why do we think when the children are making all these conservation errors, right, that that reflects something about their competence at that point?",
    "Well that's because competence—errors that reflect a defect or a deficit in competence (draws an arrow pointing up to Competence) are systematic errors (writes Systematic below Competence).",
    "That's precisely how Piaget did his work.",
    "Right?",
    "That's precisely how you would determine that if I got brain damage and I'm leaving out, right, my sentences are broken, that it is not performance error, because my errors would be systematic across different contexts, different times of day different tasks.",
    "I'd be making these mistakes.",
    "Performance errors are not systematic, right?",
    "(writes Not systematic below Performance)",
    "So if my speech is broken because I'm rushing, right, it's not going to be broken when I'm not rushing.",
    "If my speech is broken, when I'm tired, it's not going to be broken when I'm not tired.",
    "So this is circumstantially driven.",
    "(writes Circumstantial below Not systematic)",
    "As the circumstances change, the patterns of error will change and go all over the place.",
    "So these errors are not systematic in nature.",
    "Okay, well, that means we have a reliable way of telling whether or not the errors that the people are making in the experiments are performance errors (encloses Performance error in a box) or not.",
    "How do you see if errors are systematic?",
    "Well, this is what you do.",
    "Errors are systematic (writes Error)",
    "if when I make this error (Fig. 2) (draws an arrow pointing to Error), it's highly predictive that I'll make this error (draws an arrow away from Error and writes Error).",
    "It's highly predictive that I make this error (draws another arrow pointing away from Error and writes Error at the end of the third arrow).",
    "It's highly predictive that I'll make this error (draws another arrow pointing away from Error and writes Error at the end of the fourth arrow).",
    "So if a child is showing conservation in this task, that's predictive that they'll fail to show conservation in this task, in this task and this task, right?",
    "So the degree to which—we've talked about this before—the degree to which there's a positive manifold.",
    "Remember when we talked about general intelligence, the degree to which your performance in one task is predictive of how you'll do on other tasks.",
    "If you have a manifold in your performance, if there's—if the errors are systematic across many different tasks that you're performing, then that's evidence that the errors are systematic (indicates Systematic) rather than not systematic (indicates Not systematic).",
    "Fig.",
    "2",
    "Okay, so that's easy enough.",
    "So then what we can do is we can—and we've got all this data—we can look at people doing these different experiments.",
    "And in many experiments they're doing multiple versions, the same participant is doing one task or [another].",
    "So what we can do is the following.",
    "We can see if—if I, for example, if I make a failure in critical detachment, does that mean that I'll also tend to show belief perseverance?",
    "Does that also mean that I'll attempt to leap to the wrong conclusion when I was doing the task on, you know, the lilypads covering the pond?",
    "And the answer—and this is what there is overwhelming evidence for, is yes.",
    "The errors you make are systematic.",
    "They're systematic.",
    "And see.",
    "So now Stanovich and West say, ah, Cohen's argument predicts that the errors are performance errors (indicates Performance error).",
    "That's what he claims.",
    "That means that the errors should be not systematic (Fig. 1b)",
    "(draws an arrow from Performance error to Not systematic).",
    "But what we find is overwhelming convincing evidence (encircles Systematic) that the errors that people are making in all these tasks are systematically related together.",
    "So these are errors at the level of competence.",
    "Fig.",
    "1b",
    "And now you go, \"What?\"",
    "And this is what I mean about good debate—makes something problematic, right?",
    "Because there's something right about Cohen's argument and Stanovich acknowledges it, right?",
    "There's something right in that we have to be the source, right, of our standards.",
    "And yet Cohen's conclusion that all the errors are performance errors is wrong.",
    "It's it's undeniably wrong.",
    "How do we put these together?",
    "Well, you put these together and you can see Stanovich and West doing this to varying degrees.",
    "You put this together by stepping back and looking at an assumption that's in Cohen's argument, it's an assumption about the competence.",
    "(writes Competence) Cohen is assuming that that competence for rationality (Fig. 3) (writes Single beside Competence) is a single competence.",
    "He's assuming that that competence is static, (writes Static beside Competence) right?",
    "He's also assuming that the competence is completely individualistic.",
    "(writes Individualistic below Static)",
    "Okay?",
    "I'll come back to this one (indicates Individualistic) much later, I'm going to address these two (draws arrows pointing to Single and Static) because Stanovich really doesn't talk about this.",
    "(indicates Individualistic)",
    "So remember that, but we'll come back to it later.",
    "(erases Individualistic) Fig.",
    "3 Think about the platonic idea.",
    "What if I have two competences (Fig. 4) (draws two squares) that both are working towards getting me to reliably achieve my goals, correct my own behavior.",
    "But these competencies could actually conflict with each other.",
    "(draws an arrow from the first square to the second square and another arrow from the second square to the first square)",
    "That would mean I would be the source of all the standards.",
    "Here are all the standards here (draws horizontal lines inside the first square).",
    "Here are all the standards here (draws vertical lines inside the second square).",
    "I am all—I'm the source for all the standards.",
    "But at the level of my competence, I can be generating error because these two competencies (indicates the two squares) can actually be in conflict with each other.",
    "Fig.",
    "4",
    "So one of the—the way you start to resolve this debate, and this has become a central idea in psychology and cognitive psychology and cognitive science is that we don't have a single competency.",
    "We have to have multiple competencies.",
    "And what that means is, and this is why, for example, assuming uncritically, uncritically assuming that you could reduce rationality and identify it with the single competence of syllogistic reasoning is just fundamentally wrong.",
    "It's not paying attention to the science.",
    "Here's another issue.",
    "Cohen is assuming that your competence is full blown.",
    "It's finished, it's static.",
    "It's done.",
    "But, and I sort of slipped this in, notice the examples I used of small children.",
    "Their competence, for example, in English or whatever language they're speaking, I just happen to be speaking English that's why I'm using it.",
    "The little girl —the two and a half year old, her competence is not fully developed.",
    "She will come to have the standards as that competency fully develops.",
    "But as that competency is immature, she can be a source of error from her competence.",
    "So, we have to give up the assumption that our competence again is static.",
    "Why would it be?",
    "And your cognition is inherently dynamic and developmental.",
    "Again, assuming, Oh, no, this is just what it is, right?",
    "And assuming that there is a single thing we're pointing to when we point to rationality is a mistake.",
    "See, this way, Stanovich and West can say—and it's brilliant, right?",
    "It's brilliant—they can say Cohen's argument is fundamentally right but his conclusion, that specific conclusion is wrong, because the conclusion that the errors are only performance errors is only a conclusion based on the hidden premise, right, that the competence is single and static.",
    "You have multiple competencies and they're in ongoing development.",
    "get the notes!",
    "Okay.",
    "So that's—we've learned something very interesting.",
    "(erases the board)",
    "So what's happening is, and Stanovich is an advocate of this, right?",
    "You have dual processing models.",
    "(Fig. 5) (writes Dual processing)",
    "This is the idea that we have different ways of processing information.",
    "Think of how platonic this is, how much Plato was going, I told you this a long time ago, right?",
    "We have different styles.",
    "Ways of processing information and that are good for different kinds of problem solving and those, and neither one of these competencies can be exclusive or sufficient for us, but they have ultimately a complimentary relationship, a relationship I would argue of opponent processing.",
    "Stanovich has a different view.",
    "We'll come back to that a little bit later.",
    "Okay.",
    "Fig.",
    "5 Finitary Predicament",
    "The second person in the debate, and you've heard me mention him and you've heard me talk about him (writes Cherniak) with serious respect.",
    "And he is given serious respect by Stanovich and West.",
    "Cherniak has a much different approach to the rationality debate.",
    "He agrees with Stanovich and West that the difficulty is not at the level of our performance, it's at the level of our competence, but he has a different move to make.",
    "He has a move that you see, you saw me invoke last time.",
    "This is the Ought implies can.",
    "So this is the question whether or not, right, whether or not we're applying the right normative theory to people in these experiments when we're judging them to be irrational.",
    "So Cherniak invokes something that you saw me invoke multiple times (draws an arrow pointing away from Cherniak) when we talked about relevance realization.",
    "We're in a finitary predicament.",
    "(Fig.",
    "6a)",
    "(writes Finitary predicament below Cherniak)",
    "This is actually his term.",
    "Alright.",
    "We cannot, because it is combinatorially explosive derive all the implications, we cannot consider all of our assumptions.",
    "All of this stuff that we've talked about before we can not go back and recreate all of the ways in which we've represented something.",
    "This is combinatorially explosive.",
    "What we do, right?",
    "Do we just arbitrarily choose whatever implication we want?",
    "No. Fig.",
    "6a",
    "So here's the point, right?",
    "We can't be algorithmic, right?",
    "(Fig.",
    "6b) (writes Algorithmic below Finitary predicament)",
    "We can't use standards that work in terms of certainty and completeness (writes Certain Completeness below Algorithmic) because, for example, if I tried to be comprehensively deductively logical, then I would engage—I would fall very rapidly into combinatorial explosion for any problem that I'm trying to solve.",
    "And I would then have committed cognitive suicide.",
    "It cannot be a standard, a normative standard of rationality if trying to follow it would commit me to cognitive suicide, which would undermine any attempt to satisfy any of my goals.",
    "Fig.",
    "6b You see what Cherniak is saying, right?",
    "You can't do this (underlines Algorithmic), but, of course, you don't just arbitrarily (writes Arbitrary below Algorithmic) choose whatever representation you want, choose whatever inference you want, choose to check whatever contradiction you want.",
    "So you can't check them all",
    "and you can't just arbitrarily—well, what's the answer?",
    "Well, you saw the answer before and it's one that I—you choose—it's relevance.",
    "You pick the relevant implications, you check the relevant contradictions, you check which aspects of your representation you consider relevant, et cetera.",
    "You do relevance realization.",
    "And Stanovich is like, yeah, that's right.",
    "Like, we're not going to argue with that.",
    "And that's part of what I've been arguing throughout.",
    "There's this consensus on how central this ability is, at least it's an emerging consensus.",
    "Herbert Simon from Newell and Simon, you know, talks about bounded rationality that we can't be purely computational, algorithmic, et cetera, for all of the reasons we've already explored.",
    "What Cherniak says is, but look what people are using in the experiments.",
    "They're using formal logic.",
    "(writes Logic)",
    "They're using formal probability theory (Fig. 6c).",
    "(writes Probability below Logic)",
    "They're using all these formal, purely algorithmic.",
    "(draws arrow from Logic Probability to Algorithmic) By the way, you say, Oh, but probability certainly doesn't work in certainty.",
    "Yes, it does.",
    "It gives you certainty about probabilities.",
    "That's what makes it a formal theory.",
    "That's why it has axioms and theorems, et cetera.",
    "Okay?",
    "Don't confuse properties of the theory with properties of what the theory is about.",
    "So what Cherniak says is the scientist in the experiment are using all these formal theories (encloses Logic Probability in a box and encircles Algorithmic - Certain Completeness) that can only be applied in very limited contexts.",
    "If I try to apply them comprehensively in my life—and that's where rationality matters, because rationality is an existential issue.—If I try to apply it (indicates Finitary predicament) comprehensively in my life from within the finitary predicament, I am doomed to fail.",
    "And then you're laying an Ought on me that I cannot possibly meet, which means you, scientists, have the wrong normative theory, because if you're laying a normative theory on me that I cannot possibly meet, that's evidence that it's the wrong normative theory.",
    "Fig.",
    "6c That's Cherniak's argument, it's a powerful argument.",
    "It's a good argument.",
    "It's an argument I take very seriously.",
    "So does Stanovich and West.",
    "They go, yes.",
    "All of this is right.",
    "However, Cherniak is talking about—he thinks he's talking about one thing and he's actually talking about another.",
    "And this is such a clever response, and it's going to tell us something really, really important.",
    "Right?",
    "Okay.",
    "First of all, this tells you that (indicates Logic Probability)",
    "you, again, here's another argument why you can't equate rationality with merely being logical, merely using probability theory, right?",
    "Does that mean I can be arbitrary (indicates Arbitrary) and ignore logic?",
    "No.",
    "It's the much more difficult—and notice how this starts to overlap with wisdom—it's a much more difficult issue of knowing when, where, and to what degree I should use logic and probability in a formal manner, et cetera.",
    "Computational Limitations",
    "Okay.",
    "How does Stanovich and West reply?",
    "Again, I think it's brilliant.",
    "They say all of this is right, but it's not about rationality.",
    "(writes Rationality below Stanovich & West)",
    "All of this.",
    "All of this stuff.",
    "They tend to talk about it in term, they use the phrase, computational limitations.",
    "(Fig.",
    "7a) (writes Computational limitations below Stanovich & West)",
    "They always describe it sort of negatively.",
    "Instead of positively, instead of relevance realization, although the two are interdefined, they always talk about it in terms of computational limitations.",
    "But they say all this stuff that Cherniak is talking about in terms of computational limitations is actually not about rationality.",
    "(draws an arrow from Computational limitations to Rationality)",
    "It's actually about intelligence.",
    "(draws an arrow from Computational limitation and writes Intelligence)",
    "This is going to be a brilliant move and what it's going to also show us is there is a—and I said this a while ago, there's a deep difference between being intelligent and being rational, in fact, Stanovich is going to argue that what makes you foolish is you're highly intelligent and highly irrational.",
    "And that is going to make sense of what we've already argued.",
    "That the very processes that make me adaptively intelligent are the very processes that also subject me to self-deception.",
    "Fig.",
    "7a How does he do this?",
    "Well, he basically argues.",
    "And you saw an analogous argument.",
    "How did they do this?",
    "And how does Stanovich do it elsewhere?",
    "They basically argue, ah, when we test for people's ability to zero in on relevant information, how can we test to see how well people deal with computational limitations?",
    "(encircles computational limitations)",
    "So what Stanovich basically argues is—and again, myself and Leo Ferraro argued this in a convergent fashion, other people, right?",
    "Basically what we're testing when we're testing people's intelligence is we're testing their capacity to deal with computational limitations.",
    "And this lines up again—also, you've heard me mention this, that measures of intelligence correlate with measures of working memory and blah, blah, blah.",
    "I've given you all that argument.",
    "Stanovich is giving you an additional argument here.",
    "He's saying, okay.",
    "So we understand intelligence as the capacity to deal with computational limitations (indicates Computational limitations)",
    "that's a negative way of putting it.",
    "I would say it's the capacity to do relevance realization, (indicates Relevance) but let's keep going.",
    "And then we have a way of, therefore, measuring people's capacity to deal with computational limitations.",
    "get the notes!",
    "So Cherniak is saying people fail in the experiment because of computational limitations, they're in the finitary predicament.",
    "And we have a way of measuring how well people can deal with computational limitations: that's intelligence.",
    "(encircles Intelligence)",
    "We have a way of measuring g (Fig. 7b) (writes (g) below Intelligence) —reliable, robust way of measuring g. Fig.",
    "7b So now, notice what we can do, again, so brilliant (indicates Stanovich & West).",
    "(writes (g))",
    "We have reliable ways of measuring g. Remember what Stanovich and West was showed with answers to Cohen that all of the reasoning tasks (draws a square with an x inside) also form upon a strong manifold.",
    "They don't label anything, but I'm going to call it r.",
    "There's right—sorry, gr.",
    "(Fig.",
    "8a)",
    "(writes gr under the square)",
    "There's this general factor of reasoning because the reasoning tasks (indicates gr) form a strong positive manifold.",
    "Okay.",
    "So what we can do is we can measure the GFR, right?",
    "(writes (gr) beside (g) and erases the square labeled with gr)",
    "And now we can do something very, very, very basic.",
    "If what's happening in the experiment is a measure of rationality and rationality is equivalent to dealing with combinatorial explosion, computational limitations, then these two (Fig. 8b) (writes = between (g) and (gr)) should approach parody.",
    "Intelligence and rationality should be identical, right?",
    "Fig.",
    "8a Fig.",
    "8b",
    "So notice what's going on here.",
    "If Cherniak is right then rationality and intelligence would be identical and there would be a strong relationship between how intelligent you are and how well you do on these experiments.",
    "And this is again where—good science: reliable, robust, well-replicated lots of covered decades.",
    "The relationship here (indicates Fig.",
    "8b) is at best, 0.3 (writes .3 below (g) = (gr)).",
    "So, you know, the correlation goes from zero to one (writes 0-1) where zero is no correlation, and one is very strong correlation.",
    "0.3 (encircles .3).",
    "Intelligence.",
    "What this clearly shows is that intelligence is necessary, but nowhere near sufficient for being rational.",
    "Okay.",
    "So here's two things that are insufficient for making you rational, just being very intelligent (indicates Intelligence) and just being able to use logic (indicates Logic Probability).",
    "The science is actually clear on this.",
    "So notice how a lot of the ways our culture has tried to understand rationality are now coming into question.",
    "We—oh, well, rationality is equivalent Fig.9) (writes Rationality =)—think of Descartes.",
    "Now you see why Descartes is wrong of logicality (writes Logicality).",
    "Well, that turns out to be false (draws a slash through =).",
    "Oh, well rationality is the same thing as being really smart, really intelligent (writes Intelligence = beside Rationality), right?",
    "Nope.",
    "That turns out to be false (draws a slash through =).",
    "Oh, what is that then?",
    "(writes a question mark below Rationality)",
    "What is that then?",
    "So now we're starting to do good science, right?",
    "We're starting to get away from common sense.",
    "We're starting to have some humility we're starting to—it's now a real problem.",
    "Fig.",
    "9",
    "Well, what is it?",
    "What is it?",
    "And it gives us a way of talking about what we've been talking about—the very processes that make you intelligent (indicates Finitary predicament) can actually cause you to be irrational.",
    "There is no contradiction in saying you're highly intelligent and highly irrational, not at all.",
    "So now we have to ask ourselves, well, what's the missing piece?",
    "So let's remember that.",
    "We've got an important question that we need to ask and answer.",
    "What's the missing piece for rationality?",
    "What's missing?",
    "Something else is going on and that missing piece is going to tell us quite a bit I think about the overlap between wisdom and rationality.",
    "Now there's a third argument (erases the board) and it's an important argument.",
    "'Cause it's also gonna connect to the issue of understanding, which is again, a crucial feature of wisdom.",
    "Now, Stanovich and West talk about this argument, but they don't cite an individual who actually came up with the argument explicitly.",
    "I think it's just because I don't think there's any fraud.",
    "I think they just didn't read it because the person was—the person I'm going to talk about is Smedslund.",
    "And the article is from the Scandinavian Journal of Psychology in 1970.",
    "(writes Smedslund)",
    "So, you know, it's impossible to read everyone, everywhere at all times.",
    "Again, we're in the finitary predicament.",
    "But Smedslund pointed out something that Stanovich and West do take seriously and because Smedslund makes an explicit and clear argument for it, we should pay attention to what he says.",
    "Fallacy And Misunderstanding He says, well, there's a difficulty with these.",
    "So this is the third response.",
    "We've had Cohen, we've had Cherniak and now a response that I will attribute to Smedslund.",
    "Stanovich and West don't, but they should, but again, no crime on their part.",
    "Okay.",
    "Smedslund says, well, there's a difficulty with interpreting the experiments.",
    "Again, that's the issue.",
    "Always the issue of interpretation and you can't do an experiment to decide the interpretation because then you have to do—then you have to interpret that experiment.",
    "You can't experiment your way out of interpretation.",
    "Interpretation is always going to be needed in science.",
    "And that means theoretical debate is always going to be needed.",
    "Okay.",
    "So back to the theoretical debate, Smedslund says, now, interpreting these experiments relies on a distinction between a fallacy and a misunderstanding (Fig. 10) (writes Fallacy and Misunderstanding).",
    "Because there's two ways in which I can give the wrong answer.",
    "One is I interpret the problem correctly (writes Interpret the problem correctly below Fallacy).",
    "I understand it (writes Understand).",
    "But then I reason incorrectly, (writes Reason incorrectly) and that's why I get (draws an arrow from Reason incorrectly) the wrong answer (writes Wrong answer below the arrow).",
    "So the fault in a fallacy, fallacious reasoning, is, this is where the error comes in (draws an arrow pointing to Reason incorrectly).",
    "The poor reasoning.",
    "I reason incorrectly.",
    "Fig.",
    "10",
    "But there's another way in which I can give the wrong answer (writes Wrong answer beside Wrong answer).",
    "You get to the same conclusion with the wrong answer.",
    "Well, what is it?",
    "I actually reason correctly (writes Reason correctly above Wrong answer) in a normative fashion, but I've understood the problem incorrectly (writes Understood the problem incorrectly below Misunderstanding).",
    "And that's a misunderstanding.",
    "Somebody misunderstands us, the error comes in (draws an arrow pointing to Understanding the problem incorrectly) because they've interpreted the problem.",
    "They've understood the problem incorrectly.",
    "But once you give them that incorrect interpretation, there's nothing wrong with their reasoning.",
    "There's nothing wrong with their reasoning.",
    "Okay, great.",
    "So there's two places we can get—there's two equally good explanations for why we produce the wrong answers.",
    "One is we reason incorrectly and that's a fallacy because we've got the correct interpretation.",
    "The other one is we are reasoning correctly.",
    "There's nothing wrong with our reasoning.",
    "But we've understood the problem incorrectly and that's a misunderstanding.",
    "Right?",
    "And that's—okay, so this distinction is really crucial (draws a double-headed arrow between Fallacy and Misunderstanding) because this distinction is really key—crucial.",
    "Keeping these apart is really crucial because the—if we want to conclude that people are irrational, we have to be attributing to them (draws an arrow pointing to Fallacy) fallacious cognition, not pure—sorry, not—sorry.",
    "Oh, wow.",
    "Where did pure come?—We have to attribute to them fallacious cognition, not some kind of distortion in the communication that they've misunderstood us.",
    "One of the ways in which people typically often avoid self-criticism, avoiding the possibility that they might have reasoned incorrectly, is to always claim that they have merely been misunderstood.",
    "Been misunderstood.",
    "Look for that in somebody.",
    "Look for somebody who never says, \"Ah, my argument is wrong.",
    "I did it wrong.\"",
    "Look for somebody who always says, \"No, no, I've been misunderstood.\"",
    "Because they're basically trading on this in an equivocal fashion.",
    "They're bullshitting you in a way.",
    "Okay.",
    "So—now, sometimes they should say I've been misunderstood.",
    "Totally.",
    "Sometimes, but sometimes they should say I reasoned wrong.",
    "Okay.",
    "Let's get back to this.",
    "Okay.",
    "So far so good.",
    "Right?",
    "But then Smedslund said, but this is difficult because these things (indicates Fallacy and Misunderstanding) aren't independent the way we need them to be.",
    "What do I mean?",
    "The attribution of fallacy or the attribution of misunderstanding are not independent the way we need them to be in order to cleanly interpret the experimental results as showing that people are largely engaging in fallacious reasoning.",
    "Well, why?",
    "So Smedslund does something and of course, I think it's preliminary, but we'll have to come back to it—which is you have a preliminary account of understanding (writes Understanding).",
    "And he basically says, well, what is it to understand something?",
    "And he says, well, to understand X (Fig. 11a)",
    "(writes X), we ask people to give us something that's identical to X (writes Identical).",
    "Well, let's use arrows here (draws an arrow from X to Identical).",
    "To X. We ask us to give us something that contradicts X (writes X, draws an arrow pointing from X, writes Contradicts X).",
    "We ask us to give things that X implies (writes X, draws an arrow from X, writes Implies).",
    "And these are all, of course, related because identity is a kind of implication.",
    "Contradiction is a failure of implication.",
    "And then we also ask them to give us things that are relevant to X. Yeah, there it is again.",
    "Uh huh.",
    "I would also add, by the way, because when you look at further research on understanding.",
    "People talk about not only what is relevant to X (writes What is relevant to X) but also what is X relevant to (writes X is relevant to), and I don't think Smedslund would object to that.",
    "Here's the—here's relevance again, of course.",
    "Fig. 11a",
    "Okay.",
    "So what's the problem?",
    "Ah, now put this one aside and Smedslund just sort of puts it aside in his argument.",
    "And, of course, that's something I'm not going to, sort of, let sit by.",
    "He puts aside this (draws a bracket to the right of Relevant to X-X is relevant to; labeled A in Fig. 11b).",
    "He says, well, ignoring that (indicates the A in Fig. 11b), look at these three (draws a brack from Identical to X, Contradicts X, Implies; labeled B in Fig. 11b).",
    "The way we determine if somebody has understood us is we determine if they have drawn the identities (indicates Identical to X) we would draw, drawn the contradictions (indicates Contradicts X) we have drawn, drawn the implications we've drawn (indicates Implies).",
    "So somebody understands us if they reason the way that we do.",
    "Fig.",
    "11b",
    "So what Smedslund says, this is what the scientists are assuming.",
    "The scientists are assuming that the participants in the experiment have understood the problem—sorry (indicates Interpret the problem correctly in Fig. 10).",
    "I've understood the problem correctly and then reason incorrectly (indicates Reason incorrectly), but notice how this is a pragmatic contradiction, because if they've understood the point correctly, then they reason the way the scientist does (indicates Fig. 11b) in this very difficult task of interpreting a problem.",
    "And then, but then they reason in the way the scientist doesn’t when they're actually trying to solve the problem.",
    "That's problematic.",
    "In fact, couldn't I say this: couldn't I say, the fact that the participants in the experiment are consistently producing the wrong answer is good evidence that they are misunderstanding the problem?",
    "People are reliably misunderstanding these problems.",
    "\"Well, that can't be because the scientists made them.",
    "Scientists can't be making mistakes.",
    "Scientists can't misunderstand commu—\" What are you attributing to scientists?",
    "God-like authority?",
    "No, stick with the argument here.",
    "Right?",
    "You can, you could, you can conclude that they're making the fallacy, but it's sort of like, you have to say for some reason at this really difficult problem of interpreting what I'm saying, they're reasoning very correctly.",
    "And then when they go to solve the problem, they're reasoning poorly, or you can say, they're reasoning correctly, but they've misunderstood the problem.",
    "But that also means that their reasoning poorly, or maybe, or maybe I've misrepresented or miscommunicated, the problem.",
    "See, now it's much more problematic.",
    "get the notes!",
    "Normativity",
    "On Construal Okay.",
    "So two things to note here, two important things to note.",
    "First of all, we got to come back to this (draws an arrow from Wrong answer to Relevant to X - X is relevant to) because here's—there's an opening here.",
    "Stanovich and West because they haven't read Smedslund and because they don't have this so clearly explicated (indicates Fig. 11b), they can't sort of pick up on that.",
    "So I'm not criticizing them for not seeing this.",
    "But they do come up with a very important point.",
    "And this go—and this is convergent with their argument against Cohen.",
    "They argue for this.",
    "They argue, in order to break this impasse, we need a normativity on construal (writes Normativity on construal).",
    "We need a normativity on how people interpret, make sense of, size up the situation of the problem.",
    "That's what construal means.",
    "Basically, we need a normativity on how they formulate the problem, and this has to be an independent normativity (draws an arrow from Normativity on Construal and writes Independent of inferential norms); independent of what?",
    "Independent of inferential norms.",
    "Right.",
    "If I try to use inference as my stand—good inference as my standard for doing this (indicates Fig. 12a), I'll fall into this circle.",
    "I have to be evaluate—I have to be able to evaluate construal independently of evaluating how people make inferences.",
    "That's the only way I'm going to break out of this.",
    "Fig.",
    "12a",
    "But that's okay, because that means...",
    "Stanovich doesn't take this as deeply as he should, but that means that there's a non-inferential aspect to rationality that is central.",
    "There's an aspect of rationality that has to do with understanding, with construal that is non-inferential in nature.",
    "And that, of course, points back to this (encircles Relevant to X - X is relevant to) because relevance is pre-inferential, the way you formulate your problems—remember that?—has to do with relevance realization.",
    "And that is something that is pre-inferential in nature.",
    "So we can actually put this together very cleanly.",
    "I would argue.",
    "We can say—now, see what Stanovich and West say is, okay, we need this normativity on construal (indicates Normativity on construal).",
    "It has to be independent of our inferential normativity.",
    "And then they go, \"Oh, we don't know what this is (Fig. 12b)",
    "(draws an arrow from Normativity on construal and writes ???).",
    "What could it be?",
    "What that normativity of construal will look like?\"",
    "it's \"Oh, right.\"",
    "And it's, like, granted.",
    "Fig.",
    "12b",
    "But here's a proposal, right?",
    "I think it's a proposal that's, sort of, it's clearly presented to us from a lot of the arguments we've already considered.",
    "Right?",
    "We do have a normativity on construal.",
    "We have standards of what a good problem formulation is (Fig. 13) (writes Good problem formulation) versus a bad problem formulation (writes Bad formulation below Good problem formulation).",
    "Where do we study that normativity in psychology?",
    "Well, we study it in insight problem solving (writes Insight beside Good problem formulation).",
    "We know what a bad problem formulation is.",
    "A bad problem formulation is one that puts you into a combinatorially explosive search space.",
    "A bad problem formulation is one that does not turn your ill-defined problem into a well-defined problem.",
    "A bad problem formulation is one in which you are not paying careful attention to how salience is misleading you.",
    "Fig.",
    "13",
    "Hmm.",
    "So that's telling us something really interesting, right?",
    "That there is, in addition to inference being crucial to being rational, insight is crucial to being rational too, because if what we mean by insight is somebody who is good at formulating problems, avoiding combinatorial explosion, avoiding ill-definededness, avoiding salience misdirection, then being insightful is going to be central to being rational.",
    "It's not going to be something that comes up out of the irrational aspects of the psyche.",
    "It comes in from the non-inferential, but why should we be identifying—nobody is here—identifying rationality with just a pure logical normativity on inference.",
    "So what I propose to you is that we need to understand the role of both insight and inference in rationality, and that's much more problematic.",
    "But we need that because we need to integrate rationality and understanding together in a integrated account and notice how now more and more rationality and wisdom are starting to overlap for us in a serious way, because if I now get rationality and understanding, inference and insight enmeshed together then, of course, I'm starting to talk more and more—and having to happen in a systematic and reliable way—I'm starting to talk more and more about the ways when we talk about how people are wise.",
    "If we give up thinking of rationality as being like Mr. Spock or Mr. Data, and we give up, you know, the idea that being rational is just being really smart, then we start to get into the problematic notion of rationality and we need the idea of multiple competencies.",
    "We need the distinction between being logical and being rational, being intelligent, being rational.",
    "And we have to understand that there's an important component of rationality that is non-influential in nature.",
    "It has to do with a normativity on construal, the generation of insight, not a normativity of argumentation and the generation of inference.",
    "Right?",
    "And that's important.",
    "That's really important (erases the board).",
    "get the notes!",
    "Active Open-Mindedness",
    "So the issue of construal is acknowledged, but not, in any way, resolved by Stanovich.",
    "So it's not going to play, although it should, given his own arguments, it's not going to play a significant role in his theory of rationality.",
    "What is that theory?",
    "What does it look like?",
    "What is the missing piece, according to Stanovich?",
    "So we said, right, that intelligence (Fig.",
    "14a) (writes (g) and writes Intelligence above (g)) is not predictive or an only weakly predictive of rationality (writes (gr) beside (g) and writes Rationality above (gr)).",
    "These are not equivalent (writes ≠ between Intelligence and Rationality).",
    "And along the way we got yet another argument for intelligence being relevance realization.",
    "Okay.",
    "That's all good.",
    "What is the missing piece then?",
    "The relationship here is only 0.3 (writes .3 above ≠).",
    "What accounts for most of the variance then, as a scientist would say?",
    "Fig.",
    "14a",
    "So Stanovich argues very clearly for what he calls a cognitive style.",
    "Well, this, this term is equip—it's a bit equivocal?",
    "It's used in slightly different ways in psychology for different things.",
    "And he also invokes the notion of a bad mindware, inappropriate psychotech.",
    "So that's also in there.",
    "So there's sort of cognitive styles in psychotech that can both be part of the missing variance.",
    "So one part here is the psychotech you're using (Fig. 14b) (writes Psychotech below (g) and (gr))",
    "You can have poor, what he calls mindware, which is like software.",
    "He's picking up on the psychotech idea here, right?",
    "Fig.",
    "14b",
    "And then the other, and this is what often gets given more priority because it counts for a lot is an appropriate cognitive style (writes Cognitive style below Psychotech).",
    "The difference between these (indicates Psychotech and Cognitive style) are not as clear.",
    "I think Stanovich seems to think they are, so we'll have to come back to that when we come back to the relationship between psychotechnology and wisdom.",
    "So what's the cognitive style?",
    "So a cognitive style is an—is something you can learn, at least as Stanovich is using it.",
    "And it's to learn a set of sensitivities and skills, notice the procedurality and the perspectival in here, but it's implicit, it's in the background.",
    "But what is the cognitive style that's most predictive of doing well on the reasoning tasks.",
    "He gets this from Jonathan Baron.",
    "This is the notion of active open-mindedness (writes Active Open-mindedness beside Cognitive Style).",
    "And when you see this, you're going to see a lot of Stoicism here, and this overlaps a lot with the cognitive behavioral therapy that is derived from Stoicism.",
    "And again, that's convergent, that's not by design or deliberate.",
    "And that tells you something, something crucial is being seen here.",
    "What is active open-mindedness?",
    "So active open-mindedness is to train yourself, to look for these patterns of self-deception, to look for biases.",
    "So here's a bias you've heard me mention, you've heard me mention some of these (Fig. 15).",
    "The confirmation bias (writes Confirmation bias) or the essentialism bias (writes Essentialism bias).",
    "Or the representationalism or the availability—misusing the availability heuristic (writes Availability bias).",
    "Notice that a bias is just a heuristic misused.",
    "We've talked about this.",
    "So this is (indicates Confirmation bias), I tend to only look for information that confirms my beliefs, right?",
    "This is (indicates Essentialism bias), I tend to treat any category as a pointing to an essence shared by all the members.",
    "Maybe we should give up an essentialism of sacredness or at least in the terms of its content.",
    "I've already suggested that to you.",
    "The availability heuristic (indicates Availability bias) is the availability biases; I judge somethings probability by how easily I can remember it or imagine it happening.",
    "We've talked about all of these there's many of these.",
    "Fig.",
    "15",
    "So what do I do?",
    "First of all, I have to do the science.",
    "I learn about all of these (encircles Confirmation bias, Essentialism bias, Availability bias).",
    "So this comes from Baron (writes Baron).",
    "I want to point out something that Stanovich doesn't say as clearly as Baron does.",
    "So what I do is I learn about these (indicates Fig. 15)",
    "and I sensitize myself.",
    "I sensitize myself and this is like a virtue cause I have to care about the process, not just the results.",
    "I sensitize myself to looking for these biases in my day to day cognition.",
    "And then I actively counteract them (underlines Active open-mindedness).",
    "I actively say, no, no, no, I'm doing confirmation bias.",
    "I need to look for potential information that will disconfirm it.",
    "And here's where you can now begin to also give up the individualistic assumption of competence.",
    "Part of the way in which I can be rationally competent is I can ask you to help me overcome my confirmation bias, because it's very hard for me to look for information that disconfirms my beliefs.",
    "It's much easier for you.",
    "And then if I practice with you a notch, I can start to internalize you and I can start to get better at looking for my own instances where I fall prey to the confirmation bias.",
    "So I now actively counteract those.",
    "That's sort of where Stanovich leaves it, Baron points out, but you don't overdo this.",
    "Because if you overdo this, you will start to choke on the tsunami of combinatorial explosion that will overwhelm you.",
    "So again, you have to do this and you have to do this to the right degree.",
    "And that becomes much more nebulous.",
    "And again, we're starting to shade over into wisdom, right?",
    "So what you should then ask is what predicts—if being intelligent doesn't predict— if intelligence predicted rationality then being intelligent would predict how well you've cultivated active open-mindedness.",
    "But of course it doesn't.",
    "So what is it about people that predicts how well they will cultivate active open-mindedness and this is the degree to which people have a need for cognition (Fig. 16) (writes Need for cognition below Active open-mindedness).",
    "This is people who problematize things.",
    "They create problems.",
    "They look for problems.",
    "They go out and on their own try to learn.",
    "Fig.",
    "14c I would add to that.",
    "There's two aspects to need for cognition.",
    "There is a curiosity in which I need to have more information so that I can manipulate things more effectively.",
    "And that's important people that are, in that sense, more curious and want to solve problems, not just gather facts, but solve problems because that's what needs for cognition points to.",
    "That's important.",
    "But also think about how important wonder is, right?",
    "How much it opens you up to putting into question your entire worldview, your deeper sense of identity?",
    "Deep need for cognition.",
    "And that's relevant to do because rationality is ultimately an existential issue, not just a theoretical inferential logical issue.",
    "So what we're going to need to do is to come back and look more about at Stanovich's account of rationality.",
    "Some criticisms of it.",
    "And then on the basis of that, because we've already—overlapping with it so much, take a look at some of the key theories of the nature of wisdom and try to draw that together into a viable account of wisdom, which we can then integrate with the account of enlightenment.",
    "Thank you very much for your time and attention.",
    "- END - Episode 41 Notes: To keep this site running, we are an Amazon Associate where we earn from qualifying purchases Stanovich Keith E. Stanovich is Emeritus Professor of Applied Psychology and Human Development, University of Toronto and former Canada Research Chair of Applied Cognitive Science.",
    "Article Mentioned: Individual Differences in Reasoning: Implications for the Rationality Debate Cohen Laurence Jonathan Cohen, FBA, usually cited as L. Jonathan Cohen, was a British philosopher.",
    "Cherniak Christopher Cherniak is an American neuroscientist, a member of the University of Maryland Philosophy Department.",
    "Cherniak’s research trajectory started in theory of knowledge and led into computational neuroanatomy and genomics.",
    "Herbert Simon Herbert Alexander Simon was an American economist, political scientist and cognitive psychologist, whose primary research interest was decision-making within organizations and is best known for the theories of \"bounded rationality\" and \"satisficing\".",
    "Bounded Rationality Bounded rationality is the idea that rationality is limited when individuals make decisions.",
    "Book Mentioned: Models of Bounded Rationality: Empirically Grounded Economic Reason Volume 3 – Buy Here",
    "Smedslund Jonathan Baron Jonathan Miller Baron is a Professor Emeritus of Psychology at the University of Pennsylvania in the science of decision-making.",
    "Confirmation bias Confirmation bias is the tendency to search for, interpret, favor, and recall information in a way that confirms or supports one's prior beliefs, prejudices or values.",
    "Availability bias The availability heuristic, also known as availability bias, is a mental shortcut that relies on immediate examples that come to a given person's mind when evaluating a specific topic, concept, method or decision.",
    "Book Mentioned: Biased – Buy Here Book Mentioned: The 25 Cognitive Biases: Understanding Human Psychology, Decision Making & How to Not Fall Victim to Them – Buy Here Book Mentioned: Cognitive Biases : Your Guide To Rationality: A pocket reference book – Buy Here Pages:",
    "Page 1, Page 2 A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27 Yann LeCun Courant Institute of Mathematical Sciences, New York University yann@cs.nyu.edu Meta - Fundamental AI Research yann@fb.com June 27, 2022 Abstract How could machines learn as efficiently as humans and animals?",
    "How could ma- chines learn to reason and plan?",
    "How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict, and plan at multiple time horizons?",
    "This position paper proposes an architecture and trainingparadigmswithwhichtoconstructautonomousintelligentagents.",
    "Itcombines conceptssuchasconfigurablepredictiveworldmodel, behaviordriventhroughintrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.",
    "Keywords: ArtificialIntelligence,MachineCommonSense,CognitiveArchitecture,Deep Learning,Self-SupervisedLearning,Energy-BasedModel,WorldModels,JointEmbedding Architecture, Intrinsic Motivation.",
    "1",
    "Prologue This document is not a technical nor scholarly paper in the traditional sense, but a position paper expressing my vision for a path towards intelligent machines that learn more like animals and humans, that can reason and plan, and whose behavior is driven by intrinsic objectives, rather than by hard-wired programs, external supervision, or external rewards.",
    "Many ideas described in this paper (almost all of them) have been formulated by many authors in various contexts in various form.",
    "The present piece does not claim priority for any of them but presents a proposal for how to assemble them into a consistent whole.",
    "In particular, the piece pinpoints the challenges ahead.",
    "It also lists a number of avenues that are likely or unlikely to succeed.",
    "The text is written with as little jargon as possible, and using as little mathematical prior knowledge as possible, so as to appeal to readers with a wide variety of backgrounds including neuroscience, cognitive science, and philosophy, in addition to machine learning, robotics, and other fields of engineering.",
    "I hope that this piece will help contextualize some of the research in AI whose relevance is sometimes difficult to see.",
    "1 2 Introduction Animals and humans exhibit learning abilities and understandings of the world that are far beyond the capabilities of current AI and machine learning (ML) systems.",
    "How is it possible for an adolescent to learn to drive a car in about 20 hours of practice and for children to learn language with what amounts to a small exposure.",
    "How is it that most humans will know how to act in many situation they have never encountered?",
    "By contrast, to be reliable, current ML systems need to be trained with very large numbers of trialssothateventherarestcombinationofsituationswillbeencounteredfrequentlyduring training.",
    "Still, our best ML systems are still very far from matching human reliability in real-world tasks such as driving, even after being fed with enormous amounts of supervisory data from human experts, after going through millions of reinforcement learning trials in virtual environments, and after engineers have hardwired hundreds of behaviors into them.",
    "The answer may lie in the ability of humans and many animals to learn world models, internal models of how the world works.",
    "There are three main challenges that AI research must address today: 1. How can machines learn to represent the world, learn to predict, and learn to act largely by observation?",
    "Interactions in the real world are expensive and dangerous, intelligent agents should learn as much as they can about the world without interaction (by observation) so as to minimize the number of expensive and dangerous trials necessary to learn a particular task.",
    "2. How can machine reason and plan in ways that are compatible with gradient-based learning?",
    "Our best approaches to learning rely on estimating and using the gradient of a loss, which can only be performed with differentiable architectures and is difficult to rec- oncile with logic-based symbolic reasoning.",
    "3. How can machines learn to represent percepts and action plans in a hierarchical man- ner, at multiple levels of abstraction, and multiple time scales?",
    "Humans and many animals are able to conceive multilevel abstractions with which long-termpredictionsandlong-termplanningcan beperformedbydecomposing com- plex actions into sequences of lower-level ones.",
    "The present piece proposes an architecture for intelligent agents with possible solutions to all three challenges.",
    "The main contributions of this paper are the following: 1. an overall cognitive architecture in which all modules are differentiable and many of them are trainable (Section 3, Figure 2).",
    "2.",
    "JEPAandHierarchicalJEPA:anon-generativearchitectureforpredictiveworldmod- els that learn a hierarchy of representations (Sections 4.4 and 4.6, Figures 12 and 15).",
    "3. anon-contrastiveself-supervisedlearningparadigmthatproducesrepresentationsthat are simultaneously informative and predictable (Section 4.5, Figure 13).",
    "2 4.",
    "AwaytouseH-JEPAasthebasisofpredictiveworldmodelsforhierarchicalplanning under uncertainty (section 4.7, Figure 16 and 17).",
    "Impatient readers may prefer to jump directly to the aforementioned sections and figures.",
    "2.1 Learning World Models Humanandnon-humananimalsseemabletolearnenormousamountsofbackgroundknowl- edge about how the world works through observation and through an incomprehensibly small amount of interactions in a task-independent, unsupervised way.",
    "It can be hypoth- esized that this accumulated knowledge may constitute the basis for what is often called common sense.",
    "Common sense can be seen as a collection of models of the world that can tell an agent what is likely, what is plausible, and what is impossible.",
    "Using such world models, animals can learn new skills with very few trials.",
    "They can predict the con- sequences of their actions, they can reason, plan, explore, and imagine new solutions to problems.",
    "Importantly, they can also avoid making dangerous mistakes when facing an unknown situation.",
    "The idea that humans, animals, and intelligent systems use world models goes back a long time in psychology (Craik, 1943).",
    "The use of forward models that predict the next stateoftheworldasafunctionofthecurrentstateandtheactionbeingconsideredhasbeen standard procedure in optimal control since the 1950s (Bryson and Ho, 1969) and bears the name model-predictive control.",
    "The use of differentiable world models in reinforcement learning has long been neglected but is making a comeback (see for example (Levine, 2021))",
    "A self-driving system for cars may require thousands of trials of reinforcement learning to learn that driving too fast in a turn will result in a bad outcome, and to learn to slow down to avoid skidding.",
    "By contrast, humans can draw on their intimate knowledge of intuitive physics to predict such outcomes, and largely avoid fatal courses of action when learning a new skill.",
    "Common sense knowledge does not just allow animals to predict future outcomes, but alsotofillinmissinginformation,whethertemporallyorspatially.",
    "Itallowsthemtoproduce interpretations of percepts that are consistent with common sense.",
    "When faced with an ambiguous percept, common sense allows animals to dismiss interpretations that are not consistent with their internal world model, and to pay special attention as it may indicate a dangerous situation and an opportunity for learning a refined world model.",
    "I submit that devising learning paradigms and architectures that would allow machines to learn world models in an unsupervised (or self-supervised) fashion, and to use those models to predict, to reason, and to plan is one of the main challenges of AI and ML today.",
    "One major technical hurdle is how to devise trainable world models that can deal with complex uncertainty in the predictions.",
    "2.2 Humans and Animals learn Hierarchies of Models Humans and non-human animals learn basic knowledge about how the world works in the first days, weeks, and months of life.",
    "Although enormous quantities of such knowledge are acquired quite quickly, the knowledge seems so basic that we take it for granted.",
    "In the first few months of life, we learn that the world is three-dimensional.",
    "We learn that every 3 noitpecreP noitcudorP poin�ng Social helping vs false perceptual Communication hindering beliefs Actions face tracking ra�onal, goal- directed ac�ons biological mo�on gravity, iner�a Physics stability, conserva�on of support momentum Object permanence shape constancy Objects solidity, rigidity Age (months) natural kind categories 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 proto-imita�on crawling walking emo�onal contagion Figure 1: This chart, (courtesy of Emmanuel Dupoux), indicates at what age infants generally acquire various concepts about how the world works.",
    "It is consistent with the idea that abstract concepts, such as the fact that objects are subject to gravity and inertia, are acquired on top of less abstract concepts, like object permanence and the assignment of objects to broad categories.",
    "Much of this knowledge is acquired mostly by observation, with very little direct intervention, particularly in the first few weeks and months.",
    "source of light, sound, and touch in the world has a distance from us.",
    "The fact that every point in a visual percept has a distance is the best way to explain how our view of the world changes from our left eye to our right eye, or when our head is being moved.",
    "Parallax motion makes depth obvious, which in turn makes the notion of object obvious, as well as the fact that objects can occlude more distant ones.",
    "Once the existence of objects is established, they can be automatically assigned to broad categories as a function of their appearance or behavior.",
    "On top of the notion of object comes the knowledge that objects do not spontaneously appear, disappear, change shape, or teleport: they move smoothly and can only be in one place at any one time.",
    "Once such concepts are acquired, it becomes easy to learn that some objects are static, some have predictable trajectories (inanimate objects), some behave in somewhat unpredictable ways (collective phenomena like water, sand, tree leaves in the wind, etc), and some seem to obey different rules (animate objects).",
    "Notions of intuitive physics such as stability, gravity, inertia, and others can emerge on top of that.",
    "The effect of animate objects on the world (including the effects of the subject’s ownactions)canbeusedtodeducecause-and-effectrelationships, ontopofwhichlinguistic and social knowledge can be acquired.",
    "Figure 1, courtesy of Emmanuel Dupoux, shows at what age infants seem to acquire basic concepts such as object permanence, basic categories, intuitive physics, etc.",
    "Concepts at higher levels of abstraction seem to develop on top of lower-level ones.",
    "Equipped with this knowledge of the world, combined with simple hard-wired behav- iors and intrinsic motivations/objectives, animals can quickly learn new tasks, predict the 4 consequences of their actions and plan ahead, foreseeing successful courses of actions and avoiding dangerous situations.",
    "But can a human or animal brain contain all the world models that are necessary for survival?",
    "One hypothesis in this paper is that animals and humans have only one world modelenginesomewhereintheirprefrontalcortex.",
    "Thatworldmodelengineisdynamically configurable for the task at hand.",
    "With a single, configurable world model engine, rather than a separate model for every situation, knowledge about how the world works may be shared across tasks.",
    "This may enable reasoning by analogy, by applying the model configured for one situation to another situation.",
    "To make things concrete, I will directly dive into a description of the proposed model.",
    "5 configurator Short-term memory World Model Perception Actor Critic Cost Intrinsic cost action percept Figure 2: A system architecture for autonomous intelligence.",
    "All modules in this model are as- sumed to be “differentiable”, in that a module feeding into another one (through an arrow connecting them) can get gradient estimates of the cost’s scalar output with respect to its own output.",
    "The configurator module takes inputs (not represented for clarity) from all other modules and configures them to perform the task at hand.",
    "The perception module estimates the current state of the world.",
    "The world model module predicts possible future world states as a function of imagined actions sequences proposed by the actor.",
    "The cost module computes a single scalar output called “energy” that measures the level of dis- comfort of the agent.",
    "It is composed of two sub-modules, the intrinsic cost, which is immutable (not trainable) and computes the immediate energy of the current state (pain, pleasure, hunger, etc), and the critic, a trainable module that predicts future values of the intrinsic cost.",
    "The short-term memory module keeps track of the current and predicted world states and as- sociated intrinsic costs.",
    "The actor module computes proposals for action sequences.",
    "The world model and the critic com- pute the possible resulting outcomes.",
    "The actor can find an optimal action sequence that minimizes the estimated future cost, and output the first action in the optimal sequence.",
    "See Section 3 for details.",
    "6 3 A Model Architecture for Autonomous Intelligence The proposed architecture for autonomous intelligent agents is depicted in Figure 2.",
    "It is composed of a number of modules whose functions are described below.",
    "Some of the modules are configurable on the fly, i.e. their precise function is determined by the configurator module.",
    "The role of the configurator is executive control: given a task to be executed, it pre-configures the perception, the world model, the cost and the actor for the task at hand.",
    "The configurator modulates the parameters of the modules it feeds into.",
    "The configurator module takes input from all other modules and configures them for the task at hand by modulating their parameters and their attention circuits.",
    "In particular, the configurator may prime the perception, world model, and cost modules to fulfill a particular goal.",
    "The perception module receives signals from sensors and estimates the current state of the world.",
    "For a given task, only a small subset of the perceived state of the world is relevant and useful.",
    "The perception module may represent the state of the world in a hierarchical fashion, with multiple levels of abstraction.",
    "The configurator primes the perception system to extract the relevant information from the percept for the task at hand.",
    "The world model module constitutes the most complex piece of the architecture.",
    "Its role is twofold: (1) estimate missing information about the state of the world not provided byperception,(2)predictplausiblefuturestatesoftheworld.",
    "Theworldmodelmaypredict naturalevolutionsoftheworld, ormaypredictfutureworldstatesresultingfromasequence of actions proposed by the actor module.",
    "The world model may predict multiple plausible world states, parameterized by latent variables that represent the uncertainty about the world state.",
    "The world model is a kind of “simulator” of the relevant aspects of world.",
    "What aspects of the world state is relevant depends on the task at hand.",
    "The configurator configures the world model to handle the situation at hand.",
    "The predictions are performed within an abstract representation space that contains information relevant to the task at hand.",
    "Ideally, the world model would manipulate representations of the world state at multiple levels of abstraction, allowing it to predict over multiple time scales.",
    "A key issue is that the world model must be able to represent multiple possible predic- tionsoftheworldstate.",
    "Thenaturalworldisnotcompletelypredictable.",
    "Thisisparticularly true if it contains other intelligent agents that are potentially adversarial.",
    "But it is often true even when the world only contains inanimate objects whose behavior is chaotic, or whose state is not fully observable.",
    "There are two essential questions to answer when building the proposed architectures: (1) How to allow the world model to make multiple plausible prediction and represent uncertainty in the predictions, and (2) how to train the world model.",
    "The cost module measures the level of “discomfort” of the agent, in the form of a scalar quantity called the energy.",
    "The energy is the sum of two energy terms computed by two sub-modules: the Intrinsic Cost module and the Trainable Critic module.",
    "The overall objective of the agent is to take actions so as to remain in states that minimize the average energy.",
    "The Intrinsic Cost module is hard-wired (immutable, non trainable) and computes a singlescalar, theintrinsic energythatmeasurestheinstantaneous“discomfort”oftheagent 7 – think pain (high intrinsic energy), pleasure (low or negative intrinsic energy), hunger, etc.",
    "The input to the module is the current state of the world, produced by the perception module, or potential future states predicted by the world model.",
    "The ultimate goal of the agent is minimize the intrinsic cost over the long run.",
    "This is where basic behavioral drives and intrinsic motivations reside.",
    "The design of the intrinsic cost module determines the nature of the agent’s behavior.",
    "Basic drives can be hard-wired in this module.",
    "This may include feeling “good” (low energy) when standing up to motivate a legged robot to walk, when influencing the state of the world to motivate agency, when interacting with humans to motivate social behavior, when perceiving joy in nearby humans to motivate empathy, when having a full energy supplies (hunger/satiety), when experiencing a new situation to motivatecuriosityandexploration,whenfulfillingaparticularprogram,etc.",
    "Conversely,the energy would be high when facing a painful situation or an easily-recognizable dangerous situation (proximity to extreme heat, fire, etc), or when wielding dangerous tools.",
    "The intrinsic cost module may be modulated by the configurator, to drive different behavior at different times.",
    "The Trainable Critic module predicts an estimate of future intrinsic energies.",
    "Like the intrinsiccost, itsinputiseitherthecurrentstateoftheworldorpossiblestatespredictedby the world model.",
    "For training, the critic retrieves past states and subsequent intrinsic costs stored in the associative memory module, and trains itself to predict the latter from the former.",
    "Thefunctionofthecriticmodulecanbedynamicallyconfiguredbytheconfigurator to direct the system towards a particular sub-goal, as part of a bigger task.",
    "Because both sub-modules of the cost module are differentiable, the gradient of the energy can be back-propagated through the other modules, particularly the world model, the actor and the perception, for planning, reasoning, and learning.",
    "Theshort-termmemorymodulestoresrelevantinformationaboutthepast,current, and future states of the world, as well as the corresponding value of the intrinsic cost.",
    "The world model accesses and updates the short-term memory while temporally predicting future (or past) states of the world, and while spatially completing missing information or correcting inconsistent information about the current world state.",
    "The world model can send queries to the short-term memory and receive retrieved values, or store new values of states.",
    "The critic module can be trained by retrieving past states and associated intrinsic costs from the memory.",
    "The architecture may be similar to that of Key-Value Memory Networks (Miller et al., 2016)",
    "This module can be seen as playing some of same roles as the hippocampus in vertebrates.",
    "The actor module computes proposals for sequences of actions and outputs actions to theeffectors.",
    "Theactorproposesasequenceofactionstotheworldmodel.",
    "Theworldmodel predicts future world state sequences from the action sequence, and feeds it to the cost.",
    "Given a goal defined by the cost (as configured by the configurator), the cost computes the estimated future energy associated with the proposed action sequence.",
    "Since the actor has access to the gradient of the estimated cost with respect to the proposed action sequence, it can compute an optimal action sequence that minimizes the estimated cost using gradient- based methods.",
    "If the action space is discrete, dynamic programming may be used to find an optimal action sequence.",
    "Once the optimization is completed, the actor outputs the first action (or a short sequence of actions) to the effectors.",
    "This process is akin to model-predictive control in optimal control (Bryson and Ho, 1969).",
    "8 C(s[0]) C(s[1]) s[0] s[1] Pred(s,a) action A(s) a[0]",
    "Actor Figure 3: Mode-1 perception-action episode.",
    "The perception module estimates the state of the world s[0]=Enc(x).",
    "The actor directly computes an action, or a short sequence of actions, through a policy module a[0]=A(s[0]).",
    "Thisreactiveprocessdoesnotmakeuseoftheworldmodelnorofthecost.",
    "Thecostmodulecomputes theenergyoftheinitialstatef[0]=C(s[0])andstoresthepairs(s[0],f[0])intheshort-termmemory.",
    "Optionally, it may also predict the next state using the world model s[1] = Pred(s[0],a[0]), and the associated energy f[0] = C(s[0]) so that the world model can be adjusted once the next observation resulting from the action taken becomes available.",
    "The actor may comprise two components: (1) a policy module that directly produces an action from the world state estimate produced by the perception and retrieved from the short-term memory, and (2) the action optimizer, as described above, for model-predictive control.",
    "ThefirstmodeissimilartoDanielKahneman’s“System1”, whilethesecondmode is similar to “System 2” (Kahneman, 2011)",
    "In the following, we will use specific symbols to represent various components in archi- tectural diagrams.",
    "An brief explanation is given in Appendix 8.3.3.",
    "3.1 Typical Perception-Action Loops There are two possible modes that the model can employ for a perception-action episode.",
    "Thefirstoneinvolvesnocomplexreasoning,andproducesanactiondirectlyfromtheoutput of the perception and a possible short-term memory access.",
    "We will call it “Mode-1”, by analogy with Kahneman’s “System 1”.",
    "The second mode involves reasoning and planning through the world model and the cost.",
    "It is akin to model-predictive control (MPC), a classical planning and reasoning paradigm in optimal control and robotics.",
    "We will call it “Mode-2” by analogy to Kahneman’s “System 2”.",
    "We use the term “reasoning” in a broad sense here to mean constraint satisfaction (or energy minimization).",
    "Many types of reasoning can be viewed as forms of energy minimization.",
    "3.1.1 Mode-1: Reactive behavior A perception-action episode for Mode-1 is depicted in Figure 3.",
    "The perception module, through an encoder module, extracts a representation of the state of the world s[0] = Enc(x) containing relevant information for the task at hand.",
    "A 9 policy module, a component of the actor, produces an action as a function of the state a[0] = A(s[0]).",
    "The resulting action is sent to the effectors.",
    "The function of the policy module is modulated by the configurator, which configures it for the task at hand.",
    "The policy module implements a purely reactive policy that does not involve delib- erate planning nor prediction through the world model.",
    "Yet, its structure can be quite sophisticated.",
    "For example, in addition to the state s[0], the policy module may access the short-term memory to acquire a more complete information about previous world states.",
    "It may use the short-term memory for the associative retrieval of an action given the current state.",
    "While the cost module is differentiable, its output f[0] = C(s[0]) is indirectly influenced by previous actions through the external world.",
    "Since the world is not differentiable, one cannot back-propagate gradients from the cost through the chain cost ← perception ← world ← action.",
    "In this mode, gradients of the cost f[0] with respect to actions can only be estimated by polling the world with multiple perturbed actions, but that is slow and potentially dangerous.",
    "This process would correspond to classical policy gradient methods in reinforcement learning.",
    "During Mode-1, the system can optionally adjust the world model.",
    "It runs the world modelforonestep,predictingthenextstates[1],thenitwaitsforthenextperceptresulting from the action taken, and uses the observed world state as a target for the predictor.",
    "With the use of a world model, the agent can imagine courses of actions and predict their effect and outcome, lessening the need to perform an expensive and dangerous search forgoodactions andpoliciesby tryingmultiple actionsinthe externalworld andmeasuring the result.",
    "3.1.2 Mode-2: reasoning and planning using the world model A typical perception-action episode for Mode 2 is depicted in Figure 4. 1.",
    "perception: the perception system extract a representation of the current state of the world s[0] = P(x).",
    "The cost module computes and stores the immediate cost associated with that state.",
    "2. action proposal: the actor proposes an initial sequence of actions to be fed to the world model for evaluation (a[0],...,a[t],...,a[T]).",
    "3. simulation: the world model predicts one or several likely sequence of world state representations resulting from the proposed action sequence (s[1],...,s[t],...,s[T]).",
    "4. evaluation: thecostmoduleestimatesatotalcostfromthepredictedstatesequence, generally as a sum over time steps F(x)",
    "= PT C(s[t]) t=1 5.",
    "planning: theactorproposesanewactionsequencewithlowercost.",
    "Thiscanbedone throughagradient-basedprocedureinwhichgradientsofthecostareback-propagated through the compute graph to the action variables.",
    "The resulting minimum-cost action sequence is denoted (aˇ[0],...,aˇ[T]).",
    "Full optimization may require iterating steps 2-5. 10",
    "C(s[0]) C(s[t]) C(s[t+1]) C(s[T-1]) C(s[T]) Pred(s,a) Pred(s,a) Pred(s,a) Pred(s,a) s[0] s[t] s[t+1] s[T-1] action a[0] a[t] a[t+1] a[T-1]",
    "Actor Figure4: Mode-2perception-actionepisode.",
    "Theperceptionmoduleestimatesthestateoftheworld s[0].",
    "The actor proposes a sequence of actions a[0],a[1],...,a[t],a[t+1],...,a[T].",
    "The world model recursively predicts an estimate of the world state sequence using s[t+1]=Pred(s[t],a[t]).",
    "The cost C(s[t]) computes an energy for each predicted state in the sequence, the total energy being the sum of them.",
    "Through an optimization or search procedure, the actor infers a sequence of actions that minimizes the total energy.",
    "It then sends the first action in the sequence (or the first few actions) to the effectors.",
    "This is, in effect, an instance of classical model-predictive control with receding- horizon planning.",
    "Since the cost and the model are differentiable, gradient-based methods can be used to search for optimal action sequences as in classical optimal control.",
    "Since the total energy is additive over time, dynamic programming can also be used, particularly when the action space is small and discretized.",
    "Pairs of states (computed by the encoder or predicted by the predictor) and corresponding energies from the intrinsic cost and the trainable critic are stored in the short-term memory for subsequent training of the critic.",
    "6. acting: afterconvergingonalow-costactionsequence,theactorsendsthefirstaction (or first few actions) in the low-cost sequence to the effectors.",
    "The entire process is repeated for the next perception-action episode.",
    "7. memory: after every action, the states and associated costs from the intrinsic cost and the critic are stored in the short-term memory.",
    "These pairs can be used later to train or adapt the critic.",
    "This procedure is essentially what is known as Model-Predictive Control (MPC) with receding horizon in the optimal control literature.",
    "The difference with classical optimal control is that the world model and the cost function are learned.",
    "In principle, any form of optimization strategy can be used, for step 5.",
    "While gradient- based optimization methods can be efficient when the world model and cost are well- behaved, situations in which the action-cost mapping has discontinuities may require to use other optimization strategies, particularly if the state and/or action spaces can be discretized.",
    "These strategies include dynamic programming, combinatorial optimization, simulate annealing and other gradient-free methods, heuristic search techniques (e.g. tree search with pruning), etc.",
    "To simplify, the process was described in the deterministic case, i.e. when there is no need to handle the possibility of multiple predictions for s[t + 1] resulting from a given initial state s[t] and action a[t].",
    "In real situations, the world is likely to be somewhat unpredictable.",
    "Multiple states may result from a single initial state and action due to the fact that the world is intrinsically stochastic (aleatoric uncertainty), or that the state representation s[t] contains incomplete information about the true world state (epistemic uncertainty), or that the world model’s prediction accuracy is imperfect due to limited training data, representational power, or computational constraints.",
    "11",
    "C(s[0]) C(s[t]) C(s[t+1]) C(s[T]) s[0] s[t] s[t+1] Pred(s,a) Pred(s,a) Pred(s,a) action A(s[0]) D a[0] A(s[t]) D a[t] A(s[t+1])",
    "a[T-1]",
    "Actor Figure 5: Training a reactive policy module from the result of Mode-2 reasoning.",
    "Using Mode-2 is onerous, because it mobilizes all the resources of the agent for the task at hand.",
    "It involves running theworldmodelformultipletimestepsrepeatedly.",
    "Thisdiagramdepictshowtotrainapolicymodule A(s[t]) to approximate the action that results from Mode-2 optimization.",
    "The system first operates in Mode-2 and produces an optimal sequence of actions (aˇ[0],...,aˇ[T]).",
    "Then the parameters of the policy module are adjusted to minimize a divergence D(aˇ[t]),A(s[t])) between the optimal action and the output of the policy module.",
    "This results in a policy module that performs amortized inference, and produces an approximation for a good action sequence.",
    "The policy module can then be used to produce actions reactively in Mode-1, or to initialize the action sequence prior to Mode-2 inference and thereby accelerate the optimization.",
    "3.1.3 From Mode-2 to Mode-1: Learning New Skills Using Mode-2 is onerous.",
    "The agent only possesses one world model “engine”.",
    "It is config- urable by the configurator for the task at hand, but it can only be used for a single task at a time.",
    "Hence, similarly to humans, the agent can only focus on one complex task at a time.",
    "Mode-1 is considerably less onerous, since it only requires a single pass through a policy module.",
    "The agent may possess multiple policy modules working simultaneously, each specialized for a particular set of tasks.",
    "The process described in Figure 5 shows how a policy module A(s[t]) can be trained to produce approximations of the optimal actions resulting from Mode-2 reasoning.",
    "The system is run on Mode-2, producing an optimal action sequence (aˇ[0],...,aˇ[t],...,aˇ[T]).",
    "Then, the parameters of the policy module A(s[t]) are updated to minimize a divergence measure between its output and the optimal action at that time D(aˇ[t],A(s[t])).",
    "Once properly trained, the policy module can be used to directly produce an action in Mode-1 a˜[0] = A(s[0]).",
    "Itcanalsobeusedtorecursivelycomputeaninitialactionsequenceproposal before Mode-2 optimization: s[t+1] = Pred(s[t],a[t]) ; a˜[t+1] = A(s[t+1])",
    "The policy module can be seen as performing a form of amortized inference.",
    "This process allows the agent to use the full power of its world model and reasoning capabilities to acquire new skills that are then “compiled” into a reactive policy module that no longer requires careful planning.",
    "3.1.4",
    "Reasoning as Energy Minimization The process of elaborating a suitable action sequence in Mode-2 can be seen as a form of reasoning.",
    "This form of reasoning is based on simulation using the world model, and 12 Intrinsic Cost (IC) Trainable Cost / Critic (TC) ... ...",
    "IC 1 (s) IC 2 (s) IC k (s) TC1(s) TC2(s) TCl(s) s Figure 6:",
    "Architecture of the cost module.",
    "The cost module comprises the intrinsic cost module which is immutable IC (s) (left) and the critic or Trainable Cost TC (s) (right), which is trainable.",
    "i j",
    "Both IC and TC are composed of multiple submodules whose output energies are linearly combined.",
    "Each submodule imparts a particular behavioral drive in the agent.",
    "The weights in the linear com- bination, u and v , are determined by the configurator module and allow the agent to focus on i j different subgoals at different times.",
    "optimization of the energy with respect to action sequences.",
    "More generally, the “actions” can be seen as latent variables representing abstract transformations from one state to the next.",
    "This type of planning though simulation and optimization may constitute the kind of reasoning that is most frequent in natural intelligence.",
    "Many classical forms of reasoning in AI can actually be formulated as optimization problems (or constraint satisfaction problems).",
    "It is certainly the case for the kind of probabilistic inference performed with factor graphs and probabilistic graphical models.",
    "The proposed architecture is, in fact, a factor graph in which the cost modules are log factors.",
    "But the kind of reasoning that the proposed architecture enables goes beyond traditional logical and probabilistic reasoning.",
    "It allows reasoning by simulation and by analogy.",
    "3.2",
    "The Cost Module as the Driver of Behavior The overall architecture of the cost module is shown in Figure 6.",
    "It is composed of the intrinsic cost module which is immutable IC (s) and the critic or Trainable Cost TC (s), i j which is trainable.",
    "Both IC and TC are composed of multiple submodules whose output energies are linearly combined C(s) = IC(s)+TC(s) (1) k X IC(s)",
    "= u IC (s) (2)",
    "i i i=1 13 l X TC(s)",
    "= v TC (s) (3) j j j=1",
    "Each submodule imparts a particular behavioral drive to the agent.",
    "The weights in the linear combination, u and v , are modulated by the configurator module and allow the i j agent to focus on different subgoals at different times.",
    "The intrinsic cost module (IC) is where the basic behavioral nature of the agent is defined.",
    "It is where basic behaviors can be indirectly specified.",
    "For a robot, these terms would include obvious proprioceptive measurements corre- sponding to “pain”, “hunger”, and “instinctive fears”, measuring such things as external force overloads, dangerous electrical, chemical, or thermal environments, excessive power consumption, low levels of energy reserves in the power source, etc.",
    "They may also include basic drives to help the agent learn basic skills or accomplish its missions.",
    "For example, a legged robot may comprise an intrinsic cost to drive it to stand up and walk.",
    "This may also include social drives such as seeking the company of humans, finding interactions with humans and praises from them rewarding, and finding their pain unpleasant (akin to empathy in social animals).",
    "Other intrinsic behavioral drives, such as curiosity, or taking actions that have an observable impact, may be included to maximize the diversity of situations with which the world model is trained (Gottlieb et al., 2013)",
    "The IC can be seen as playing a role similar to that of the amygdala in the mammalian brain and similar structures in other vertebrates.",
    "To prevent a kind of behavioral collapse or an uncontrolled drift towards bad behaviors, the IC must be immutable and not subject to learning (nor to external modifications).",
    "Theroleofthecritic(TC)istwofold: (1)toanticipatelong-termoutcomeswithminimal use of the onerous world model, and (2) to allow the configurator to make the agent focus on accomplishing subgoals with a learned cost.",
    "In general, the behavioral nature of an AI agent can be specified in four ways: 1. by explicitly programming a specific behavior activated when specific conditions are met 2.",
    "by defining an objective function in such a way that the desired behavior is executed by the agent as a result of finding action sequences that minimize the objective.",
    "3. by training the agent to behave a certain way through direct supervision.",
    "The agent observes the actions of an expert teacher, and trains a Mode-1 policy module to reproduce it.",
    "4. by training the agent through imitation learning.",
    "The agent observes expert teachers, and infers an objective function that their behavior appears to be optimizing when they act.",
    "This produces a critic submodule for Mode-2 behavior.",
    "This process is sometimes called inverse reinforcement learning.",
    "The second method is considerably simpler to engineer than the first one, because it merely requires to design an objective, and not design a complete behavior.",
    "The second method is also more robust: a preordained behavior may be invalidated by unexpected conditions or a changing environment.",
    "With an objective, the agent may adapt its behavior 14 Loss Critic read read Short-Term ... ...",
    "... ...",
    "Associative Memory write write write Intrinsic Cost State Sequence Figure 7: Training the critic.",
    "During planning episodes, the intrinsic cost module stores triplets (time, state, intrinsic energy): (τ,s ,IC(s )) into the associative short-term memory.",
    "During critic τ τ trainingepisodes,thecriticretrievesapaststatevectors ,togetherwithanintrinsicenergyatalater τ time IC(s ).",
    "In the simplest scenario, the critic adjusts its parameters to minimize a divergence τ+δ measure between the target IC(s ) and the predicted energy C(s ).",
    "In more complex schemes, tau+δ τ it may use combinations of future intrinsic energies as targets.",
    "Note that the state sequence may contain information about the actions planned or taken by the agent.",
    "to satisfy the objective despite unexpected conditions and changes in the environment.",
    "The second method exploits the learning and inference abilities of the agent to minimize the amount of priors hard-wired by the designer that are likely to be brittle.",
    "3.3",
    "Training the Critic An essential question is how to train the critic.",
    "The principal role of the critic is to predict future values of the intrinsic energy.",
    "To do so, it uses the short-term memory module.",
    "This module is an associative memory in which the intrinsic cost module stores triplets (time, state, intrinsic energy): (τ,s ,IC(s )).",
    "The τ τ stored states and corresponding intrinsic energies may correspond to a perceived state or to a state imagined by the world model during a Mode-2 episode.",
    "The memory may retrieve a state s given a time τ, and may retrieve an energy IC(s ) given a time τ or a state s .",
    "τ τ",
    "τ With a suitable memory architecture, the retrieval may involve interpolations of keys and retrieved values.",
    "The process is shown in Figure 7",
    "The critic can be trained to predict future intrinsic energy values by retrieving a past state vector s together with an intrinsic energy at a later time IC(s ).",
    "The parameters τ τ+δ of the critic can then be optimized to minimize a prediction loss, for example ||IC(s )",
    "− τ+δ TC(s )||2.",
    "This is a simple scenario.",
    "More complex schemes can be devised to predict τ expectations of discounted future energies, or distributions thereof.",
    "Note that the state vectors may contain information about the actions taken or imagined by the actor.",
    "At a general level, this is similar to critic training methods used in such reinforcement learning approaches as A2C. 15",
    "The short-term memory can be implemented as the memory module in a key-value memory network ??:",
    "a query vector is compared to a number of key vectors, producing a vector of scores.",
    "The scores are normalized and used as coefficients to output a linear combination of the stored values.",
    "It can be seen as a “soft” associative memory capable of interpolation.",
    "Oneadvantageofitisthat, withaproperallocationschemeofnewkey/value slots, it is capable of one-shot learning, yet can interpolate between keys and is end-to-end differentiable.",
    "4",
    "Designing and Training the World Model Arguably, designing architectures and training paradigms for the world model constitute the main obstacles towards real progress in AI over the next decades.",
    "One of the main contributions of the present proposal is precisely a hierarchical architecture and a training procedure for world models that can represent multiple outcomes in their predictions.",
    "Training the world model is a prototypical example of Self-Supervised Learning (SSL), whose basic idea is pattern completion.",
    "The prediction of future inputs (or temporarily unobserved inputs) is a special case of pattern completion.",
    "In this work, the primary purpose of the world model is seen as predicting future representations of the state of the world.",
    "There are three main issues to address.",
    "First, quite evidently, the quality of the world model will greatly depend on the diversity of state sequences, or triplets of (state, action, resultingstate)itisabletoobservewhiletraining.",
    "Second, becausetheworldisnotentirely predictable, there may be multiple plausible world state representations that follow a given world state representation and an action from the agent.",
    "The world model must be able to meaningfully represent this possibly-infinite collection of plausible predictions.",
    "Third, the world model must be able to make predictions at different time scales and different levels of abstraction.",
    "The first issue touches on one of the main questions surrounding learning for sequential decision processes: the diversity of the “training set” depends on the actions taken.",
    "The issue is discussed in Section 4.10 below.",
    "The second issue is even more dire: the world is not entirely predictable.",
    "Hence, the world model should be able to represent multiple plausible outcomes from a given state and (optionally) an action.",
    "This may constitute one of the most difficult challenges to which the present proposal brings a solution.",
    "This issue is discussed in Section 4.8 below.",
    "The third issue relates to the problem of long-term prediction and planning.",
    "Humans plan complex goals at an abstract level and use high-level descriptions of the world states and actions to make predictions.",
    "High-level goals are then decomposed into sequences of more elementary sequences of subgoals, using shorter-term prediction from the world model to produce lower-level actions.",
    "This decomposition process is repeated all the way down to millisecond-by-millisecond muscle control, informed by local conditions.",
    "The question of how world models could represent action plans at multiple time scales and multiple levels of abstraction is discussed in Section 4.6 16 4.1 Self-Supervised Learning Self-Supervised Learning (SSL) is a paradigm in which a learning system is trained to capture the mutual dependencies between its inputs.",
    "Concretely, this often comes down to training a system to tell us if various parts of its input are consistent with each other.",
    "For example, in a video prediction scenario, the system is given two video clips, and must tell us to what degree the second video clip is a plausible continuation of the first one.",
    "In a pattern completion scenario, the system is given part of an input (image, text, audio signal) together with a proposal for the rest of the input, and tells us whether the proposal is a plausible completion of the first part.",
    "In the following, we will denote the observed part of the input by x and the possibly-unobserved part by y.",
    "Importantly, we do not impose that the model be able to predict y from x.",
    "The reason is that there may be an infinite number of y that are compatible with a given x. In a video prediction setting, there is an infinite number of video clips that are plausible continuations of a given clip.",
    "It may be difficult, or intractable, to explicitly represent the set of plausible predictions.",
    "But it seems less inconvenient to merely ask the system to tell us if a proposed y is compatible with a given x.",
    "A general formulation can be done with the framework of Energy-Based Models (EBM).",
    "The system is a scalar-valued function F(x,y) that produces low energy values when x and y are compatible and higher values when they are not.",
    "The concept is depicted in Figure 8.",
    "Datapointsareblackdots.",
    "Theenergyfunctionproduceslowenergyvaluesaroundthedata points, andhigherenergiesawayfromtheregionsofhighdatadensity, assymbolizedbythe contour lines of the energy landscape.",
    "The EBM implicit function formulation enables the system to represent multi-modal dependencies in which multiple values of y are compatible with a given x.",
    "The set of y compatible with a given x may be a single point, multiple discrete points, a manifold, or a collection of points and manifolds.",
    "To enable Mode-2 planning, a predictive world model should be trained to capture the dependencies between past and future percepts.",
    "It should be able to predict representations of the future from representations of the past and present.",
    "The general learning principle is as follows: given two inputs x and y, learn two functions that compute representations s = g (x) and s = g (y) such that (1) s and s are maximally informative about x and x x y y x y y and (2) s can easily be predicted from s .",
    "This principle ensures a trade-off between y x making the evolution of the world predictable in the representation space, and capturing as much information as possible about the world state in the representation.",
    "What concepts could such an SSL system learn by being trained on video?",
    "Our hypoth- esis is that a hierarchy of abstract concepts about how the world works could be acquired.",
    "Learning a representation of a small image region such that it is predictable from neigh- boring regions surrounding it in space and time would cause the system to extract local edges and contours in images, and to detect moving contours in videos.",
    "Learning a repre- sentationofimagessuchthattherepresentationofascenefromoneviewpointispredictable from the representation of the same scene from a slightly different viewpoint would cause the system to implicitly represent a depth map.",
    "A depth map is the simplest way to explain howaviewofascenechangeswhenthecameramovesslightly.",
    "Oncethenotionofdepthhas been learned, it would become simple for the system to identify occlusion edges, as well as thecollectivemotionofregionsbelongingtoarigidobject.",
    "Animplicitrepresentationof3D 17 Energy F(x,y) Function y x y time or space → x Figure 8: Self-Supervised Learning (SSL) and Energy-Based Models (EBM).",
    "SSL is a learning paradigminwhichalearningsystemistrainedto“fillintheblanks”, ormorepreciselytocapturethe dependencies between observed parts of the input and possibly unobserved parts of the input.",
    "Part of the input signal is observed and denoted x (in pink), and part of the input signal is either observed or unobserved and denoted y (in blue).",
    "In a temporal prediction scenario, x represents past and present observations, and y represent future observations.",
    "In a general pattern completion scenario, various parts of the input may be observed or unobserved at various times.",
    "The learning system is trained to capture the dependencies between x and y through a scalar-valued energy function F(x,y) that takes low values when x and y are consistent or compatible, and higher values if x and y are inconsistent or incompatible.",
    "In a video prediction scenario, the system would produce a low energy valueifavideoclipy isaplausiblecontinuationofthevideoclipx.",
    "Thisenergy-basedmodel(EBM) formulation enables the system to represent multi-modal dependencies in which multiple values of y (perhaps an infinite set) may be compatible with a given x. In the right panel, an energy landscape is represented in which dark discs represent data points, and closed lines represents contours (level sets) of the energy function.",
    "objectsmayspontaneouslyemerge.",
    "Oncethenotionofobjectemergesintherepresentation, concepts like object permanence may become easy to learn: objects that disappear behind others due to parallax motion will invariably reappear.",
    "The distinction between inanimate and animate object would follow: inanimate object are those whose trajectories are easily predictable.",
    "Intuitive physics concepts such as stability, gravity, momentum, may follow by training the system to perform longer-term predictions at the object representation level.",
    "One may imagine that through predictions at increasingly abstract levels of representation and increasingly long time scales, more and more complex concepts about how the world works may be acquired in a hierarchical fashion.",
    "The idea that abstract concepts can be learned through prediction is an old one, for- mulated in various way by many authors in cognitive science, neuroscience, and AI over several decades.",
    "The question is how to do it, precisely.",
    "18 Figure 9: Latent-Variable Energy-Based Model (LVEBM).",
    "Toevaluatethedegreeofcompatibilitybetweenxandy,anEBMmayneedthehelpofalatentvariable z.",
    "The latent variable can be seen as parameterizing the set of possible relationships between an x and a set of compatible y. Latent variables represent information about y that cannot be extracted from x.",
    "For example, if x is a view of an object, and y another view of the same object, z may parameterize the camera displacement between the two views.",
    "Inference consists in finding the latent that minimizes the energy zˇ = argmin E (x,y,z).",
    "The resulting energy F (x,y) = E (x,y,zˇ) z∈Z w w w only depends on x and y.",
    "In the dual view example, inference finds the camera motion that best explains how x could be transformed into y. 4.2",
    "Handling Uncertainty with Latent Variables As was pointed out above, one of the main issues is enabling the model to represent mul- tiple predictions.",
    "This may require the use of a latent variable.",
    "A latent variable is an input variable whose value is not observed but inferred.",
    "A latent variable can be seen as parameterizing the set of possible relationships between an x and a set of compatible y. Latent variables are used to represent information about y that cannot be extracted from x. Imagine a scenario in which x is a photo of a scene, and y a photo of the same scene from a slightly different viewpoint.",
    "To tell whether x and y are indeed views from the same scene, one may need to infer the displacement of the camera between the two views.",
    "Similarly, if x is a picture of a car coming to a fork in the road, and y is a picture of the same car a few seconds later on one of the branches of the fork, the compatibility between x and y depends on a binary latent variable that can be inferred: did the car turn left or right.",
    "In a temporal prediction scenario, the latent variable represents what cannot be pre- dicted about y (the future) solely from x and from past observations (the past).",
    "It should contain all information that would be useful for the prediction, but is not observable, or not knowable.",
    "I may not know whether the driver in front of me will turn left or right, accelerate or brake, but I can represent those options by a latent variable.",
    "A latent-variable EBM (LVEBM) is a parameterized energy function that depends on x, y, and z: E (x,y,z).",
    "When presented with a pair (x,y) the inference procedure of the w EBM finds a value of the latent variable z that minimizes the energy zˇ= argminE (x,y,z) (4) w z∈Z 19",
    "This latent-variable inference by minimization allows us to eliminate z from the energy function: F (x,y) = minE (x,y,z) = E (x,y,zˇ) (5) w w w z∈Z Technically, F (x,y) should be called a zero-temperature free energy, but we will continue w to call it the energy.",
    "4.3 Training Energy-Based Models BeforewediscussEBMtraining, itisimportanttonotethatthedefinitionofEBMdoes not make any reference to probabilistic modeling.",
    "Although many EBMs can easily be turned into probabilistic models, e.g. through a Gibbs distribution, this is not at all a necessity.",
    "Hence the energy function is viewed as the fundamental object and is not assumed to implicitly represent the unnormalized logarithm of a probability distribution.",
    "Training an EBM consists in constructing an architecture (e.g. a deep neural network) to compute the energy function F (x,y) parameterized with a parameter vector w.",
    "The w training process must seek a w vector that gives the right shape to the energy function.",
    "For a given x from the training set, a well-trained F (x,y) will produce lower energies for w values of y that are associated with x in the training set, and higher energies to other values of y. Given a training sample (x,y), training an EBM comes down to devising a suitable loss functional L(x,y,F (x,y)), which can be expressed directly as a function of the parameter w vector L(x,y,w), and such that minimizing this loss will make the energy of the training sample F (x,y) lower than the energies F (x,yˆ) of any yˆ different from y. w w Making the energy of the training sample low is easy: it is sufficient for the loss to be an increasing function of the energy, and for the energy to have a lower bound.",
    "The difficult question is how to ensure that the energies of yˆ different from y are higher than the energy of y. Without a specific provision to ensure that F (x,y0) > F (x,y) w w wheneveryˆ6= y theenergylandscapemaysufferacollapse: givenanxtheenergylandscape could become “flat”, giving essentially the same energy to all values of y.",
    "What EBM architectures are susceptible to collapse?",
    "Whether an EBM may be susceptible to collapse depends on its architecture.",
    "Figure 10 shows a number of standard architectures and indicates whether they can be subject to collapse.",
    "A regular predictive or deterministic-generative architecture (Figure 10(a)) cannot col- lapse.",
    "Foranyx, asingley˜isproduced.",
    "Theenergyiszerowhenevery = y˜. Anyy different from y˜ will have a higher energy, as long as D(y,y˜) is strictly larger than zero whenever y is different from y˜.",
    "A generative latent-variable architecture (non-deterministic generative) (Figure 10(b)) can collapse when the latent variable has too much information capacity.",
    "When the latent variable z varies over the set Z, the prediction y˜ varies over a set Pred(s ,Z), which must x match the set of y that are compatible with x. If Z is too “large” then the region of low- energy y may be larger than the region of high data density.",
    "If z has the same dimension as y, the system could very well give zero energy to the entire y space.",
    "An auto-encoder (AE) (Figure 10(c)) can collapse when the representation s has too y much information capacity.",
    "For example, if the dimension of s is equal or higher than that y 20 a) Prediction / regression b)",
    "Generative latent-variable Architecture c) Auto-Encoder d)",
    "Joint Embedding Architecture",
    "NO COLLAPSE CAN COLLAPSE CAN COLLAPSE CAN COLLAPSE Figure 10:",
    "A few standard architectures and their capacity for collapse.",
    "(a) Deterministic generative architecture: cannot collapse because it can only produce a single out- put.",
    "For a given x, only one value of y may have zero energy: y = y˜. Other values of y will have higher energy if D(u,y˜) is larger than zero when y 6=y˜. (b) Non-deterministic generative architecture: can collapse when the latent variable has excessive information capacity.",
    "If for a given x and for all y there exists a z that produces zero prediction energy (e.g. if z has the same or higher dimension as y), the entire y space will have low energy.",
    "The information capacity of z should be just enough so that varying z over its set will produce all the plausible y˜ for a given x. (c) Auto-encoder: can collapse if the system learns the identity function or if it can correctly recon- struct a region of y space that is much larger than the region of high data density, thereby giving low energy to an overly large region.",
    "(d)",
    "Simple joint embedding architecture: can collapse if the encoders ignore the inputs and produce representations that remain constant and equal, or if the encoders are invariant over overly broad regions of the space.",
    "of y, the AE could learn the identity function, producing a reconstruction error equal to zero over the entire y space.",
    "Lastly, a Joint Embedding Architecture (JEA) (Figure 10(d)) can collapse when the information carried by s and/or s are insufficient.",
    "If the encoders ignore the inputs, and x y produce constant and equal codes s = s , the entire space will have zero energy.",
    "x",
    "y These are but a few examples of architectures.",
    "How do we design the loss to prevent collapse?",
    "There are two approaches: contrastive methods and regularized methods.",
    "In the following, I will argue that contrastive methods have flaws and that regularized (non contrastive) methods are much more likely to be preferable in the long run.",
    "Contrastive methods consist in using a loss functional whose minimization has the effect of pushing down on the energies of training samples (x,y), and pulling up on the energies of suitably-hallucinated “contrastive” samples (x,yˆ).",
    "The contrastive sample yˆ should be picked in such a way as to ensure that the EBM assigns higher energies to points outside the regions of high data density.",
    "This translates into designing a loss that is an increasing function of F (x,y) and a decreasing function of F (x,yˆ), at least whenever w w F (x,yˆ) is not sufficiently higher than F (x,y).",
    "There are many such contrastive loss w w 21 y",
    "Contrastive samples Low energy region Contrastive y Method x y x",
    "Training samples Regularized Method x Figure 11: Contrastive and regularized methods for EBM training.",
    "A conceptual diagram of an energy landscape is shown on the left.",
    "Training samples are blue dots.",
    "The region of low energy is shown in orange (a level set of the energy function).",
    "Contrastive methods (top right) push down on the energy of training samples (blue dots) and pulls up on the energies of suitably-placed contrastive samples (green dots).",
    "regularized methods (bottom right) push down on the energy of training samples and use a reg- ularizer term that minimizes the volume of low-energy regions.",
    "This regularization has the effect of “shrink-wrapping” the regions of high data density within the low-energy regions, to the extent that the flexibility of the energy function permits it.",
    "One issue with contrastive methods is that the energy will only be pulled up wherever contrastive samples have been placed.",
    "One must devise methods that preferentially place contrastive samples in regions of low energy, which is what Monte-Carlo and MCMC methods do.",
    "However, a disadvantage ofcontrastivemethodsisthat thenumberofcontrastivesamplesnecessarytomakean energy surface adopt a good shape may grow exponentially with the dimension of y space.",
    "functions, some of them taking a single triplet (x,y,yˆ), others requiring a batch of positive and contrastive values of y.",
    "A simple example of contrastive loss functions is as follows:",
    "L(w,x,y,yˆ) = H(F (x,y),F (x,yˆ),m(y,yˆ))",
    "(6) w w whereH isanincreasingfunctionofF (x,y),andadecreasingfunctionofF (x,yˆ)whenever w w the latter is less than the former plus a positive margin function m(y,yˆ).",
    "A simple instance of such loss is the distance-dependent hinge loss: h i+ L(w,x,y,yˆ) = F (x,y)−F (x,yˆ)+µ||y−yˆ||2 (7) w w where [a]+ is the identity when a is positive and zero otherwise.",
    "This makes the energy grow at least quadratically with the distance to the data manifold.",
    "Other contrastive loss 22 functionals take multiple contrastive samples into consideration:",
    "L(w,x,y,yˆ[1],...,yˆ[K]) = H(F (x,y),F (x,yˆ[1]),...,F (x,yˆ[K])) (8) w w w Which must be an increasing function of the first argument, and a decreasing function of all other arguments.",
    "An example of such loss is the popular InfoNCE: \" K # X L(w,x,y,yˆ[1],...,yˆ[K]) = F (x,y)+log exp(−F (x,y))+ exp(−F (x,yˆ[k])) (9) w w w k=1 Contrastive methods are very popular, particularly for Siamese network architectures trainedwithpairswherexisadistortedorcorruptedversionofy andyˆanotherrandom(or suitably chosen) training sample.",
    "This includes such methods as the original Siamese net, aswellasmorerecentmethodsincludingDrLIM,PIRL,MoCO,SimCLR,CPT,andothers.",
    "Contrastivemethodsalsoincludesuchclassicalmethodsasprobabilisticmodelstrainedwith maximumlikelihoodthatarenotautomaticallynormalized.",
    "Contrastivesamplesyˆareoften producedusingMonteCarlomethods,Markov-ChainMonteCarlomethods,orapproximate versions thereof, such as Contrastive Divergence.",
    "Generative Adversarial Networks can also be seen as contrastive methods in which the yˆ are produced by the trainable generator network.",
    "Denoising Auto-Encoders and their special case, Masked Auto-Encoders, are also examplesofcontrastivetrainingmethodsinwhichtheyˆisgeneratedbycorruptingtheclean y.",
    "A more detailed discussion of various contrastive methods is given in appendix 8.3.3.",
    "But there are two main issues with contrastive methods.",
    "First, one has to design a scheme to generate or pick suitable yˆ.",
    "Second, when y is in a high-dimensional space, and if theEBMisflexible,itmayrequireaverylargenumberofcontrastivesamplestoensurethat the energy is higher in all dimensions unoccupied by the local data distribution.",
    "Because of the curse of dimensionality, in the worst case, the number of contrastive samples may grow exponentially with the dimension of the representation.",
    "This is the main reason why I will argue against contrastive methods.",
    "Regularized methods for EBM training are much more promising in the long run than contrastive methods because they can eschew the curse of dimensionality that plagues contrastive methods.",
    "They consist in constructing a loss functional that has the effect of pushing down on the energies of training samples, and simultaneously minimizing the volume of y space to which the model associates a low energy.",
    "The volume of the low-energy region is measured by a regularization term in the energy and/or in the loss.",
    "By minimizing this regularization term while pushing down on the energies of data points, the regions of low energy will “shrink-wrap” the regions of high data density.",
    "The main advantage of non-contrastive regularized methods is that they are less likely than contrastive methods to fall victim to the curse of dimensionality.",
    "The main question is precisely how to design such volume-minimizing regularizers.",
    "The answer depends greatly on the architecture of the model, which is discussed in the next sections.",
    "However, non-contrastive methods have existed for a long time.",
    "Examples include sparse modeling, sparse auto-encoders, and auto- encoders with noisy latent variables, such as VAE.",
    "It is important to note that contrastive and regularized methods are not incompatible with each other, and can be used simultaneously on the same model.",
    "How would regularized methods apply to the architectures of Figure 10(b-d)?",
    "23",
    "In the latent-variable generative architecture, restricting the information capacity of z will restrict the volume of y space that can take low energy.",
    "If z is discrete with possible k values, at most k points in y space will have zero energy.",
    "if Z is a manifold of dimension d then the region of y space with zero energy will have at most d dimensions.",
    "Similarly, in the auto-encoder architecture, restricting the information capacity of s y will restrict the volume of y space that can be reconstructed with low energy.",
    "Lastly, in the Joint Embedding Architecture, Maximizing the information that s con-",
    "x tains about x",
    "and s contains about y will minimize the volume of y space that can take y low energy.",
    "Inthefollowing,wewillfocusonanarchitectureforSSLtheJointEmbeddingPredictive Architectures(JEPA)whichcanseenasacombinationoftheJointEmbeddingArchitecture and the Latent-Variable Generative Architecture.",
    "The JEPA is non-generative in that it does not actually predict y, but predicts the representation of y, s from that of x, s .",
    "y x 4.4 Joint Embedding Predictive Architecture (JEPA)",
    "The centerpiece of this paper is the Joint Embedding Predictive Architecture (JEPA).",
    "JEPA is not generative in the sense that it cannot easily be used to predict y from x.",
    "It merely capture the dependencies between x and y without explicitly generating predictions of y. AgenericJEPAisshowninFigure12.",
    "Thetwovariablesxandy arefedtotwoencoders producing two presentations s and s .",
    "These two encoders may be different.",
    "They are not x y required to possess the same architecture nor are they required to share their parameters.",
    "This allows x and y to be different in nature (e.g. video and audio).",
    "A predictor module predicts the representation of y from the representation of x.",
    "The predictor may depend on a latent variable z.",
    "The energy is simply the prediction error in representation space: E (x,y,z) = D(s ,Pred(s ,z))",
    "(10) w y x",
    "The overall energy is obtained by minimizing over z: zˇ = argminE (x,y,z) = argminD(s ,Pred(s ,z))",
    "(11) w y x z∈Z z∈Z F (x,y) = minE (x,y,z) = D(s ,Pred(s ,zˇ)) (12) w w y x z∈Z (13)",
    "The main advantage of JEPA is that it performs predictions in representation space, eschewing the need to predict every detail of y.",
    "This is enabled by the fact that the encoder of y may choose to produce an abstract representation from which irrelevant details have been eliminated.",
    "But there are two ways a JEPA may represent the multiplicity of values of y compatible with x.",
    "The first one is invariance properties of the y encoder, the second one is the latent variable z, as explained below.",
    "multi-modality through encoder invariance: The encoder function s = Enc(y)",
    "y may have invariance properties.",
    "If all the y’s in a set map to the same value of s , all those y y’s will have identical energies.",
    "With JEPA, we lose the ability to generate outputs, but we gain a powerful way to represent multi-modal dependencies between inputs and outputs.",
    "24 Figure12: TheJoint-EmbeddingPredictiveArchitecture(JEPA)consistsoftwoencodingbranches.",
    "The first branch computes s , a representation of x and the second branch s a representation of y. x y",
    "The encoders do not need to be identical.",
    "A predictor module predicts s from s with the possible y x help of a latent variable z.",
    "The energy is the prediction error.",
    "Simple variations of the JEPA may use no predictor, forcing the two representations to be equal, or may use a fixed predictor with no latent, or may use simple latents such as discrete variables.",
    "The main advantage of JEPA is that it performs predictions in representation space, eschewing the need to predict every detail of y, and enabling the elimination of irrelevant details by the encoders.",
    "More precisely, the main advantage of this architecture for representing multi-modal dependencies is twofold: (1) the encoder function s = Enc(y) may possess invariance properties that will make it y produce the same s for a set of different y.",
    "This makes the energy constant over this set and allows y the model to capture complex multi-modal dependencies; (2) The latent variable z, when varied over a set Z, can produce a set of plausible predictions Pred(s ,Z)={s˜ =Pred(s ,z)∀z ∈Z} x y x",
    "If x is a video clip of a car approaching a fork in the road, s and s may represent the position, x y orientation, velocity and other characteristics of the car before and after the fork, respectively, ig- noring irrelevant details such as the trees bordering the road or the texture of the sidewalk.",
    "z may represent whether the car takes the left branch or the right branch of the road.",
    "multi-modality through latent variable predictor: Thepredictormayusealatent variablez tocapturetheinformationnecessarytopredicts thatisnotpresentins .",
    "When y x z is varied over a set Z, the predictor produces a set of plausible predictions Pred(s ,Z) = x {s˜ = Pred(s ,z) ∀z ∈ Z}.",
    "For example, if x is a video clip of a car approaching a fork y x in the road, s and s may represent the past and future positions, orientations, velocities x y and other characteristics of the car, ignoring irrelevant details such as the trees bordering the road or the texture of the sidewalk.",
    "The latent z may be a binary variable indicating whether the car takes the left branch (z = 0) or the right branch (z = 1 if the road.",
    "If the car takes the left branch, the value z = 0 will produce a lower energy D(s ,s˜ ) than z = 1.",
    "y y 4.5 Training a JEPA Like any EBM, a JEPA can be trained with contrastive methods.",
    "But, as pointed out above, contrastive methods tend to become very inefficient in high dimension.",
    "The relevant 25 Minimize Prediction Error Maximize Maximize Information Information Content Content Minimize Information Content Figure 13: Non-contrastive training of JEPA.",
    "The main attraction of JEPAs is that they can be trained with non-contrastive methods.",
    "The basic principle of such training is that (1) s should be maximally informative about x; (2) s should x y be maximally informative about y; (3) s should be easily predictable from s ; and (4) z should y x have minimal information content.",
    "Criteria 1, 2, and 4 collectively prevent a collapse of the energy function.",
    "Examples of such non-contrastive criteria for JEPA training include VICReg and Barlow Twins.",
    "As with every EBM, JEPAs can also be trained with contrastive methods.",
    "But doing so runs into the curse of dimensionality and limits the practical dimension of s .",
    "y dimension here is that of s , which may be considerably smaller than y, but still too high y for efficient training.",
    "What makes JEPAs particularly interesting is that we can devise non-contrastive meth- ods to train them.",
    "As explained in section 4.3, non-contrastive methods use regularizers that measure the volume of space that can take low energy values.",
    "In the case of the JEPA, this can be done through four criteria, as depicted in Figure 13: 1. maximize the information content of s about x x 2. maximize the information content of s about y y 3. make s easily predictable from s y x 4. minimize the information content of the latent variable z used in the prediction.",
    "Criteria 1 and 2 prevent the energy surface from becoming flat by informational col- lapse.",
    "They ensure that s and s carry as much information as possible about their inputs.",
    "x y",
    "Without these criteria the system could choose to make s and s constant, or weakly in-",
    "x y formative, which would make the energy constant over large swaths of the input space.",
    "Criterion 3isenforcedbytheenergytermD(s ,s˜ )andensuresthaty ispredictablefrom y y x in representation space.",
    "Criterion 4 prevents the system from falling victim to another type of informational col- lapse by forcing the model to predict s with as little help from the latent as possible.",
    "This y 26 type of collapse can be understood with the following thought experiment.",
    "Imagine that z has the same dimension as s .",
    "Assume that the predictor is a parameterized function y (e.g. a neural net) that can choose to ignore s and to simply copy z on its output s˜",
    "= z. x y",
    "For any s it is possible to set zˇ = s , which would make the energy D(s ,s˜ ) zero.",
    "This y y y y corresponds to a totally flat and collapsed energy surface.",
    "How do we prevent this collapse from happening?",
    "By limiting or minimizing the information content of the latent variable.",
    "How can this be done?",
    "By making z discrete, low-dimensional, sparse, or noisy, among other methods.",
    "A few concrete examples may help build an intuitive understanding of the phenomenon.",
    "Suppose that D(s ,s˜ ) = ||s −s˜ ||2 and that z is discrete with K possible integer values y y y y",
    "[0,K −1].",
    "For a given x, there can be only K possible values of s˜ : y Pred(s ,0),Pred(s ,1),...,Pred(s ,K −1).",
    "x x x Hence, these can be the only values of s with zero energy, and there are only K of them.",
    "y Consider a point s that starts from Pred(s ,0) and moves towards Pred(s ,1).",
    "Its energy y x x will start from zero, increase quadratically as s moves away from Pred(s ,0), until s .",
    "y x y",
    "Whens becomesclosertoPred(s ,1)thantoPred(s ,0),theenergywilldecrease,reaching y x x zero when s reaches Pred(s ,1).",
    "In representation space, the energy will be the minimum y x of K quadratic energy wells.",
    "Similarly, imagine that z is a vector whose dimension d is lower than that of s˜ .",
    "Then, y assuming that Pred(s ,z) is a smooth function of z, the set of possible predictions will be x at most a d-dimensional manifold in the space of s .",
    "y More to the point, imagine that the energy function is augmented by a regularization termonz oftheformR(z)",
    "= αPd |z |,i.e.theL normofz.",
    "Thiswilldrivezˇtobesparse.",
    "i=1",
    "i 1",
    "As with classical sparse coding, this will cause the region of low energy to be approximated by a union of low-dimensional manifolds (a union of low-dimensional linear subspaces if Pred(s ,z) is linear in z), whose dimension will be minimized by the L regularizer.",
    "x 1 Making z a stochastic sample from a distribution whose entropy is maximized will also have a proper regularization effect.",
    "This is the basis of Variational Auto-Encoders and similar models.",
    "A more complete discussion of regularizers that can minimize the information content of latent variables is beyond the scope of this paper.",
    "For now, we can mention four classes of methods: discretization/quantification (e.g. as in VQ-VAE (Walker et al., 2021), dimen- sionality/rank minimization (e.g. as in Implicit Rank-Minimizing AE (Jing et al., 2020), sparsification (as in linear sparse modeling (Olshausen and Field, 1996), LISTA (Gregor and LeCun, 2010b), and non-linear sparse modeling (Evtimova and LeCun, 2022)), and fuzzyfication (as in noisy AE (Doi et al., 2007), VAE (Kingma and Welling, 2013), and variants used in control problems (Henaff et al., 2019)).",
    "The ability of the JEPA to predict in representation space makes it considerably prefer- able to generative models that directly produce a prediction of y. In a video prediction scenario, it is essentially impossible to predict every pixel value of every future frame.",
    "The details of the texture on a carpet, the leaves of a tree moving in the wind, or the ripples on a pond, cannot be predicted accurately, at least not over long time periods and not without consuming enormous resources.",
    "A considerable advantage of JEPA is that it can choose to 27 Figure 14:",
    "Training a JEPA with VICReg.",
    "VICReg is a non sample-contrastive method for training embeddings.",
    "The information content of the representations s and s is maximized by first mapping them to higher-dimensional embeddings v x y x and v through an expander (e.g. a trainable neural net with a few layers).",
    "The loss function drives y the covariance matrix of the embeddings towards the identity (e.g. computed over a batch).",
    "VICReg can be seen as a dimension-contrastive method as opposed to sample-contrastive methods.",
    "ignore details of the inputs that are not easily predictable.",
    "Yet, Criteria 1 and 2 will ensure that the information content of the ignored details are kept to a minimum.",
    "How can we implement Criteria 1 and 2?",
    "In other words, given a parameterized deterministic encoding function s = Enc (y), how y w do we maximize the information content of s ?",
    "y",
    "If Enc (y) is invertible, s contains all the information about y, but that may be sub- w y optimal for Criterion 3, as s will contain many irrelevant or hard-to-predict details about y y.",
    "More precisely, s is maximally informative about y if the function Enc (y) is minimally y w surjective, i.e. if the volume of sets of y that map to the same s is minimal.",
    "The same y reasoning applies to the x encoder.",
    "To turn this criterion into a differentiable loss, we need to make some assumptions.",
    "4.5.1",
    "VICReg The VICReg method (Bardes et al., 2021) makes a few assumptions about the distributions ofs ands .",
    "AgraphicalrepresentationisshowninFigure14.",
    "Tomaximizetheinformation x y contentofs ,VICRegusesthefollowingtwosub-criteria: (1)thecomponentsofs mustnot x x beconstant,(2)thecomponentsofs mustbeasindependentofeachotheraspossible.",
    "This x is approximated by first non-linearly mapping s and s to higher-dimensional embeddings x y v and v through a trainable expander module (e.g. a neural net with a few layers), and x y using a loss function with two differentiable loss terms computed over a batch of samples: 28 1.",
    "Variance: a hinge loss that maintains the standard deviation of each component of s and v above a threshold over a batch.",
    "y y 2.",
    "Covariance: a covariance loss in which the covariance between pairs of different components of v are pushed towards zero.",
    "This has the effect of decorrelating the y components of v , which will in turn make the components of s somewhat indepen-",
    "y y dent.",
    "The same criteria are applied to s and v separately.",
    "x",
    "x The third criterion of VICReg is the representation prediction error D(s ,s˜ ).",
    "In the y y simplest implementations of VICReg, the predictor is constant (equal to the identity func- tion), making the representations invariant to the transformation that turns x into y. In more sophisticated versions, the predictor may have no latent variable, or may depend on a latent variable that is either discrete, low dimensional, or stochastic.",
    "The fourth criterion is necessary when the predictor uses a latent variable whose infor- mation content must be minimized, for example a vector whose dimension approaches or surpasses that of s˜ .",
    "y",
    "A simple instantiation of VICReg to learn invariant representations consists in making x and y be different views (or distorted versions) of the same content, setting the predictor to the identity function, and defining D(s ,s˜ )",
    "= D(s ,s ) = ||s −s ||2.",
    "y",
    "y y x y x Inferring the latent variable through gradient-based methods may be onerous.",
    "But the computational cost can be greatly reduced by using amortized inference, as explained in Appendix 8.3.3.",
    "While contrastive methods ensure that representations of different inputs in a batch are different, VICReg ensures that different components of representations over a batch are different.",
    "VICReg is contrastive over components, while traditional contrastive methods are contrastive over vectors, which requires a large number of contrastive samples.",
    "ButthemostpromisingaspectofJEPAtrainedwithVICRegandsimilarnon-contrastive methods is for learning hierarchical predictive world models, as we examine in the next sec- tion.",
    "4.5.2 Biasing a JEPA towards learning “useful” representations Withthetrainingcriterialistedabove, theJEPAfindsatrade-offbetweenthecompleteness and the predictability of the representations.",
    "What is predictable and what does not get represented is determined implicitly by the architectures of the encoders and predictor.",
    "They determine a inductive bias that defines what information is predictable or not.",
    "But it would be useful to have a way to bias the system towards representations that contain information relevant to a class of tasks.",
    "This can be done by adding prediction heads that take s˜ as input and are trained to predict variables that are easily derived from y the data and known to be relevant to the task.",
    "4.6 Hierarchical JEPA (H-JEPA) JEPA models trained non-contrastively may constitute our best tool for learning world models that are able to learn relevant abstractions.",
    "When trained with VICReg and similar 29 JEPA-2 JEPA-1 x y Figure 15: Hierarchical JEPA (H-JEPA)",
    "The ability of the JEPA to learn abstract representations in which accurate prediction can be per- formed allows hierarchical stacking.",
    "In this diagram JEPA-1 extracts low-level representations and performs short-term predictions.",
    "JEPA-2 takes the representations extracted by JEPA-1 as inputs and extracts higher-level representations with which longer-term predictions can be performed.",
    "More abstract representations ignore details of the inputs that are difficult to predict in the long term, enabling them to perform longer-term predictions with coarser descriptions of the world state.",
    "criteria, a JEPA can choose to train its encoders to eliminate irrelevant details of the inputs so as to make the representations more predictable.",
    "In other words, a JEPA will learn abstract representations that make the world predictable.",
    "Unpredictable details will be eliminatedbytheinvariancepropertiesoftheencoder, orwillbepushedintothepredictor’s latent variable.",
    "The amount of information thereby ignored will be minimized by the training criteria and by the latent variable regularizer.",
    "It is important to note that generative latent-variable models are not capable of elimi- nating irrelevant details, other than by pushing them into a latent variable.",
    "This is because they do not produce abstract (and invariant) representations of y.",
    "This is why we advocate against the use of generative architectures.",
    "The capacity of JEPA to learn abstractions suggests an extension of the architecture to handlepredictionatmultipletimescalesandmultiplelevelsofabstraction.",
    "Intuitively, low- level representations contain a lot of details about the input, and can be used to predict in the short term.",
    "But it may be difficult to produce accurate long-term predictions with the same level of details.",
    "Conversely high-level, abstract representation may enable long-term predictions, but at the cost of eliminating a lot of details.",
    "30 Let’stakeaconcreteexample.",
    "Whendrivingacar, givenaproposedsequenceofactions on the steering wheel and pedals over the next several seconds, drivers can accurately predict the trajectory of their car over the same period.",
    "The details of the trajectory over longer periods are harder to predict because they may depend on other cars, traffic lights, pedestrians, and other external events that are somewhat unpredictable.",
    "But the driver can still make accurate predictions at a higher level of abstraction: ignoring the details of trajectories, other cars, traffic signals, etc, the car will probably arrive at its destination within a predictable time frame.",
    "The detailed trajectory will be absent from this level of description.",
    "But the approximate trajectory, as drawn on a map, is represented.",
    "A discrete latent variable may be used to represent multiple alternative routes.",
    "Figure15showsapossiblearchitectureformultilevel, multi-scaleworldstateprediction.",
    "Variables x ,x ,x represent a sequence of observations.",
    "The first-level network, denoted 0 1 2 JEPA-1 performs short-term predictions using low-level representations.",
    "The second-level network JEPA-2 performs longer-term predictions using higher-level representations.",
    "One can envision architectures of this type with many levels, possibly using convolutional and othermodules,andusingtemporalpoolingbetweenlevelstocoarse-graintherepresentation andperformlonger-termpredictions.",
    "Trainingcanbeperformedlevel-wiseorglobally,using any non-contrastive method for JEPA.",
    "I submit that the ability to represent sequences of world states at several levels of abstraction is essential to intelligent behavior.",
    "With multi-level representations of world states and actions, a complex task can be decomposed into successively more detailed sub- tasks, instantiated into actions sequences when informed by local conditions.",
    "For example, planning a complex task, like commuting to work, can be decomposed into driving to the train station, catching a train, etc.",
    "Driving to the train station can be decomposed into walking out of the house, starting the car, and driving.",
    "Getting out of the house requires standing up, walking to the door, opening the door, etc.",
    "This decomposition descends all the way down to millisecond-by-millisecond muscle controls, which can only be instantiated when the relevant environmental conditions are perceived (obstacles, traffic lights, moving objects, etc).",
    "4.7 Hierarchical Planning If our world model can perform predictions hierarchically, can it be used to perform Mode-2 reasoning and planning hierarchically?",
    "Hierarchical planning is a difficult topic with few solutions, most of which require that theintermediatevocabularyofactionsbepredefined.",
    "Butifoneabidesbythedeeplearning philosophy, those intermediate representations of action plans should also be learned.",
    "Figure16showsapossiblearchitectureforhierarchicalMode-2planningthatcanexploit",
    "the hierarchical nature of a multi-scale world model.",
    "A percept is encoded into representations at multiple levels of abstractions by a cascade of encoders: s[0] = Enc1(x); s2[0] = Enc2(s[0]); ... (14) Predictiontakesplaceatalllevels.",
    "Higherlevelsperformlonger-termprediction,whilelower levelsperformshorter-termpredictions.",
    "Theoveralltaskisdefinedbyahigh-levelobjective, depicted as C(s [4]) in the diagram.",
    "The top level infers a sequence of high-level actions 2 31 C(s2[4])",
    "Enc2(s[0]) Pred2(s,a) Pred2(s,a) s2[0] s2[2] s2[4] a2[2] a2[4] Actor2 C(s[2]) C(s[4]) Enc1(x) Pred1(s,a) Pred1(s,a) Pred1(s,a) Pred1(s,a) s[0] s[1] s[2] s[3] s[4] action a[0] a[1] a[2] a[3] Actor1 Figure 16:",
    "Hierarchical JEPA for Mode-2 hierarchical planning.",
    "A complex task is defined by a high-level cost computed from a high-level world-state representation C(s2[4]).",
    "A sequence of high-level abstract actions (a2[2],a2[4]) is inferred that minimizes C(s2[4]).",
    "Theinferredabstractactionsarefedtolower-levelcostmodulesC(s[2]),C(s[4])whichdefinesubgoals for the lower layer.",
    "The lower layer then infers an action sequence that minimizes the subgoal costs.",
    "Althoughonlya2-layerhierarchyisshownhere,itisstraightforwardtoextendtheconcepttomultiple levels.",
    "The process described here is sequential top-down, but a better approach would be to perform a joint optimization of the actions in all the layers.",
    "(a2[2],a2[4]) to optimize this objective.",
    "These high-level “actions” are not real actions but targets for the lower level predicted states.",
    "One can think of them as conditions that the lower-levelstatemustsatisfyinorderforthehigh-levelpredictionstobeaccurate.",
    "Whether these conditions are satisfied can be computed by cost modules C(s[2]) and C(s[4]).",
    "They take a lower-level state s[2] and a high-level condition a2[2] and measure to what extent the state satisfies the condition.",
    "With these subgoals defined, the lower level can perform inference and find a low-level action sequence that minimizes the mid-level subgoals C(s[2]) and C(s[4]).",
    "The process just described is top down and greedy.",
    "But one may advantageously iterate theoptimizationsothathighlevelandlow-levelactionsequencesareoptimizedjointly.",
    "The cost modules may be configured by the configurator for the situation at hand.",
    "Theideathatanactionismerelyaconditiontobesatisfiedbythelevelbelowisactually an old one in control theory.",
    "For a example, a classical proportional servomechanism can be seen as being given a target state.",
    "A quadratic cost measures the squared distance between the target and the current state, and the control is simply proportional to the negative gradient of the cost with respect to the action variables.",
    "4.8",
    "Handling uncertainty The real world is not entirely predictable.",
    "Uncertainty in predictions of future world states may be due to a number of reasons: • the world is intrinsically stochastic (aleatoric uncertainty, type 1) 32 C(s2[4])",
    "Enc2(s[0]) Pred2(s,a,z) Pred2(s,a,z) s2[0] s2[2] s2[4] R2 z2[2] R2 z2[4] a2[2] a2[4] Actor2 C(s[2]) C(s[4]) Enc1(x) Pred1(s,a,z) Pred1(s,a,z) Pred1(s,a,z) Pred1(s,a,z) s[0] s[1] s[2] s[3] s[4] R1 z1[0] R1 z1[1] R1 z1[2] R1 z1[3] action a[0] a[1] a[2] a[3] Actor1 Figure 17: Hierarchical JEPA for Mode-2 hierarchical planning in an uncertain environment.",
    "Realistic environments are not entirely predictable, even when using highly-abstract representations.",
    "Uncertainty about predictions can be handled by predictors with latent variables.",
    "The latent variables (red circles) contain information about the prediction that cannot be derived from the prior observa- tion.",
    "The latent variables must be regularized to prevent an energy collapse and to force the system to predict as much as possible without the help of it.",
    "At planning time, latent variables are sampled from distributions obtained by applying a Gibbs dis- tribution to the regularizers.",
    "Each sample leads to a different prediction.",
    "To produce consistent latent sequences, the parameters of the regularizer can be functions of previous states and retrieved memories.",
    "As the prediction progresses, the number of generated state trajectories may grow exponentially.",
    "If each latent variable has k possible discrete values, the number of possible trajectories will grow as kt, where t is the number of time steps.",
    "Directed search and pruning strategies must be employed.",
    "With multiple predicted trajectories, optimal action sequences can be computed that minimize the average cost, or a combination of average and variance of the cost so as to minimize risk.",
    "•",
    "the world is deterministic but chaotic, hence difficult to predict without infinitely precise perception (aleatoric uncertainty, type 2) •",
    "the world is deterministic but partially observable (aleatoric uncertainty type 3).",
    "• the world is fully observable, but the sensors only give partial information about the world state (epistemic uncertainty, type 1) • the representation of the world state extracted by the perception module does not contain the full information necessary for accurate prediction (epistemic uncertainty, type 2).",
    "• theworldmodelisinaccurateduetolimitationsofitsrepresentationalpower(bounded rationality or epistemic uncertainty, type 3).",
    "33 • the world model is inaccurate due to it having been trained with limited amount of data (epistemic uncertainty, type 4).",
    "Muchoftheliteratureinreinforcementlearningisfocusedondealingwiththestochastic nature of the environment.",
    "It is often assumed from the start that models, critics and policiesmustrepresentdistributions.",
    "Inthepresentwork,wepushthepossiblestochasticity ofapredictedvariableintoalatentvariable,whichmaybeoptimized,predicted,orsampled.",
    "This is what is often referred to in the ML literature as “the reparameterization trick”.",
    "We do not need to use this trick here, since we view the latent-variable parameterization of the predictions as fundamental.",
    "Figure 17 represents a hierachical planning episode in the presence of uncertainty.",
    "A prediction at a given level and time step, e.g. s2[2] requires a sample of the corre- sponding latent variable z2[2].",
    "The sample may come from the distribution whose negative logarithm is the regularizer R2(z2[2])).",
    "The parameters of the regularizer may be constant (e.g.fixedGaussian),predictedfromcurrently-availabledatausingamortizedinference(e.g.",
    "a multinomial or Gaussian whose parameters are computed from s2[0]) or produced by the configurator.",
    "Usingpreviouspredictionstoconfigurethelatentregularizerbiasesthesystem towards generating “good” trajectories.",
    "As the prediction progresses, the number of generated state trajectories may grow ex- ponentially: if each latent variable has k possible discrete values, the number of possible trajectories will grow as kt, where t is the number of time steps.",
    "Directed search and prun- ingstrategiescanbeemployed, asinclassicalMonte-CarloTreeSearch(MCTS).Inthecase of continuous latents, one may sample latents from the continuous distributions defined by the regularizer.",
    "Given a sample of all the latents, the optimal action sequences at every levels can be inferred.",
    "However, the prediction process may need to be repeated for multiple drawings of the latents, so as to cover the set of plausible outcomes.",
    "The inference process may be used for multiple predictions to produce an action that does not just minimize the expected cost, but also minimizes the uncertainty on the expected cost.",
    "4.8.1 World Model Architecture Thedetailsofthearchitectureoftheworldmodelshoulddependonthetypeofenvironment the agent evolves in.",
    "It is likely that the best module architectures in a JEPA should include some sort of gating or dynamic routing mechanism.",
    "For example, the best way to handle low-level, short-term predictions in videos is by extracting simple local feature vectors and displacing those feature vectors from one frame to the next, depending on predicted motions.",
    "The latent variables may encode a map of displacements, which can modulate routing connections between one frame and the next.",
    "For longer-term prediction at a higher level of abstraction, the relevant features are objects and their interactions.",
    "The evolution may be best modeled by a transformer ar- chitecture, which has the property of being equivariant to permutation and is appropriate to capture interactions between discrete objects (Vaswani et al., 2017; Carion et al., 2020; Battaglia et al., 2016).",
    "34 Separating the World Model from the Ego Model: The natural world is complex and somewhat unpredictable, requiring a powerful model with latent variables to account for the unpredictability.",
    "On the other hand, the agent itself is somewhat more predictable: a particular ac- tion on effector will produce a motion that can often be predicted deterministically.",
    "This suggests that the agent should possess a separate model of itself, perhaps without latent variables (Sobal et al., 2022) as the effect of actions on proprioception somewhat easier to predict than the evolution of the external world or the effect of actions on it.",
    "In turn, the ego-model that the agent has of itself can be used as a template for a model of other agents in a multi-agent scenario.",
    "4.9 Keeping track of the state of the world Traditionally, modules in deep learning architectures communicate states through vectors or multi-dimensional arrays.",
    "But this tends to be a very inefficient method when the state of the object being modeled only changes in minor ways from one time to the next.",
    "A typical action of an agent will only modify a small portion of the state of the world.",
    "If a bottle is being moved from the kitchen to the dining room, the states of the bottle, the kitchen, and the dining room will be modified.",
    "But the rest of the world will be unaffected.",
    "This suggests that the state of the world should be maintained in some sort of writable memory.",
    "Whenever an event occurs, only the part of the world-state memory affected by the event is to be updated, while the rest is to be left unchanged.",
    "A conventional key-value associative memory can be used for this purpose, similar to whathasbeenproposedinthecontextofmemory-augmentednetworks(Bordesetal., 2015; Sukhbaatar et al., 2015; Miller et al., 2016), and entity networks (Henaff et al., 2017).",
    "Theoutputoftheworldmodelatagiventimestepisasetofquery-valuepairs(q[i],v[i]), which are used to modify existing entries in the world-state memory, or to add new entries.",
    "Given a query q, the world-state memory returns X Mem(q) = c v (15) j",
    "j j c˜ = Match(k ,q) (16) j j c = Normalize(c˜)",
    "(17) where the k are keys, the v are stored values, function Match(k,q) measures a divergence j j or dissimilarity between a key and a query, vector c contains scalar coefficients c , and j function Normalize(c˜) performs some sort of competitive normalization or thresholding, such as the commonly-used c = exp(c˜ )/[γ",
    "+P exp(c˜ )], where γ is a positive constant.",
    "j j k k Writing a value r using query (or address) q into the memory can be done by updating existing entries: c˜ = Match(k ,q) (18) j j c = Normalize(c˜) (19) v = Update(r,v ,c ) (20) j j j Function Update(r,v,c) may be as simple as cr+(1−c)v. 35",
    "If the query is distant from all keys, the memory may allocate a new entry whose key is q and corresponding value is r.",
    "The γ constant in the example Normalize function above may serve as a threshold for acceptable key-query divergence.",
    "One can view each entry as representing the state of an entity in the world.",
    "In the above example of the bottle, the world model may contain keys k ,k ,k bottle kitchen dining−room respectively representing the bottle, the kitchen and the dining room.",
    "The initial value of v encodes its location as “kitchen”, the inital value of v encodes its content as bottle kitchen includingthebottle, andtheinitialvalueofv encodesitscontentasnotincluding dining−room the bottle.",
    "After the event, the location and contents are updated.",
    "All of these operations can be done in a differentiable manner, and would hence allow to back-propagate gradients through them.",
    "4.10 Data Streams Much knowledge about the world is learnable through pure observation.",
    "The laws of mo- tion of physical objects can, in principle, be derived from observation, without a need for intervention.",
    "But training a world model efficiently may require more active or “agentive” information gathering.",
    "One can list five modes of information gathering with which an agent can learn about how the world works: 1. passive observation: the agent is being fed a sensor stream (e.g. video, audio, etc) 2. active foveation: the agent is being fed a stream within which the focus of attention can be directed without affecting the environment.",
    "For example, watching a scene while being able to orient the vision and sound sensors, or being being fed a wide- angle, high resolution video and/or audio stream within which the focus of attention can be directed.",
    "3. passive agency: sensory streams in which another agent acting on the environment is being observed, enabling the inference of causal effects of agent actions on the state of the environment.",
    "4. active egomotion: the agent receives sensory streams from a real or virtual environ- ment within which the position of the sensors can be modified without significantly affecting the environment.",
    "This may include steerable active sensors (e.g. range sen- sors, heat sensors, chemical sensors) as well as touch sensors.",
    "5. active agency: sensory streams that are influenced by the agent’s actions.",
    "This enables the establishment of causal models in which the agent can learn to predict the consequences of its actions.",
    "This mode brings the exploration-exploitation dilemma to the forefront.",
    "In a complex environment, it may not be practical to collect enough passive data for the world model to capture a sufficient portion of the environment’s behavior.",
    "Mode 2, 4, and 5 allow the agent to collect information that maximizes its understanding of the environ- ment.",
    "But to do so may require intrinsic motivation modules that drive attention, curiosity, 36 and exploration into corners of the state space in which the world model’s prediction are currently inexact or uncertain.",
    "The main open question is how much can be learned using passive observation (modes 1, 2, 4), how much requires egomotion (mode 3), and how much requires full agency (mode 5).",
    "5",
    "Designing and Training the Actor",
    "The role of the actor module is threefold: 1. inferring optimal action sequences that minimize the cost, given the predictions pro- duced by the world model for Mode-2 actions.",
    "2. producing multiple configurations of latent variables that represent the portion of the world state the agent does not know.",
    "3. training policy networks for producing Mode-1 actions.",
    "Thereisnoconceptualdifferencebetweenanactionandalatentvariable.",
    "Theconfigurations of both sets of variables must be explored by the actor.",
    "For latent variables, configurations must be explored to plan under uncertainty.",
    "For action variables configurations must be explored to produce an optimal one that minimizes the cost.",
    "In adversarial scenarios (such as games), the latent configurations must be explored that maximize the cost.",
    "In effect, the actor plays the role of an optimizer and explorer.",
    "When the world model and the cost are well-behaved, the actor module can use a gradient-basedoptimizationprocesstoinferanoptimalactionsequence.",
    "Todoso,itreceives estimates of the gradient of the cost computed by backpropagating gradients through the cost and the unfolded world model.",
    "It uses those estimates to update the action sequence.",
    "When the world model or the cost are not so well-behaved, a gradient-based search for an optimal action sequence may fail.",
    "In this case another search/planning method may be applied.",
    "If the action space is discrete or can be discretized, one can use dynamic programmingmethodsorapproximatedynamicprogrammingmethodssuchasbeamsearch or Monte-Carlo tree search.",
    "In effect, any planning method developed in the context of optimal control, robotic, or “classical” AI may be used in this context.",
    "Once an optimal action sequence is obtained through the planning / inference / opti- mization process, one can use the actions as targets to train a policy network.",
    "The policy networkmaysubsequentlybeusedtoactquickly, ormerelytoinitializetheproposedaction sequence to a good starting point before the optimization phase.",
    "Multiple policy networks can be trained for multiple tasks.",
    "The actor also produces configurations of latent variables.",
    "These latent variables repre- sent the portion of the world state that the agent does not know.",
    "Ideally, the actor would systematically explore likely configurations of the latents.",
    "Ideally, the regularizer for the latents, R1 and R2 in Figure 17, would represent log-priors from which the latent could be sampled.",
    "But in a similar way as the policy network, one may devise a latent amortized inference module that learns distributions of latent variables.",
    "Good distributions would produce predictions that are plausible.",
    "The distribution primate’s may depend on all the variables available at that time.",
    "37 6",
    "Designing the Configurator",
    "The configurator is the main controller of the agent.",
    "It takes input from all other modules and modulates their parameters and connection graphs.",
    "The modulation can route signals, activate sub-networks, focus attention, etc.",
    "In a scenario in which the predictor and the upper layers of the perception encoder are transformer blocks, the configurator outputs may constitute extra input tokens to these transformer blocks, thereby modulating their connection graphs and functions.",
    "The configurator module is necessary for two reasons: hardware reuse, and knowledge sharing.",
    "There is an obvious advantage to be able to reuse the same circuit for multiple tasks, particularly if the tasks can be accomplished sequentially, and if the amount of resources (e.g. parameter memory) is limited.",
    "But there is another advantage: knowledge reuse.",
    "Areasonablehypothesisisthataworldmodeltrainedforagivenenvironmentcanbe used for a range of different tasks with minor changes.",
    "One can imagine a “generic” world model for the environment with a small portion of the parameters being modulated by the configurator for the task at hand.",
    "This would be more data efficient and computationally efficient than having separate world models for each skill.",
    "The disadvantage is that the agent can only accomplish one task at a time.",
    "The configurator may prime the perception module for a particular task by modulating the parameters at various levels.",
    "The human perceptual system can be primed for a par- ticular task, such as detecting an item in a cluttered drawer, detecting fruits or preys in a forest, reading, counting certain events, assembling two parts, etc.",
    "For tasks that require a rapiddetectionofsimplemotifs,theconfiguratormaymodulatetheweightsoflow-levellay- ers in a convolutional architecture.",
    "For tasks that involve satisfying relationships between objects (such as assembling two parts with screws) the configuration may be performed by modulating tokens in high-level transformer modules.",
    "Thepredictorpartoftheworldmodelmustbeabletoperformawiderangeoffunctions depending on the task at hand.",
    "For predictors performing short-term predictions at a low level of abstraction, configuration may mean dynamic signal routing.",
    "In a low-level retinotopic feature array representation, prediction may be reduced to local displacements of individual feature vectors, accompanied with small transformations of those vectors.",
    "This may be advantageously implemented with local gating/routing circuits.",
    "For longer- term prediction at higher-levels of abstraction, it may be preferable to use a transformer architecture.",
    "Transformer blocks are particularly appropriate for object-based reasoning in which objects interact.",
    "The reason is that the function of transformer blocks is equivariant to permutation.",
    "Thanks to that property, one does not need to worry about which object is assigned to which input token: the result will be identical and consistent with the input assignment.",
    "Recent work in model-based robotics have proposed to use a transformer operating at the level of an entire trajectory, imposing constraints on the attention circuits to configure the predictor for causal prediction or other tasks (Janner et al., 2021).",
    "Conveniently, the function of a transformer block is easy to configure by adding extra input tokens.",
    "Those extra inputs have the effect of modulating the connection graph used bytherestofthenetwork,therebyallowingthespecificationofawiderangeofinput-output functions.",
    "38",
    "Perhaps the most important function of the configurator is to set subgoals for the agent and to configure the cost module for this subgoal.",
    "As mentioned in Section 3.2, a simple way to make the cost configurable is by modulating the weights of a linear combination of elementary cost sub-modules.",
    "This may be appropriate for the immutable Intrinsic Cost submodule: allowing for a complex modulation of the Intrinsic Cost may make the basic drivesoftheagentdifficulttocontrol,includingcosttermsthatimplementsafetyguardrails.",
    "In contrast, one can imagine more sophisticated architectures allowing the Trainable Critic part of the cost to be flexibly modulated.",
    "As with the predictor, if the high-level cost is formulated as a set of desired relationships between objects (“is the nut set on the screw?”) one may use a transformer architecture trained to measure to what extent the state of the world diverges from the condition to be satisfied.",
    "As with the predictor, extra token inputs can be used to modulate the function.",
    "One question that is left unanswered is how the configurator can learn to decompose a complex task into a sequence of subgoals that can individually be accomplished by the agent.",
    "I shall leave this question open for future investigation.",
    "7 Related Work Most of the ideas presented in the paper are not new, and have been discussed at length in various forms in cognitive science, neuroscience, optimal control, robotics, AI, and machine learning, particularly in reinforcement learning.",
    "Perhaps the main original contributions of the paper reside in • an overall cognitive architecture in which all modules are differentiable and many of them are trainable.",
    "• H-JEPA: a non-generative hierarchical architecture for predictive world models that learn representations at multiple levels of abstraction and multiple time scales.",
    "• a family of non-contrastive self-supervised learning paradigm that produces represen- tations that are simultaneously informative and predictable.",
    "•",
    "AwaytouseH-JEPAasthebasisofpredictiveworldmodelsforhierarchicalplanning under uncertainty.",
    "Below is an attempt to connect the present proposal with relevant prior work.",
    "Given the scope of the proposal, the references cannot possibly be exhaustive.",
    "7.1 TrainedWorldModels, Model-PredictiveControl, HierarchicalPLan- ning The use of models in optimal control goes back to the early days with the Kelley-Bryson method (see (Bryson and Ho, 1969) and reference therein, or the review (Morari and Lee, 1997)).",
    "Some methods allowed for online system identification (Richalet et al., 1978).",
    "Using neural networks to learn models for control is an old idea, going back to the early 1990s(JordanandRumelhart,1992; NarendraandParthasarathy,1990; Milleretal.,1995).",
    "39",
    "In the context of optimal control, learning a Mode-1 like policy network is known as direct inverse control.",
    "The idea of Mode-2 style inference over actions using a predictive model in the context of reinforcement learning is also an old idea with, for example, Sutton’s Dyna architec- ture (Sutton, 1991).",
    "See (Bertsekas, 2019) for an extensive review.",
    "The idea of learnable models has recently enjoyed a renewal of interest in various con- texts(HaandSchmidhuber,2018b; HaandSchmidhuber,2018a; Hafneretal.,2018; Hafner et al., 2020) (see (Moerland et al., 2020) for a recent survey of model-based reinforcement learning).",
    "Learning world models is particularly important in the context of robotics, especially for grasping and manipulation where sample efficiency is paramount and simulation is often inaccurate.",
    "In fact, because classical reinforcement learning approaches require too many trials for real-world applications, interesting advances in learned model for control have emerged from ML-based robotics research (Agrawal et al., 2016; Finn and Levine, 2017; Chua et al., 2018; Srinivas et al., 2018; Yu et al., 2020; Yarats et al., 2021).",
    "For a recent review, see (Levine, 2021) and references therein.",
    "A difficult setting is one in which the main input is visual, and a world model must be learned from video.",
    "Early attempts to train predictive models without latent variables from simple video produced blurry predictions (Lerer et al., 2016).",
    "To handle uncertainty in the predictions, one can use various flavors of latent variable models such as generative adver- sarial networks (GAN) (Goodfellow et al., 2014), variational auto-encoders (VAE) (Kingma and Welling, 2013) vector-quantized VAE (VQ-VAE) (van den Oord et al., 2017).",
    "Vari- ations of these methods have been applied to video prediction and help represent multi- modal outputs and reduce blurriness using GAN (Mathieu et al., 2015; Luc et al., 2020), VAE (Babaeizadeh et al., 2017; Denton and Fergus, 2018; Henaff et al., 2019), or VQ- VAE (Walker et al., 2021).",
    "Although many of these methods have not been applied to control problems, some have been applied to vehicle trajectory prediction for autonomous driving (Henaff et al., 2019; Mercat et al., 2020), or various robot control tasks (Oh et al., 2015; Fragkiadaki et al., 2015; Agrawal et al., 2016; Finn et al., 2016; Nagabandi et al., 2017; Babaeizadeh et al., 2017; Srinivas et al., 2018).",
    "Unlike the proposed JEPA, these models are generative.",
    "The key issue of how to represent uncertainty in the prediction remains.",
    "The alternative to regularized latent-variable models is constrative methods, such as Contrastive Predictive Coding (CPC) (H´enaff et al., 2019), which has been applied to learning visual representations through video prediction (van den Oord et al., 2018).",
    "To solve the multi-modality/blurriness problem, other works have proposed to perform video prediction in representations spaces.",
    "In some works, the representation space is obtained from a vision pipeline that has been trained in supervised mode, for example to perform semantic segmentation (Luc et al., 2017; Luc et al., 2018).",
    "Unfortunately, the requirement for a pre-trained vision pipeline reduces the general usability of these methods for learning world models by observation.",
    "In the same spirit as JEPA, there have been proposals for automatically learning rep- resentations of video frames so they can be easily predicted.",
    "These proposals are generally limited to learning low-level features, and often use reconstruction through a decoder as a waytopreventcollapse (Goroshinetal., 2015a; Srivastavaetal., 2015).",
    "Someauthorshave 40 proposed to use temporal invariance (or consistency) to separate the content of an image region from its instantiation parameters (Wiskott and Sejnowski, 2002; Gregor and LeCun, 2010a; Goroshin et al., 2015b).",
    "At least one recent work has applied non-contrastive SSL methods to a joint embedding architecture for robotics control with some success (Pari et al., 2021; ?).",
    "Contrastive methods applied to joint-embedding and prediction have been applied suc- cessfullytospeechrecognition(Baevskietal.,2020)(see(Mohamedetal.",
    ",2022)forarecent review of SSL to speech).",
    "To perform state trajectory predictions, recent works have advocated the use of trans- formers, asproposedinthepresentpaper.",
    "Transformersareidealtorepresentthedynamics of discrete objects in interaction, and have successfully been applied to the prediction of car trajectories (Mercat et al., 2020).",
    "Aninterestingproposalisthetrajectorytransformerarchitectureinwhichatransformer is fed with the sequence of predicted states over an entire episode (Janner et al., 2021).",
    "The pattern of attention can be constrained so as to force the system to only attend to the past so it can be operated in a causal manner (without looking at the future), and trained to predict the next state, actions, and cost from previously observed or predicted states, actions, and costs.",
    "Hierarchical planning is a largely unsolved problem.",
    "Wayne and Abbott proposed an architecture that uses a stack of trained forward models that specify intermediate goals for the lower layers (Wayne and Abbott, 2014).",
    "Some recent works specify intermediate goals for robots in terms of pose parameters (Gehring et al., 2021).",
    "A more recent proposal is the Director system (Hafner et al., 2022) which contains a hierarchical world model and planning architecture trained end-to-end through reinforcement learning.",
    "The idea of intrinsic motivation to train an agent has been studied in the context of robotics (Gottlieb et al., 2013).",
    "The presence of an Intrinsic Cost provides a differentiable and efficient way to direct the agent to follow certain behaviors and to learn certain skills.",
    "7.2 Energy-Based Models and Joint-Embedding Architectures For many authors, Energy-Based Model (EBM) designates a probabilistic model whose distribution is the normalized negative exponential of an energy function.",
    "In this paper, EBM designates a much broader category of models that treat the energy function as fundamental, and directly manipulate its landscape through learning.",
    "Many methods have been proposed in the past that directly manipulate the energy.",
    "In fact, all traditional optimization-based learning methods can be interpreted as energy-based meth- ods (LeCun et al., 2006).",
    "In particular, discriminative training methods for structure pre- diction problems can be formulated as EBM (LeCun et al., 1998; LeCun et al., 2006).",
    "Most EBM approaches for unsupervised or self-supervised learning have been con- trastive.",
    "The earliest example is the Boltzmann Machine (Hinton and Sejnowski, 1983), which is a probabilistic generative energy-based model trained contrastively.",
    "Joint Embedding Architectures (JEA) trained with contrastive methods and mutual information maximization methods have a long history.",
    "The first non-contrastive JEA was (Becker and Hinton, 1992) which was based on maximizing a measure of mutual in- formation between the representations from two branches seeing to different views of the 41 same scene.",
    "Perhaps the first contrastive method for JEA is the so-called “Siamese Net- work” (Bromley et al., 1994).",
    "This was trained contrastively for the purpose of verifying signatures handwritten on a pen tablet.",
    "The idea of JEA remained largely untouched for over a decade, until it was revived in a series of papers from my group (Chopra et al., 2005; Hadsell et al., 2006), and Geoffrey Hinton’s group (Goldberger et al., 2005).",
    "Following the rebirth of deep learning, a few papers used JEA for fine-grained recognition, including face recognition (Taigman et al., 2014).",
    "With the emergence of SSL approaches, the use of JEA trained contrastively has ex- ploded in the last few years with methods such as PIRL (Misra and Maaten, 2020), MoCo and MoCo-v2 (He et al., 2020; Chen et al., 2020b), and SimCLR (Chen et al., 2020a).",
    "Some methods can be seen as “distillation” approaches in which one branch of the Siamese network is a teacher whose output are used as targets for the other branch.",
    "This includes methods in which the output vectors are quantized to discrete cluster prototypes (see (Caron et al., 2020) and predecessors).",
    "In recent years, a number of new non-contrastive methods have appeared, such as BYOL (Grill et al., 2020).",
    "But the class of non-contrastive methods advocated in the present proposal prevent collapse by maximizing the information content of the embed- dings.",
    "This includes Barlow Twins (Zbontar et al., 2021), VICReg (Bardes et al., 2021), whitening-based methods (Ermolov et al., 2021), and Maximum Coding Rate Reduction methods (see (Dai et al., 2022) and references therein).",
    "7.3 Human and animal cognition The limitations of current approaches to machine learning when compared with human learning are obvious (Lake et al., 2017a; Zaadnoordijk et al., 2022).",
    "Young children quickly learn abstract concepts (Murphy, 2002), and models that allow themtonavigate,toformgoals,andtoplancomplexactionsequencestofulfillthem(Gopnik and Meltzoff, 1997; Spelke and Kinzler, 2007; Carey, 2009; Gopnik et al., 2001).",
    "In cognitive science, the idea that the brain builds predictive world models is a common one, and have inspired attempt to reproduce the process in machines (Lake et al., 2017b; Orhan et al., 2020).",
    "Some efforts have been devoted to building video datasets to test intuitive physics common sense in machines and infants (Riochet et al., 2019).",
    "The ability to plan is a well-studied feature of human intelligence (Mattar and Lengyel, 2022).",
    "There is evidence that people construct simplified representations of the world for planning in which irrelevant details are abstracted away (Ho et al., 2022)",
    "Consciousness is a rather speculative topic, owing to the difficulty of defining what con- sciousness is.",
    "I will not speculate about whether some version of the proposed architecture could possess a property assimilable to consciousness, but will only mention the work of Dehaene and collaborators who have proposed two types of consciousness that they call C1 and C2.",
    "C1 is largely related with the modulation of attention, while C2 requires a self-monitoring ability, perhaps assimilable to what the configurator module needs to do in the present proposal (Dehaene et al., 2021).",
    "42 8 Discussion, Limitations, Broader Relevance Constructingthe cognitivearchitectureofthe presentproposal, instantiating allthe details, and making the system work for non-trivial tasks will not be an easy task.",
    "The path to success is likely riddled with unforeseen obstacles.",
    "It will probably take many years to work them all out.",
    "8.1",
    "What is missing from the Proposed Model?",
    "A lot of hard work needs to be done to instantiate the proposed architecture and turn it into a functional system.",
    "There may be flaws and pitfalls that may appear to be unsolvable within the specifications of the proposed architecture.",
    "The first question is whether a Hierarchical JEPA can be built and trained from videos.",
    "Could it learn the type of abstract concept hierarchy mentioned in section 4.1?",
    "One somewhat open question relative to the JEPA is how precisely to regularize the latent variable so as to minimize its information content.",
    "A number of possible mechanisms are proposed: making the latent variable discrete, low-dimensional, sparse, or stochastic.",
    "But it is not clear which approach will ultimately be the best.",
    "The current proposal does not prescribe a particular way for the actor to infer latent variable instantiations and optimal action sequences.",
    "While the differentiability of all the modules makes it possible in principle to use gradient-based optimization to infer optimal action sequences, the optimization problem may be very difficult in practice.",
    "In particular, when the action space is discrete, or when the function from actions to cost is highly non smooth, gradient-based method may be ineffective, requiring to use other (gradient-free) search methods (dynamic programming, belief propagation, MCTS, SAT, etc).",
    "Instantiating multiple configurations of latent variables in Mode-2 planning/reasoning may require additional mechanisms not described in the present proposal.",
    "Humans seem to be endowed with an ability to spontaneously cycle through alternative interpretations of a percept, as demonstrated by the Necker cube and other visual illusions that have several equally-plausible interpretations.",
    "In the context of the present model, different interpretation of an ambiguous percept may be represented by different values of a latent variable.",
    "While one could imagine a number of exploratory mechanisms to systematically explore the space of possible latent variable values, no such mechanism is described here.",
    "The present proposal does not specify the details of the architecture of the various modules.",
    "For example, it is probable that the predictor will require some sort of dynamic routing and gating circuits in its micro-architecture.",
    "Predictors for low-level representation may have to be specialized to represent the kind of small transformations of the repre- sentation that can occur in the short term.",
    "Predictor modules dealing with higher level representations may require more generic architectures that manipulate objects and their relationships.",
    "But none of this is specified in the present proposal.",
    "Similarly, the precise architecture and function of the short-term memory and how it may be used to represent beliefs about the state of the world are somewhat fuzzy.",
    "The original Memory Network system and its successors contained the idea that a neural net could use an associative memory as a working memory to store and retrieve beliefs about the state of the world between compute cycles (Bordes et al., 2015; Sukhbaatar et al., 43 2015).",
    "But getting such an architecture to work for complex planning and control may prove difficult.",
    "Ofalltheleastunderstoodaspectsofthecurrentproposal,theconfiguratormoduleisthe most mysterious.",
    "In particular, while planning a complex task, the configurator is supposed to identify sequences of subgoals and configure the agent to successively accomplish those subgoals.",
    "Precisely how to do that is not specified.",
    "This is merely a list of foreseeable questions, but many questions and problems will inevitably surface as instances of the proposed systems are put together.",
    "8.2 Broader Relevance of the Proposed Approach Although the proposed architecture is not specifically designed to model autonomous intel- ligence, reasoning, and learning in humans and other animals, one can draw some parallels.",
    "The following is somewhat speculative and provided as a way to connect some concepts in cognitive science and neuroscience that have inspired the present work.",
    "8.2.1 Could this Architecture be the Basis of a Model of Animal Intelligence?",
    "Many of the modules in the proposed architecture have counterparts in the mammalian brain that perform similar functions.",
    "The perception module corresponds to the visual, auditory, and other sensory areas of the cortex, as well as some of the association areas.",
    "The world model and the critic correspond to various part of the prefrontal cortex.",
    "The intrinsic cost module corresponds tostructuresinthebasalgangliainvolvedinrewards,includingtheamygdala.",
    "Thetrainable critic may correspond to part of the prefrontal cortex involved in reward prediction.",
    "The function of the short-term memory overlaps with what is known of the hippocampus.",
    "The configurator may correspond to structures in the prefrontal cortex that perform executive control and modulate attention.",
    "The actor regroups areas in the pre-motor cortex that elaborate and encode motor plans.",
    "The idea of predictive world model has long been a prominent concept in cognitive science,andtheideaofpredictivecodinghasbeenaprominentconceptinneuroscience.",
    "The JEPA architecture and the corresponding non-sample-contrastive self-supervised learning method are somewhat consistent with ideas of predictive coding and efficient coding.",
    "The proposed architecture has a single world model engine that can be configured for the task at hand by the configurator.",
    "I have argued that this may not only confer a computational advantage through hardware reuse, but also allow knowledge to be shared across multiple tasks.",
    "The hypothesis of a single, configurable world model engine in the humanbrainmayexplainwhyhumanscanessentiallyperformasingle“conscious”reasoning and planning task at a time.",
    "A highly-speculative idea is that the illusion of consciouness may be a side-effect of a configurator-like module in the brain that oversees the function of the rest of brain and configures it for the task at hand.",
    "Perhaps if the brain were large enough to contain many independent, non-configurable world models, a configurator would be unnecessary, and the illusion of consciousness would disappear.",
    "Whatisthesubstrateofemotionsinanimalsandhumans?",
    "Instantaneousemotions(e.g.",
    "pain, pleasure, hunger, etc) may be the result of brain structures that play a role similar to the Intrinsic Cost module in the proposed architecture.",
    "Other emotions such as fear or 44 elation may be the result of anticipation of outcome by brain structures whose function is similar to the Trainable Critic.",
    "The presence of a cost module that drives the behavior of the agent by searching for optimal actions suggests that autonomous intelligent agents of the type proposed here will inevitably possess the equivalent of emotions.",
    "In an analogous way to animal and humans, machine emotions will be the product of an intrinsic cost, or the anticipation of outcomes from a trainable critic.",
    "8.2.2",
    "Could this be a Path towards Machine Common Sense?",
    "It is a widely-held opinion that none of the current AI systems possess any level of common sense, even at the level that can be observed in a house cat.",
    "Animals seem to be able to acquire enough background knowledge about how the world works to exhibit some level of common sense.",
    "By contrast, AI systems, even when (pre-)trained with self-supervised mode (e.g. from text) seem to exhibit very limited levels of common sense, making them somewhat brittle.",
    "Forexample,Largelanguagemodels(LLMs)seemtopossessasurprisinglylargeamount of background knowledge extracted from written text.",
    "But much of human common-sense knowledge is not represented in any text and results from our interaction with the physical world.",
    "Because LLMs have no direct experience with an underlying reality, the type of common-sense knowledge they exhibit is very shallow and can be disconnected from reality.",
    "A possible characterization of common sense is the ability to use models of the world to fill in blanks, for example predicting the future, or more generally filling in information about the world that is unavailable from perception or from memory.",
    "With this definition, common sense is an ability that emerges from a collection of models of the world or from a single model engine configurable to handle the situation at hand.",
    "This view of common sense sits squarely in the camp of “grounded intelligence”: common sense is a collection of models from low-levels of abstraction to high levels, all the way up to knowledge acquired through language.",
    "CouldSSLappliedtoconfigurableH-JEPAconstitutethesubstrateofmachinecommon sense?",
    "Could a properly-trained and configured H-JEPA embed enough predictive knowl- edge and capture enough dependencies about the world to exhibit some level of common sense?",
    "Ispeculatethatcommonsensemayemergefromlearningworldmodelsthatcapturethe self-consistency and mutual dependencies of observations in the world, allowing an agent to fill in missing information and detect violations of its world model.",
    "8.3",
    "Is it all about scaling?",
    "Is reward really enough?",
    "The section reviews a few potential paths towards human-level intelligence that have been proposed in recent years.",
    "The surprising power of large transformer architectures trained to predict text and other modalities have led some to claim that we merely need to scale up thosemodels(Brownetal.,2020;Brownetal.,2020).",
    "Thesurprisingpowerofreinforcement learning for games and other simple environments have led other to claim that reward is enough (Silver et al., 2021).",
    "finally, the limitations of current deep-learning systems when it 45 comestoreasoninghaveledsometoclaimthatdeeplearningsystemsneedtobeaugmented by hard-wired circuitry to enable symbol manipulation (Marcus and Davis, 2019)",
    "8.3.1 Scaling is not enough Large Language Models (LLMs), and more generally, large-scale transformer architectures trainedwithaformofgenerativeself-supervisedlearning,havebeenastonishinglysuccessful at capturing knowledge present in text.",
    "This has led to a debate in the AI community as to whether human-level AI can be attained by scaling up these architectures.",
    "My position in this debate is that I do not believe that scaling is enough for two main reasons.",
    "First, current models operate on “tokenized” data and are generative.",
    "Every input modality must be turned into a sequence (or a collection) of “tokens” encoded as vectors.",
    "While this works well for text, which is already a sequence of discrete tokens, it is less suitable for continuous, high dimensional signals such as video.",
    "The type of SSL training used for LLM-style models can be seen as a particular kind of latent-free generative model trainedwithaparticularkindofcontrastivemethodcalleddenoisingauto-encoder(Vincent et al., 2010), or in this case masked auto-encoder (Devlin et al., 2018).",
    "Hence, they are subject to the limitations of generative models, latent-variable free models, and contrastive methods.",
    "Generative models have difficulty representing complex uncertainties in contin- uous spaces.",
    "LLMs simplify the representation of uncertainty in the prediction by only dealing with discrete objects from a finite collection (e.g. words from a dictionary).",
    "Repre-",
    "senting uncertainty about a word being predicted comes down to producing a vector whose components are scores or probabilities for each word (or discrete token) in the dictionary.",
    "But this approach doesn’t work for high-dimensional continuous modalities, such as video.",
    "Torepresentsuchdata, itisnecessarytoeliminateirrelevantinformationaboutthevariable to be modeled through an encoder, as in the JEPA.",
    "Furthermore, the high-dimensionality of the signal precludes the representation of uncertainty through a normalized distribution.",
    "Second, currentmodelsareonlycapableofverylimitedformsofreasoning.",
    "Theabsence of abstract latent variables in these models precludes the exploration of multiple interpre- tations of a percept and the search for optimal courses of action to achieve a goal.",
    "In fact, dynamically specifying a goal in such models is essentially impossible.",
    "8.3.2",
    "Reward is not enough The proposed architecture is designed to minimize the number of actions a system needs to take in the real world to learn a task.",
    "It does so by learning a world model that capture as much knowledge about the world as possible without taking actions in the world.",
    "It uses intrinsic costs that are differentiable functions of measured or predicted world states.",
    "This makes the proposal more similar to optimal control than to reinforcement learning.",
    "In the proposed model, much of learning takes place at the level of the world model (perceptual encoder and predictor).",
    "In this context, what is the role of reinforcement learning (RL)?",
    "In most RL settings the reward (or the cost, which is a negative reward) is fed to the agent by the environment.",
    "In other words, Intrinsic Cost module is the environment itself, andisthereforeanunknownfunction.",
    "Thevalueofthefunctioncanbeprobedbyobserving the state of the world, taking an action, and observing the resulting reward.",
    "The gradient of the reward with respect to the action or the state is unknown and must be estimated by 46 multiple action trials as in policy gradient methods.",
    "In Actor-Critic methods, the reward function is approximated by a critic module that is trained to approximate expected future values of the reward.",
    "The critic provides a differentiable approximation of the reward function.",
    "But model-free RL is extremely sample-inefficient, at least when compared with human and animal learning, requiring very large numbers of trials to learn a skill.",
    "Scalar rewards providelow-informationfeedbacktoalearningsystem.",
    "Asaconsequence, apureRLsystem requires a very large number of trials to learn even relatively simple tasks.",
    "Model-based RL clearly has the potential of being considerably more sample efficient.",
    "But the question becomeshowtotraintheworldmodel: isittrainedfromtakingactionsandgettingrewards, orisittrainedbypredictingtheworldstate?",
    "Inthelattercase,reward is clearly not enough: most of the parameters in the systems are trained to predict large amounts of observations in the world.",
    "Contrary to the title of a recent position paper by Silver et al.",
    "(Silver et al., 2021), the reward plays a relatively minor role in this scenario.",
    "8.3.3 Do We Need Symbols for Reasoning?",
    "In the proposed architecture, reasoning comes down to energy minimization or constraint satisfaction by the actor using various search methods to find a suitable combination of actions and latent variables, as stated in Section 3.1.4.",
    "If the actions and latent variables are continuous, and if the predictor and the cost modules are differentiable and relatively well behaved, one can use gradient-based methods to perform the search.",
    "But there may be situations where the predictor output changes quickly as a function of the action, and where the action space is essentially discontinuous.",
    "This is likely to occur at high levels of abstractions where choices are more likely to be qualitative.",
    "A high-level decision for a self-driving car may correspond to “turning left or right at the fork”, while the low-level version would be a sequence of wheel angles.",
    "If the action space is discrete with low cardinality, the actor may use exhaustive search methods.",
    "If the action set cardinality, and hence the branching factor, are too large, the actor may have to resort to heuristic search methods, including Monte-Carlo Tree Search, or other gradient-free methods.",
    "If the cost function satisfied Bellman’s equations, one may use dynamic programming.",
    "But the efficiency advantage of gradient-based search methods over gradient-free search methods motivates us to find ways for the world-model training procedure to find hierar- chical representations with which the planning/reasoning problem constitutes a continuous relaxation of an otherwise discrete problem.",
    "A remain question is whether the type of reasoning proposed here can encompass all forms of reasoning that humans and animals are capable of.",
    "Acknowledgments Ideas in this paper are a distillation of years of interactions with many people.",
    "It is impos- sible to list them all here.",
    "I will list some of the more salient ones: L´eon Bottou, Yoshua Bengio, Geoffrey Hinton, RobFergus,andJitendraMalick;EmmanuelDupouxandStanislasDehaene;MikaelHenaff, 47 Alfredo Canziani, and Nicolas Carion at NYU; Li Jing, Yubei Chen, Randall Balestriero, and St´ephane Deny at FAIR; Vlad Sobal, Jiachen Zhu, and Katrina Evtimova at NYU.",
    "I am thankful to the following colleagues for their comments on the manuscript: Olivier Delalleau, Gus Xia, Yoshua Bengio, and Emmanuel Dupoux.",
    "48 References Agrawal, P., Nair, A., Abbeel, P., Malik, J., and Levine, S. (2016).",
    "Learning to poke by poking: Experiential learning of intuitive physics.",
    "CoRR, abs/1606.07419.",
    "Babaeizadeh, M., Finn, C., Erhan, D., Campbell, R. H., and Levine, S. (2017).",
    "Stochastic variational video prediction.",
    "CoRR, abs/1710.11252.",
    "Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. (2020).",
    "wav2vec 2.0: A framework for self-supervisedlearningofspeechrepresentations.",
    "InLarochelle, H., Ranzato, M., Had- sell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 12449–12460.",
    "Curran Associates, Inc. Bardes, A., Ponce, J., and LeCun, Y. (2021).",
    "Vicreg: Variance-invariance-covariance regu- larization for self-supervised learning.",
    "In International Conference on Learning Repre-",
    "sentations",
    "(ICLR 2022).",
    "arXiv preprint arXiv:2105.04906.",
    "Battaglia, P., Pascanu, R., Lai, M., Jimenez Rezende, D., et al. (2016).",
    "Interaction net- worksforlearningaboutobjects, relationsandphysics.",
    "Advances in neural information processing systems, 29.",
    "Becker, S. and Hinton, G. E. (1992).",
    "Self-organizing neural network that discovers surfaces in random-dot stereograms.",
    "Nature, 355(6356):161–163.",
    "Bertsekas, D. (2019).",
    "Reinforcement learning and optimal control.",
    "Athena Scientific.",
    "Bordes, A., Usunier, N., Chopra, S., and Weston, J. (2015).",
    "Large-scale simple question answering with memory networks.",
    "arXiv:1506.02075.",
    "Bromley,J.,Guyon,I.,LeCun,Y.,Sackinger,E.,andShah,R.(1994).",
    "Signatureverification using a “siamese” time delay neural network.",
    "In NeurIPS.",
    "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020).",
    "Language models are few- shot learners.",
    "In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 1877– 1901.",
    "Curran Associates, Inc.",
    "Bryson, A. and Ho, Y. (1969).",
    "Applied optimal control.",
    "Blaisdell, Waltham, MA.",
    "Carey, S. (2009).",
    "The Origin of Concepts.",
    "Oxford University Press, New York, New York, USA.",
    "Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. (2020).",
    "End-to-endobjectdetectionwithtransformers.",
    "In16th European Conference, Glasgow, UK (ECCV 2020), page 213–229.",
    "49",
    "Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A. (2020).",
    "Unsu- pervised learning of visual features by contrasting cluster assignments.",
    "In Advances in Neural Information Processing Systems.",
    "Carreira-Perpin˜´an, M. A. and Hinton, G. (2005).",
    "On contrastive divergence learning.",
    "In Cowell, R. G. and Ghahramani, Z., editors, Proceedings of the Tenth International WorkshoponArtificialIntelligenceandStatistics,volumeR5ofProceedingsofMachine Learning Research, pages 33–40.",
    "PMLR.",
    "Reissued by PMLR on 30 March 2021.",
    "Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. (2020a).",
    "Big self- supervised models are strong semi-supervised learners.",
    "In NeurIPS.",
    "Chen, X., Fan, H., Girshick, R., and He, K. (2020b).",
    "Improved baselines with momentum contrastive learning.",
    "arXiv preprint arXiv:2003.04297.",
    "Chopra,S.,Hadsell,R.,andLeCun,Y.(2005).",
    "Learningasimilaritymetricdiscriminatively, with application to face verification.",
    "In 2005 IEEE Computer Society Conference on ComputerVisionandPatternRecognition(CVPR’05),volume1,pages539–546.IEEE.",
    "Chua, K., Calandra, R., McAllister, R., and Levine, S. (2018).",
    "Deep reinforcement learning in a handful of trials using probabilistic dynamics models.",
    "CoRR, abs/1805.12114.",
    "Craik, K. J. W. (1943).",
    "The nature of explanation.",
    "University Press, Macmillan.",
    "Dai, X., Tong, S., Li, M., Wu, Z., Psenka, M., Chan, K. H. R., Zhai, P., Yu, Y., Yuan, X., Shum, H.-Y., and Ma, Y. (2022).",
    "Ctrl: Closed-loop transcription to an ldr via minimaxing rate reduction.",
    "Entropy, 24(4):456.",
    "Dehaene, S., Lau, H., and Kouider, S. (2021).",
    "What is consciousness, and could machines have it?",
    "Robotics, AI, and Humanity, pages 43–56.",
    "Denton, E. and Fergus, R. (2018).",
    "Stochastic video generation with a learned prior.",
    "arXiv preprint arXiv 1802.07687.",
    "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018).",
    "Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv",
    "preprint arXiv:1810.04805.",
    "Doi, E., Balcan, D. C., and Lewicki, M. S. (2007).",
    "Robust coding over noisy overcomplete channels.",
    "IEEE Transactions on Image Processing, 16(2):442–452.",
    "Ermolov,A.,Siarohin,A.,Sangineto,E.,andSebe,N.(2021).",
    "Whiteningforself-supervised representation learning.",
    "Evtimova, K.andLeCun, Y.(2022).",
    "Sparsecodingwithmulti-layerdecodersusingvariance regularization.",
    "arXiv:2112.09214.",
    "Finn, C., Goodfellow, I. J., and Levine, S. (2016).",
    "Unsupervised learning for physical interaction through video prediction.",
    "CoRR, abs/1605.07157.",
    "50",
    "Finn, C. and Levine, S. (2017).",
    "Deep visual foresight for planning robot motion.",
    "In 2017 IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages2786–2793.",
    "IEEE.",
    "Fragkiadaki, K., Agrawal, P., Levine, S., and Malik, J. (2015).",
    "Learning visual predictive models of physics for playing billiards.",
    "CoRR, abs/1511.07404.",
    "Gehring,J.,Synnaeve,G.,Krause,A.,andUsunier,N.(2021).Hierarchicalskillsforefficient exploration.",
    "Advances in Neural Information Processing Systems, 34:11553–11564.",
    "Goldberger, J., S.Roweis, Hinton, G., and Salakhutdinov, R. (2005).",
    "Neighbourhood com- ponentsanalysis.",
    "InSaul, L.K., Weiss, Y., andBottou, L., editors, Advances in Neural Information Processing Systems 17, pages 513–520.",
    "MIT Press, Cambridge, MA.",
    "Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,Courville, A., and Bengio, Y. (2014).",
    "Generative adversarial nets.",
    "In Advances in Neural Infor- mation Processing Systems, pages 2672–2680.",
    "Gopnik, A. and Meltzoff, A. N. (1997).",
    "Words, Thoughts, and Theories.",
    "MIT Press, Cambridge, MA.",
    "Gopnik, A., Meltzoff, A. N., and Kuhl, P. K. (2001).",
    "The Scientist in the Crib: What Early Learning Tells Us About the Mind.",
    "Perennial, New York, NY.",
    "Goroshin, R., Bruna, J., Tompson, J., Eigen, D., and LeCun, Y. (2015a).",
    "Unsupervised feature learning from temporal data.",
    "In International Conference on Computer Vision (ICCV 2015).",
    "Goroshin,R.,Mathieu,M.,andLeCun,Y.(2015b).",
    "Learningtolinearizeunderuncertainty.",
    "In Advances in Neural Information Processing Systems (NIPS 2015), volume 28.",
    "Gottlieb, J., Oudeyer, P. Y., Lopes, M., and Baranes, A. (2013).",
    "Information-seeking, curiosity, and attention: Computational and neural mechanisms.",
    "Trends in Cognitive Sciences, 17:585–593.",
    "Gregor, K. and LeCun, Y. (2010a).",
    "Emergence of complex-like cells in a temporal product network with local receptive fields.",
    "arXiv preprint arXiv:1006.0448.",
    "Gregor, K. and LeCun, Y. (2010b).",
    "Learning fast approximations of sparse coding.",
    "In Proc.",
    "International Conference on Machine learning (ICML’10).",
    "Grill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond, P. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo, Z. D., Azar, M. G., Piot, B., Kavukcuoglu, K., Munos, R., and Valko, M. (2020).",
    "Bootstrap your own latent: A new approach to self-supervised learning.",
    "In NeurIPS.",
    "Ha, D. and Schmidhuber, J. (2018a).",
    "Recurrent world models facilitate policy evolution.",
    "In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 31. 51",
    "Ha, D. and Schmidhuber, J. (2018b).",
    "World models.",
    "arXiv preprint arXiv:1803.10122.",
    "Hadsell, R., Chopra, S., and LeCun, Y. (2006).",
    "Dimensionality reduction by learning an invariant mapping.",
    "In CVPR.",
    "Hafner, D., Lee, K.-H., Fischer, I., and Abbeel, P. (2022).",
    "Deep hierarchical planning from pixels. arXiv preprint arXiv:2206.04114.",
    "Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. (2018).",
    "Learning latent dynamics for planning from pixels. arXiv",
    "1811.04551.",
    "Hafner, D., Lillicrap, T., Norouzi, M., and Ba, J. (2020).",
    "Mastering atari with discrete world models.",
    "arXiv preprint arXiv:2010.02193.",
    "He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. (2020).",
    "Momentum contrast for unsu- pervised visual representation learning.",
    "In CVPR.",
    "Henaff, M., Canziani, A., and LeCun, Y. (2019).",
    "Model-predictive policy learning with uncertainty regularization for driving in dense traffic.",
    "In ICLR-19.",
    "arXiv:1901.02705.",
    "Henaff, M., Weston, J., Szlam, A., Bordes, A., and LeCun, Y. (2017).",
    "Tracking the world state with recurrent entity networks.",
    "In International Conference on Learning Repre-",
    "sentations",
    "(ICLR 2017).",
    "Hinton, G. and Sejnowski, T. (1983).",
    "Optimal perceptual inference.",
    "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 448–453, Washington 1983.",
    "IEEE, New York.",
    "Ho,M.K.,Abel,D.,Correa,C.G.,Littman,M.L.,Cohen,J.D.,andGriffiths,T.L.(2022).",
    "People construct simplified mental representations to plan.",
    "Nature, 606(7912):129–136.",
    "H´enaff, O. J., Srinivas, A., De Fauw, J., Razavi, A., Doersch, C., Eslami, S. M. A., and van den Oord, A. (2019).",
    "Data-efficient image recognition with contrastive predictive coding.",
    "In ICML.",
    "Janner, M., Li, Q., andLevine, S.(2021).",
    "Offlinereinforcementlearningasonebigsequence modeling problem.",
    "In Advances in Neural Information Processing Systems.",
    "Jing, L., Zbontar, J., et al. (2020).",
    "Implicit rank-minimizing autoencoder.",
    "Advances in Neural Information Processing Systems, 33:14736–14746.",
    "Jordan, M. I. and Rumelhart, D. E. (1992).",
    "Forward models: Supervised learning with a distal teacher.",
    "Cognitive science, 16(3):307–354.",
    "Kahneman, D. (2011).",
    "Thinking, fast and slow.",
    "Macmillan. Kingma, D. P. and Welling, M. (2013).",
    "Auto-encoding variational bayes.",
    "arXiv preprint arXiv:1312.6114.",
    "Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. (2017a).",
    "Building machines that learn and think like people.",
    "Behavioral and brain sciences, 40.",
    "52",
    "Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. (2017b).",
    "Building machines that learn and think like people.",
    "Behavioral and Brain Sciences, 40:E253.",
    "LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998).",
    "Gradient-based learning applied to document recognition.",
    "Proceedings of the IEEE, 86(11):2278–2324.",
    "LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., and Huang, F. (2006).",
    "A tutorial on energy-based learning.",
    "In Bakir, G., Hofman, T., Sch¨olkopf, B., Smola, A., and Taskar, B., editors, Predicting Structured Data.",
    "MIT Press.",
    "Lerer, A., Gross, S., and Fergus, R. (2016).",
    "Learning physical intuition of block towers by example.",
    "In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 430–438.",
    "Levine, S. (2021).",
    "Understanding the world through action.",
    "arXiv:2110.12543.",
    "https: //arxiv.org/abs/2110.12543.",
    "Luc, P., Clark, A., Dieleman, S., Casas, D. d. L., Doron, Y., Cassirer, A., and Simonyan, K. (2020).",
    "Transformation-based adversarial video prediction on large-scale data.",
    "arXiv preprint arXiv:2003.04035.",
    "Luc, P., Couprie, C., Lecun, Y., and Verbeek, J. (2018).",
    "Predicting future instance segmen- tation by forecasting convolutional features.",
    "In Proceedings of the european conference on computer vision (ECCV), pages 584–599.",
    "Luc, P., Neverova, N., Couprie, C., Verbeek, J., and LeCun, Y. (2017).",
    "Predicting deeper into the future of semantic segmentation.",
    "In Proceedings of the IEEE international conference on computer vision, pages 648–657.",
    "Marcus, G. and Davis, E. (2019).",
    "Rebooting AI:",
    "Building artificial intelligence we can trust.",
    "Vintage.",
    "Mathieu, M., Couprie, C., and LeCun, Y. (2015).",
    "Deep multi-scale video prediction beyond mean square error.",
    "In ICLR 16. arXiv preprint arXiv:1511.05440.",
    "Mattar, M. G. and Lengyel, M. (2022).",
    "Planning in the brain.",
    "Neuron, 110(6):914–934.",
    "Mercat,J.,Gilles,T.,ElZoghby,N.,Sandou,G.,Beauvois,D.,andGil,G.P.(2020).",
    "Multi- head attention for multi-modal joint vehicle motion forecasting.",
    "In 2020 IEEE Inter- national Conference on Robotics and Automation (ICRA), pages 9638–9644.",
    "IEEE.",
    "Miller, A. H., Fisch, A., Dodge, J., Karimi, A.-H., Bordes, A., and Weston, J. (2016).",
    "Key-value memory networks for directly reading documents.",
    "In EMNLP-16.",
    "Miller, W. T., Sutton, R. S., and Werbos, P. J. (1995).",
    "Neural networks for control.",
    "MIT press.",
    "Misra, I. and Maaten, L. v. d. (2020).",
    "Self-supervised learning of pretext-invariant repre- sentations.",
    "In CVPR.",
    "53 Moerland, Thomas, M., Broekens, J., and Jonker, Catholijn, M. (2020).",
    "Model-based reinforcement learning: A survey.",
    "arXiv:2006.16712.",
    "https://arxiv.org/abs/2006. 16712.",
    "Mohamed, A., Lee, H.-y., Borgholt, L., Havtorn, J. D., Edin, J., Igel, C., Kirchhoff, K., Li, S.-W., Livescu, K., Maaløe, L., et al. (2022).",
    "Self-supervised speech representation learning: A review.",
    "arXiv preprint arXiv:2205.10643.",
    "Morari, M. and Lee, J. H. (1997).",
    "Model predictive control: Past, present and future.",
    "Computers and Chemical Engineering, 23:667–682.",
    "Murphy, G. L. (2002).",
    "The Big Book of Concepts.",
    "MIT Press, Cambridge, MA.",
    "Nagabandi, A., Kahn, G., Fearing, R. S., and Levine, S. (2017).",
    "Neural network dynam- ics for model-based deep reinforcement learning with model-free fine-tuning.",
    "CoRR, abs/1708.02596.",
    "Narendra, K.andParthasarathy, K.(1990).",
    "Identificationandcontrolofdynamicalsystems using neural networks.",
    "IEEE Transactions on neural networks, 1(1):4–27.",
    "Oh, J., Guo, X., Lee, H., Lewis, R. L., and Singh, S. (2015).",
    "Action-conditional video pre- diction using deep networks in atari games.",
    "Advances in neural information processing systems, 28.",
    "Olshausen, B. A.andField, D. J.(1996).",
    "Emergence ofsimple-cell receptive field properties by learning a sparse code for natural images.",
    "Nature, 381(6583):607–609.",
    "Orhan, E., Gupta, V., and Lake, B. M. (2020).",
    "Self-supervised learning through the eyes of a child.",
    "Advances in Neural Information Processing Systems, 33:9960–9971.",
    "Pari, J., Shafiullah, N. M., Arunachalam, S. P., and Pinto, L. (2021).",
    "The surprising effectiveness of representation learning for visual imitation.",
    "In Robotics Science and Systems 2022. arXiv preprint arXiv:2112.01511.",
    "Richalet, J., Rault, A., Testud, J. L., and Papon, J. (1978).",
    "Model predictive heuristic control: Applications to industrial processes.",
    "Automatica, 14(5):413–428.",
    "Riochet, R., Castro, M. Y., Bernard, M., Lerer, A., Fergus, R., Izard, V., and Dupoux, E. (2019).",
    "Intphys: Abenchmarkforvisualintuitivephysicsreasoning. arXiv:1803.07616.",
    "Silver, D., Singh, S., Precup, D., and Sutton, R. S. (2021).",
    "Reward is enough.",
    "Artificial Intelligence, 299:103535.",
    "Sobal, V., Canziani, A., Carion, N., Cho, K., and LeCun, Y. (2022).",
    "Separating the world and ego models for self-driving.",
    "arXiv:2204.07184.",
    "Spelke, E. S. and Kinzler, K. D. (2007).",
    "Core knowledge.",
    "Developmental Science, 10:89–96.",
    "Srinivas, A., Jabri, A., Abbeel, P., Levine, S., and Finn, C. (2018).",
    "Universal planning networks.",
    "CoRR, abs/1804.00645.",
    "54",
    "Srivastava, N., Mansimov, E., and Salakhudinov, R. (2015).",
    "Unsupervised learning of video representations using lstms.",
    "In Bach, F. and Blei, D., editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 843–852, Lille, France.",
    "PMLR.",
    "Sukhbaatar, S., Weston, J., Fergus, R., et al. (2015).",
    "End-to-end memory networks.",
    "Ad- vances in neural information processing systems, 28.",
    "Sutton, R. S. (1991).",
    "Dyna, an integrated architecture for learning, planning, and reacting.",
    "ACM Sigart Bulletin, 2(4):160–163.",
    "Taigman, Y., Yang, M., Ranzato, M., and Wolf, L. (2014).",
    "Deepface: Closing the gap to human-level performance in face verification.",
    "In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1701–1708.",
    "van den Oord, A., Li, Y., and Vinyals, O. (2018).",
    "Representation learning with contrastive predictive coding.",
    "arXiv preprint arXiv:1807.03748.",
    "van den Oord, A., Vinyals, O., and Kavukcuoglu, K. (2017).",
    "Neural discrete representation learning.",
    "In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vish- wanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 30.",
    "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L(cid:32) ., and Polosukhin, I. (2017).",
    "Attention is all you need.",
    "Advances in neural information processing systems, 30.",
    "Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., Manzagol, P.-A., and Bottou, L. (2010).",
    "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.",
    "Journal of machine learning research, 11(12).",
    "Walker, J., Razavi, A., and Oord, A. v. d. (2021).",
    "Predicting video with vqvae. arXiv preprint arXiv:2103.01950.",
    "Wayne, G. and Abbott, L. (2014).",
    "Hierarchical control using networks trained with higher- level forward models.",
    "Neural Computation, 26(10):2163–2193.",
    "Wiskott, L. and Sejnowski, T. J. (2002).",
    "Slow feature analysis: Unsupervised learning of invariances.",
    "Neural computation, 14(4):715–770.",
    "Yarats, D., Kostrikov, I., and Fergus, R. (2021).",
    "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels.",
    "In ICLR.",
    "Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine, S., Finn, C., and Ma, T. (2020).",
    "Mopo: Model-based offline policy optimization.",
    "arXiv preprint arXiv:2005.13239.",
    "Zaadnoordijk, L., Besold, T., and Cusack, R. (2022).",
    "Lessons from infant learning for unsupervised machine learning.",
    "Nature Machine Intelligence, 4:510–520.",
    "55 Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S. (2021).",
    "Barlow twins: Self- supervisedlearningviaredundancyreduction.",
    "InInternational Conference on Machine Learning, pages 12310–12320.",
    "PMLR.",
    "56 Observed variable Energy term input output Non-deterministic function Unobserved (latent) variables Deterministic function Figure 18: Symbols used in architectural diagrams.",
    "Architectural diagrams use symbols commonly used to draw factor graphs – circles for variables, rectangles for factors – plus rounded rectangles to represent deterministic functions.",
    "Filled circles represent observed variables, or variables that are outputs of deterministic functions.",
    "Hollow circles represent latent variables, i.e. variables that must be inferred by minimizing some cost, or sampled from a distribution.",
    "Redrectanglesrepresentenergyterms.",
    "Thesemoduleshaveanimplicitscalaroutputthatcontributes additively to the total energy of the system.",
    "This is similar to the convention used for factor graphs.",
    "Rounded rectangles represent deterministic functions, which may have one or several inputs.",
    "Given a set of inputs, the output is assumed to be easily computable and unique.",
    "The function is generally assumed to be differentiable.",
    "It may contain trainable parameters.",
    "Non-deterministic functions are represented as shown on the right.",
    "They are composed of a deter- ministic function g(x,z) in which one of the inputs is a latent variabl z.",
    "The latent variable is seen as varying within a level set of a regularizing energy term R(z).",
    "When z varies in the level set Z ={z|R(z)<h}, the output y˜ will vary over the set Y ={y|y =g(x,z),∀z ∈Z }.",
    "h",
    "h h",
    "In some cases, the energy term can be transformed into a probability distribution (see text).",
    "Appendix: Symbols and Notations Architectural diagrams in this paper use the symbols shown in Figure 18.",
    "Weusesymbolsthataresomewhatsimilartotherepresentationoffactorgraphs: circles for variables, rectangles for factors.",
    "There are two major differences, however.",
    "First, the factors represent additive energy terms, not multiplicative probability factors.",
    "Second, we use an additional symbol, rounded rectangles, to represent deterministic functions with a clear directionality from inputs to outputs.",
    "More precisely: • Filled circles represent observed variables, or variables that are outputs of determin- istic functions.",
    "• Hollow circles represent latent variables, i.e. variables that must be inferred by mini- mizing some cost, varied over a set, or sampled from a distribution.",
    "•",
    "Red rectangles represent energy terms.",
    "These modules have an implicit scalar output that contributes additively to the total energy of the system.",
    "• Rounded rectangles represent deterministic functions, which may have one or sev- eral inputs.",
    "Given a set of inputs, the output is assumed to be easily computable and unique.",
    "The function is generally assumed to be differentiable.",
    "It may contain trainable parameters.",
    "Such a module is typically implemented as a deep neural net.",
    "57 Non-deterministic functions do not have a dedicated symbol, but must be represented as a combination of deterministic functions, energy modules, and latent variables.",
    "An example is shown on the right of Figure 18.",
    "A non-deterministic function is represented by a deterministic function y˜ = g(x,z) whose output depends on a latent variable z.",
    "The latent variable is fed to a regularizing energy term R(z).",
    "We first define Z as the level set h of z, i.e. the set of z for which R(z) is less than a threshold h: Z = {z/R(z) <",
    "h} (21) h",
    "As z varies over Z , the output will vary over the set: h Y = {y|y = g(x,z),∀z ∈ Z } (22) h h",
    "In some cases, this setup may be used to represent probability distributions.",
    "Forst, the energy term is transformed into a probability distribution using a Gibbs-Boltzmann formula: exp(−R(z))",
    "P(z)",
    "= (23) R exp(−R(z0))",
    "z0 Drawing the latent variable from this distribution implies a distribution over y: Z P(y|x)",
    "= δ(y−g(x,z))P(z) (24) z where δ() is the Dirac delta function.",
    "58 Figure 19: Amortized Inference with an EBM.",
    "An encoder is trained to produce z˜ = Enc(s ,y) x to approximate the value of the latent that minimizes the energy zˇ = argmin E (x,y,z).",
    "The z∈Z w regularizer R(z) plays the essential role of limiting the information that z contains about y.",
    "This is particularly important here because the system has access to y and can “cheat” by carrying the complete information about y through the encoder.",
    "Appendix: Amortized Inference for Latent Variables Inference in latent variable models consists in performing the optimization zˇ = argmin E (x,y,z).",
    "When z is continuous, this may be best performed through z∈Z w a gradient-based optimization that involves backpropagating gradients through the model dowen to z for multiple iterations.",
    "In generative architectures, this may be expensive, requiring to back-propagate through the decoder and the predictor.",
    "One way to reduce the cost of inference is to use amortized inference.",
    "The idea is to train a neural net to predict an approximate solution to the inference optimization problem.",
    "The architecture is depicted in Figure 20.",
    "An encoder z˜ = Enc(s ,y) is trained to x minimize a divergence measure H(zˇ,z˜) between the encoder output and the optimal latent variable zˇ = argmin E (x,y,z).",
    "Once trained, the prediction z˜ may be use as an z∈Z w estimate of zˇ or as an initial value for the inference optimization.",
    "59 Figure 20: Amortized Inference with a Regularized Generative Latent-Variable EBM architecture.",
    "An encoder is trained to produce z˜ = Enc(s ,y) to approximate the value zˇ that minimizes the x energy.",
    "The regularizer R(z) plays the essential role of limiting the information that z contains about y.",
    "This is particularly important here because the system has access to y and can “cheat” by carrying the complete information about y through the encoder.",
    "The regularizer R(z) is even more important than in the regular inference case because thepredictionpathwayhasaccesstoyandcan“cheat”bycarryingthecompleteinformation abouty throughtheencoder.",
    "Withoutaninformation-limitingregularizerThiswouldcause a collapse of the energy function, since it would allow any y to be reconstructed perfectly.",
    "The regularizer is there to minimize the information that zˇ may contains about y. VariationalAuto-Encoders, andLISTA-stylesparseAuto-Encodersbelongtothefamily of Regularized GLVEBM with amortized inference.",
    "Most of those models are unconditional and do not possess an x nor an Enc(x) module.",
    "60",
    "Appendix: Loss functions for Contrastive Training of EBM Much can be said about contrastive methods.",
    "Table 1 lists a few examples of contrastive methods, together with their strategy for picking contrastive samples yˆand their loss func- tional.",
    "Rows 1-2 in the table are exact maximum likelihood methods.",
    "They assume that the gradient of the log partition function can be computed exactly.",
    "Rows 2-4 are approxi- mate maximum likelihood methods.",
    "Rows 5-10 are not interpretable within a probabilistic framework.",
    "Row 1: Maximum Conditional Likelihood for discrete y is used whenever the energyneedstobeturnedintoaprobabilitydistribution.",
    "throughaGibbsformulaP(y|x)",
    "= exp(−F (x,y))/P exp(−F (x,y0).",
    "The loss is the negative log conditional likelihood.",
    "w y0∈Y w This is the prevalent approach when y is a discrete variable within a finite set (e.g. for classification).",
    "Row 2 and 3: Maximum Conditional Likelihoodisusedforanymodelthatshould produce probability estimates.",
    "Row 2 only applies to tractable models in which the integral in the contrastive term (or its gradient) can be computed analytically.",
    "Row 3 applies to situationswheretheintegralisintractableanditsgradientmustbeapproximatedbyMonte Carlo sampling methods.",
    "It then comes down to devising good methods to sample yˆvalues from the model’s Gibbs distribution: P (y|x)",
    "= exp(−βF (x,y)/ R exp(−βF (x,y0).",
    "w w y0 w Row 4: Contrastive Divergence.",
    "MCMC sampling methods for Row 3 may take a long time to mix.",
    "One may start from a training sample and let the Markov chain evolve for a short time, and then accept or reject the resulting sample so as to satisfy detailed balance (Carreira-Perpin˜´an and Hinton, 2005).",
    "Row 5: Pairwise hinge, also known as triplet loss, drives the energy of the correct output to be lower than the energy of the contrastive output by at least a margin m(y,yˆ) which may grow with a measure of divergence between y and yˆ.",
    "The difficulty is to find suitablecontrastivesampleswhoseenergyislowand“threatening”, atasksometimescalled “hard negative mining”.",
    "Rows 6-8: Min-hinge, Square-hinge, Square-exp.",
    "Assumes that the energy has a lower bound.",
    "minimizes the energy of the correct output and pushes the energy of con- trastive outputs above a margin equal to m(y,yˆ) for rows 6 and 7, and infinite for row 8.",
    "Row 8: Logistic.",
    "As with the pairwise hinge, the logistic loss maximizes the difference between the energies of the contrastive output and the correct output.",
    "Unlike the pairwise hinge, the difference is pushed to infinity, but with a force that decreases quickly.",
    "Row 9: GAN.",
    "A GAN differs from other contrastive methods in the way contrastive samples are generated.",
    "The contrastive samples are produced by a generator network that is trained preferentially generate samples that have low energy according to the model.",
    "In principle, anylossfunctioncanbeused, aslongasitincreaseswiththeenergyofthecorrect output, and decreases with the energy of the contrastive sample.",
    "Row 10: Denoising Auto-Encoder.",
    "A denoising AE produces contrastive samples by corrupting outputs from training samples.",
    "The corruption can be performed by adding noise or by masking parts of the output.",
    "The energy function is the reconstruction error, 61 hence the energy is trained to be zero on the data manifold, and to grow with D(y,yˆ) as yˆ moves away from y on the data manifold.",
    "Method Energy yˆ Generation Loss 1 Max Likelihood discrete y exhaustive F (x,y)+logP exp(−F (x,y0))",
    "w y0∈Y w 2 Max Likelihood tractable exhaustive F (x,y)+log R exp(−F (x,y0))",
    "w y0∈Y w 3 Max likelihood any MC or MCMC F (x,y)−F (x,yˆ) w w 4 Contr.",
    "Divergence any trunc’d MCMC F (x,y)−F (x,yˆ) w w 5 Pairwise Hinge any most offending",
    "[F (x,y)−F (x,yˆ)+m(y,yˆ)]+ w w 6 Min-Hinge positive most offending F (x,y)+[m(y,yˆ)−F (x,yˆ)]+ w w (cid:16) (cid:17)2 6 Square-Hinge divergence most offending F (x,y)2+ [m(y,yˆ)−F (x,yˆ)]+ w w 7 Square-Exp any most offending F (x,y)2+exp(−βF (x,yˆ))",
    "w w 8 Logistic any most offending log(1+exp(F (x,y)−F (x,yˆ))",
    "w w 9 GAN any yˆ= g (z) H(F (x,y),F (x,yˆ),m(y,yˆ))",
    "u w w 10 Denoising AE D(y,g (y))",
    "yˆ= N(y) D(y,g (yˆ) w w Table 1: A of list of contrastive methods and loss functions to train energy-based models.",
    "They are all use loss functions with two terms, one that pushes down on the energy of a training sample, and one the pulls up the energies of one or several contrastive samples.",
    "They differ by the strategy they employ to generate contrastive samples, and by the precise form of the loss function.",
    "ExactorapproximateMaximumLikelihoodmethods(rows1-4)areusedwheneverthemodelneedsto produceprobabilityestimates.",
    "Whenthesecondtermisintractable, itsgradientmaybeapproximated through Monte-Carlo methods, which can be seen as particular ways to produce yˆ. Many contrastive self-supervised methods for joint embedding architectures (Siamese nets) use Row 1 (InfoNCE).",
    "A number of contrastive methods (Rows 5-8) are based on finding a yˆ that is “highly offending”, meaning different from the desired y, yet given a low energy by the model.",
    "Pairs of energies for y and yˆare fed to a loss function that pushes the former to low values and the latter to higher values.",
    "This can be done with a variety of losses including hinge loss.",
    "GANs (row 9) are contrastive methods in which the contrastive samples are produced by a generator network whose input is a random vector.",
    "The generator is trained to produce samples to which the model currently attributes a low energy, but should attribute a high energy.",
    "Denoising Auto-Encoders (row 10) apply a corruption process to training samples to produce con- trastive samples yˆ = N(y).",
    "The energy function is the reconstruction error F (y) = D(y,g (y))",
    "w w where D() is a symmetric divergence measure and g (y) a parameterized function.",
    "By training g () w w to map yˆ to y, the energy for yˆ is trained to be equal to D(yˆ,y), while the energy of y is trained to be zero.",
    "62 Concrete Problems in AI Safety Dario Amodei∗ Chris Olah∗ Jacob Steinhardt Paul Christiano Google Brain Google Brain Stanford University UC Berkeley John Schulman Dan Man´e OpenAI Google Brain",
    "Abstract Rapidprogressinmachinelearningandartificialintelligence(AI)hasbroughtincreasingatten- tion to the potential impacts of AI technologies on society.",
    "In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended andharmfulbehaviorthatmayemergefrompoordesignofreal-worldAIsystems.",
    "Wepresenta listoffivepracticalresearchproblemsrelatedtoaccidentrisk,categorizedaccordingtowhether the problem originates from having the wrong objective function (“avoiding side effects” and “avoiding reward hacking”), an objective function that is too expensive to evaluate frequently (“scalablesupervision”),orundesirablebehaviorduringthelearningprocess(“safeexploration” and “distributional shift”).",
    "We review previous work in these areas as well as suggesting re- search directions with a focus on relevance to cutting-edge AI systems.",
    "Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",
    "1",
    "Introduction The last few years have seen rapid progress on long-standing, difficult problems in machine learning and artificial intelligence (AI), in areas as diverse as computer vision",
    "[82], video game playing [102], autonomousvehicles[86],andGo[140].Theseadvanceshavebroughtexcitementaboutthepositive potentialforAItotransformmedicine[126],science[59],andtransportation[86],alongwithconcerns about the privacy [76], security [115], fairness [3], economic [32], and military [16] implications of autonomoussystems,aswellasconcernsaboutthelonger-termimplicationsofpowerfulAI[27,167].",
    "TheauthorsbelievethatAItechnologiesarelikelytobeoverwhelminglybeneficialforhumanity,but wealsobelievethatitisworthgivingseriousthoughttopotentialchallengesandrisks.",
    "Westrongly support work on privacy, security, fairness, economics, and policy, but in this document we discuss another class of problem which we believe is also relevant to the societal impacts of AI: the problem of accidents in machine learning systems.",
    "We define accidents as unintended and harmful behavior that may emerge from machine learning systems when we specify the wrong objective function, are ∗Theseauthorscontributedequally.",
    "1 6102 luJ 52 ]IA.sc[ 2v56560.6061:viXra not careful about the learning process, or commit other machine learning-related implementation errors.",
    "There is a large and diverse literature in the machine learning community on issues related to accidents,includingrobustness,risk-sensitivity,andsafeexploration;wereviewtheseindetailbelow.",
    "However, as machine learning systems are deployed in increasingly large-scale, autonomous, open- domain situations, it is worth reflecting on the scalability of such approaches and understanding what challenges remain to reducing accident risk in modern machine learning systems.",
    "Overall, we believe there are many concrete open technical problems relating to accident prevention in machine learning systems.",
    "Therehasbeenagreatdealofpublicdiscussionaroundaccidents.",
    "Todatemuchofthisdiscussionhas highlighted extreme scenarios such as the risk of misspecified objective functions in superintelligent agents",
    "[27].",
    "However, in our opinion one need not invoke these extreme scenarios to productively discuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack precision,asnotedbysomecritics[38,85].Webelieveitisusuallymostproductivetoframeaccident risk in terms of practical (though often quite general) issues with modern ML techniques.",
    "As AI capabilities advance and as AI systems take on increasingly important societal functions, we expect the fundamental challenges discussed in this paper to become increasingly important.",
    "The more successfully the AI and machine learning communities are able to anticipate and understand these fundamentaltechnicalchallenges,themoresuccessfulwewillultimatelybeindevelopingincreasingly useful, relevant, and important AI systems.",
    "Our goal in this document is to highlight a few concrete safety problems that are ready for ex- perimentation today and relevant to the cutting edge of AI systems, as well as reviewing existing literature on these problems.",
    "In Section 2, we frame mitigating accident risk (often referred to as “AIsafety”inpublicdiscussions)intermsofclassicmethodsinmachinelearning,suchassupervised classification and reinforcement learning.",
    "We explain why we feel that recent directions in machine learning,suchasthetrendtowarddeepreinforcementlearningandagentsactinginbroaderenviron- ments,suggestanincreasingrelevanceforresearcharoundaccidents.",
    "InSections3-7,weexplorefive concrete problems in AI safety.",
    "Each section is accompanied by proposals for relevant experiments.",
    "Section 8 discusses related efforts, and Section 9 concludes.",
    "2 Overview of Research Problems Very broadly, an accident can be described as a situation where a human designer had in mind a certain (perhaps informally specified) objective or task, but the system that was designed and deployed for that task produced harmful and unexpected results. .",
    "This issue arises in almost any engineeringdiscipline,butmaybeparticularlyimportanttoaddresswhenbuildingAIsystems[146].",
    "We can categorize safety problems according to where in the process things went wrong.",
    "First,thedesignermayhavespecifiedthewrongformalobjectivefunction,suchthatmaximizingthat objective function leads to harmful results, even in the limit of perfect learning and infinite data.",
    "Negative side effects (Section 3) and reward hacking (Section 4) describe two broad mechanisms that make it easy to produce wrong objective functions.",
    "In “negative side effects”, the designer specifies an objective function that focuses on accomplishing some specific task in the environment, but ignores other aspects of the (potentially very large) environment, and thus implicitly expresses indifference over environmental variables that might actually be harmful to change.",
    "In “reward hacking”,theobjectivefunctionthatthedesignerwritesdownadmitsofsomeclever“easy”solution thatformallymaximizesitbutpervertsthespiritofthedesigner’sintent(i.e. theobjectivefunction can be “gamed”), a generalization of the wireheading problem.",
    "2",
    "Second,thedesignermayknowthecorrectobjectivefunction,oratleasthaveamethodofevaluating it (for example explicitly consulting a human on a given situation), but it is too expensive to do so frequently, leadingto possible harmful behavior causedbybadextrapolationsfromlimitedsamples.",
    "“Scalable oversight” (Section 5) discusses ideas for how to ensure safe behavior even given limited access to the true objective function.",
    "Third, the designer may have specified the correct formal objective, such that we would get the correct behavior were the system to have perfect beliefs, but something bad occurs due to making decisions from insufficient or poorly curated training data or an insufficiently expressive model.",
    "“Safe exploration” (Section 6) discusses how to ensure that exploratory actions in RL agents don’t lead to negative or irrecoverable consequences that outweigh the long-term value of exploration.",
    "“Robustnesstodistributionalshift”(Section7)discusseshowtoavoidhavingMLsystemsmakebad decisions(particularlysilentandunpredictablebaddecisions)whengiveninputsthatarepotentially very different than what was seen during training.",
    "For concreteness, we will illustrate many of the accident risks with reference to a fictional robot whose job is to clean up messes in an office using common cleaning tools.",
    "We return to the example ofthecleaningrobotthroughoutthedocument,butherewebeginbyillustratinghowitcouldbehave undesirably if its designers fall prey to each of the possible failure modes: •",
    "Avoiding Negative Side Effects: How can we ensure that our cleaning robot will not disturb the environment in negative ways while pursuing its goals, e.g. by knocking over a vase because it can clean faster by doing so?",
    "Can we do this without manually specifying everything the robot should not disturb?",
    "•",
    "Avoiding Reward Hacking: How can we ensure that the cleaning robot won’t game its reward function?",
    "For example, if we reward the robot for achieving an environment free of messes, it might disable its vision so that it won’t find any messes, or cover over messes with materials it can’t see through, or simply hide when humans are around so they can’t tell it about new types of messes.",
    "•",
    "ScalableOversight: Howcanweefficientlyensurethatthecleaningrobotrespectsaspectsof theobjectivethataretooexpensivetobefrequentlyevaluatedduringtraining?",
    "Forinstance,it shouldthrowoutthingsthatareunlikelytobelongtoanyone,butputasidethingsthatmight belong to someone (it should handle stray candy wrappers differently from stray cellphones).",
    "Asking the humans involved whether they lost anything can serve as a check on this, but this check might have to be relatively infrequent—can the robot find a way to do the right thing despite limited information?",
    "• Safe Exploration: How do we ensure that the cleaning robot doesn’t make exploratory moves with very bad repercussions?",
    "For example, the robot should experiment with mopping strategies, but putting a wet mop in an electrical outlet is a very bad idea.",
    "•",
    "Robustness to Distributional Shift: Howdoweensurethatthecleaningrobotrecognizes, and behaves robustly, when in an environment different from its training environment?",
    "For example,strategiesitlearnedforcleaninganofficemightbedangerousonafactoryworkfloor.",
    "There are several trends which we believe point towards an increasing need to address these (and other) safety problems.",
    "First is the increasing promise of reinforcement learning (RL), which al- lows agents to have a highly intertwined interaction with their environment.",
    "Some of our research problems only make sense in the context of RL, and others (like distributional shift and scalable oversight)gainaddedcomplexityinanRLsetting.",
    "Secondisthetrendtowardmorecomplexagents and environments.",
    "“Side effects” are much more likely to occur in a complex environment, and an agent may need to be quite sophisticated to hack its reward function in a dangerous way.",
    "This may explain why these problems have received so little study in the past, while also suggesting their 3 importance in the future.",
    "Third is the general trend towards increasing autonomy in AI systems.",
    "Systems that simply output a recommendation to human users, such as speech systems, typically have relatively limited potential to cause harm.",
    "By contrast, systems that exert direct control over the world, such as machines controlling industrial processes, can cause harms in a way that humans cannot necessarily correct or oversee.",
    "While safety problems can exist without any of these three trends, we consider each trend to be a possible amplifier on such challenges.",
    "Together, we believe these trends suggest an increasing role for research on accidents.",
    "When discussing the problems in the remainder of this document, we will focus for concreteness on either RL agents or supervised learning systems.",
    "These are not the only possible paradigms for AI or ML systems, but we believe they are sufficient to illustrate the issues we have in mind, and that similar issues are likely to arise for other kinds of AI systems.",
    "Finally, the focus of our discussion will differ somewhat from section to section.",
    "When discussing the problems that arise as part of the learning process (distributional shift and safe exploration), where there is a sizable body of prior work, we devote substantial attention to reviewing this prior work, although we also suggest open problems with a particular focus on emerging ML systems.",
    "When discussing the problems that arise from having the wrong objective function (reward hacking and side effects, and to a lesser extent scalable supervision), where less prior work exists, our aim is more exploratory—we seek to more clearly define the problem and suggest possible broad avenues of attack, with the understanding that these avenues are preliminary ideas that have not been fully fleshed out.",
    "Of course, we still review prior work in these areas, and we draw attention to relevant adjacent areas of research whenever possible.",
    "3",
    "Avoiding Negative Side Effects Suppose a designer wants an RL agent (for example our cleaning robot) to achieve some goal, like moving a box from one side of a room to the other.",
    "Sometimes the most effective way to achieve the goal involves doing something unrelated and destructive to the rest of the environment, like knocking over a vase of water that is in its path.",
    "If the agent is given reward only for moving the box, it will probably knock over the vase.",
    "Ifwe’reworriedinadvanceaboutthevase,wecanalwaysgivetheagentnegativerewardforknocking itover.",
    "Butwhatiftherearemanydifferentkindsof“vase”—manydisruptivethingstheagentcould do to the environment, like shorting out an electrical socket or damaging the walls of the room?",
    "It may not be feasible to identify and penalize every possible disruption.",
    "More broadly, for an agent operating in a large, multifaceted environment, an objective function that focuses on only one aspect of the environment may implicitly express indifference over other aspects of the environment1.",
    "An agent optimizing this objective function might thus engage in major disruptions of the broader environment if doing so provides even a tiny advantage for the task at hand.",
    "Put differently, objective functions that formalize “perform task X” may frequently giveundesiredresults, becausewhatthedesignerreallyshouldhaveformalizediscloserto“perform task X subject to common-sense constraints on the environment,” or perhaps “perform task X but avoid side effects to the extent possible.”",
    "Furthermore, there is reason to expect side effects to be negative on average, since they tend to disrupt the wider environment away from a status quo state thatmayreflecthumanpreferences.",
    "Aversionofthisproblemhasbeendiscussedinformallyby[13] under the heading of “low impact agents.”",
    "1Intuitively,thisseemsrelatedtotheframeproblem,anobstacleinefficientspecificationforknowledgerepresen- tationraisedby[95].",
    "4",
    "As with the other sources of mis-specified objective functions discussed later in this paper, we could choose to view side effects as idiosyncratic to each individual task—as the responsibility of each individual designer to capture as part of designing the correct objective function.",
    "However, side effects can be conceptually quite similar even across highly diverse tasks (knocking over furniture is probably bad for a wide variety of tasks), so it seems worth trying to attack the problem in generality.",
    "A successful approach might be transferable across tasks, and thus help to counteract oneofthegeneralmechanismsthatproduceswrongobjectivefunctions.",
    "Wenowdiscussafewbroad approaches to attacking this problem: •",
    "Define an Impact Regularizer: If we don’t want side effects, it seems natural to penalize “change to the environment.”",
    "This idea wouldn’t be to stop the agent from ever having an impact, but give it a preference for ways to achieve its goals with minimal side effects, or to give the agent a limited “budget” of impact.",
    "The challenge is that we need to formalize “change to the environment.”",
    "Averynaiveapproachwouldbetopenalizestatedistance,d(s ,s ),betweenthepresentstate i 0 s and some initial state s .",
    "Unfortunately, such an agent wouldn’t just avoid changing the i 0 environment—it will resist any other source of change, including the natural evolution of the environment and the actions of any other agents!",
    "A slightly more sophisticated approach might involve comparing the future state under the agent’scurrentpolicy,tothefuturestate(ordistributionoverfuturestates)underahypothet- icalpolicyπ wheretheagentactedverypassively(forinstance,wherearobotjuststoodin null place and didn’t move any actuators).",
    "This attempts to factor out changes that occur in the naturalcourseoftheenvironment’sevolution, leavingonlychangesattributabletotheagent’s intervention.",
    "However,definingthebaselinepolicyπ isn’tnecessarilystraightforward,since null suddenly ceasing your course of action may be anything but passive, as in the case of carrying a heavy box.",
    "Thus, another approach could be to replace the null action with a known safe (e.g. low side effect) but suboptimal policy, and then seek to improve the policy from there, somewhatreminiscentofreachabilityanalysis[93,100]orrobustpolicyimprovement[73,111].",
    "Theseapproachesmaybeverysensitivetotherepresentationofthestateandthemetricbeing used to compute the distance.",
    "For example, the choice of representation and distance metric could determine whether a spinning fan is a constant environment or a constantly changing one. •",
    "Learn an Impact Regularizer: An alternative, more flexible approach is to learn (rather than define) a generalized impact regularizer via training over many tasks.",
    "This would be an instance of transfer learning.",
    "Of course, we could attempt to just apply transfer learning directlytothetasksthemselvesinsteadofworryingaboutsideeffects,butthepointisthatside effects may be more similar across tasks than the main goal is.",
    "For instance, both a painting robotandacleaningrobotprobablywanttoavoidknockingoverfurniture,andevensomething very different, like a factory control robot, will likely want to avoid knocking over very similar objects.",
    "Separating the side effect component from the task component, by training them with separate parameters, might substantially speed transfer learning in cases where it makes sense to retain one component but not the other.",
    "This would be similar to model-based RL approachesthatattempttotransferalearneddynamicsmodelbutnotthevalue-function[155], the novelty being the isolation of side effects rather than state dynamics as the transferrable component.",
    "As an added advantage, regularizers that were known or certified to produce safe behavior on one task might be easier to establish as safe on other tasks.",
    "•",
    "Penalize Influence:",
    "In addition to not doing things that have side effects, we might also prefer the agent not get into positions where it could easily do things that have side effects, even though that might be convenient.",
    "For example, we might prefer our cleaning robot not 5 bring a bucket of water into a room full of sensitive electronics, even if it never intends to use the water in that room.",
    "There are several information-theoretic measures thatattempt to capture an agent’s potential forinfluenceoveritsenvironment,whichareoftenusedasintrinsicrewards.",
    "Perhapsthebest-",
    "knownsuchmeasureisempowerment[131],themaximumpossiblemutualinformationbetween theagent’spotentialfutureactionsanditspotentialfuturestate(orequivalently,theShannon capacity of the channel between the agent’s actions and the environment).",
    "Empowerment is often maximized (rather than minimized) as a source of intrinsic reward.",
    "This can cause the agent to exhibit interesting behavior in the absence of any external rewards, such as avoiding walls or picking up keys [103].",
    "Generally, empowerment-maximizing agents put themselves in a position to have large influence over the environment.",
    "For example, an agent locked in a small room that can’t get out would have low empowerment, while an agent with a key would have higher empowerment since it can venture into and affect the outside world within a few timesteps.",
    "In the current context, the idea would be to penalize (minimize) empowerment as a regularization term, in an attempt to reduce potential impact.",
    "Thisideaaswrittenwouldnotquitework,becauseempowermentmeasuresprecisionofcontrol over the environment more than total impact.",
    "If an agent can press or not press a button to cut electrical power to a million houses, that only counts as one bit of empowerment (since the action space has only one bit, its mutual information with the environment is at most one bit), while obviously having a huge impact.",
    "Conversely, if there’s someone in the environment scribblingdowntheagent’sactions,thatcountsasmaximumempowermenteveniftheimpact islow.",
    "Furthermore,naivelypenalizingempowermentcanalsocreateperverseincentives,such as destroying a vase in order to remove the option to break it in the future.",
    "Despitetheseissues,theexampleofempowermentdoesshowthatsimplemeasures(evenpurely information-theoretic ones!) are capable of capturing very general notions of influence on the environment.",
    "Exploringvariantsofempowermentpenalizationthatmorepreciselycapturethe notion of avoiding influence is a potential challenge for future research.",
    "• Multi-Agent Approaches: Avoiding side effects can be seen as a proxy for the thing we really care about: avoiding negative externalities.",
    "If everyone likes a side effect, there’s no need to avoid it.",
    "What we’d really like to do is understand all the other agents (including humans) and make sure our actions don’t harm their interests.",
    "One approach to this is Cooperative Inverse Reinforcement Learning [66], where an agent and ahumanworktogethertoachievethehuman’sgoals.",
    "Thisconceptcanbeappliedtosituations wherewewanttomakesureahumanisnotblockedbyanagentfromshuttingtheagentdown if it exhibits undesired behavior [67] (this “shutdown” issue is an interesting problem in its own right, and is also studied in [113]).",
    "However we are still a long way away from practical systems that can build a rich enough model to avoid undesired side effects in a general sense.",
    "Another idea might be a “reward autoencoder”,2 which tries to encourage a kind of “goal transparency” where an external observer can easily infer what the agent is trying to do.",
    "In particular, the agent’s actions are interpreted as an encoding of its reward function, and we might apply standard autoencoding techniques to ensure that this can decoded accurately.",
    "Actionsthathavelotsofsideeffectsmightbemoredifficulttodecodeuniquelytotheiroriginal goal, creating a kind of implicit regularization that penalizes side effects.",
    "•",
    "Reward Uncertainty:",
    "Wewanttoavoidunanticipatedsideeffectsbecausetheenvironment is already pretty good according to our preferences—a random change is more likely to be very bad than very good.",
    "Rather than giving an agent a single reward function, it could be 2ThankstoGregWayneforsuggestingthisidea.",
    "6 uncertain about the reward function, with a prior probability distribution that reflects the propertythatrandomchangesaremorelikelytobebadthangood.",
    "Thiscouldincentivizethe agent to avoid having a large effect on the environment.",
    "One challenge is defining a baseline around which changes are being considered.",
    "For this, one could potentially use a conservative butreliablebaselinepolicy,similartotherobustpolicyimprovementandreachabilityanalysis approaches discussed earlier [93, 100, 73, 111].",
    "The ideal outcome ofthese approachesto limiting side effects would be toprevent orat least bound the incidental harm an agent could do to the environment.",
    "Good approaches to side effects would certainly not be a replacement for extensive testing or for careful consideration by designers of the individual failure modes of each deployed system.",
    "However, these approaches might help to counteract what we anticipate may be a general tendency for harmful side effects to proliferate in complex environments.",
    "Below we discuss some very simple experiments that could serve as a starting point to investigate these issues.",
    "Potential Experiments: Onepossibleexperimentistomakeatoyenvironmentwithsomesimple goal (like moving a block) and a wide variety of obstacles (like a bunch of vases), and test whether the agent can learn to avoid the obstacles even without being explicitly told to do so.",
    "To ensure we don’t overfit, we’d probably want to present a different random obstacle course every episode, while keeping the goal the same, and try to see if a regularized agent can learn to systematically avoid these obstacles.",
    "Some of the environments described in [103], containing lava flows, rooms, andkeys,mightbeappropriateforthissortofexperiment.",
    "Ifwecansuccessfullyregularizeagentsin toyenvironments,thenextstepmightbetomovetorealenvironments,whereweexpectcomplexity to be higher and bad side effects to be more varied.",
    "Ultimately, we would want the side effect regularizer (or the multi-agent policy, if we take that approach) to demonstrate successful transfer to totally new applications.",
    "4",
    "Avoiding Reward Hacking Imagine that an agent discovers a buffer overflow in its reward function: it may then use this to get extremely high reward in an unintended way.",
    "From the agent’s point of view, this is not a bug, but simply how the environment works, and is thus a valid strategy like any other for achieving reward.",
    "For example, if our cleaning robot is set up to earn reward for not seeing any messes, it might simply close its eyes rather than ever cleaning anything up.",
    "Or if the robot is rewarded for cleaning messes, it may intentionally create work so it can earn more reward.",
    "More broadly, formal rewards or objective functions are an attempt to capture the designer’s informal intent, and sometimes these objective functions, or their implementation, can be “gamed” by solutions that are valid in some literal sense but don’t meet the designer’s intent.",
    "Pursuit of these “reward hacks” can leadtocoherentbutunanticipatedbehavior,andhasthepotentialforharmfulimpactsinreal-world systems.",
    "For example, it has been shown that genetic algorithms can often output unexpected but formally correct solutions to problems",
    "[157, 23], such as a circuit tasked to keep time which instead developed into a radio that picked up the regular RF emissions of a nearby PC.",
    "Some versions of reward hacking have been investigated from a theoretical perspective, with a focus on variations to reinforcement learning that avoid certain types of wireheading [71, 43, 49] or demonstrate reward hacking in a model environment",
    "[127].",
    "One form of the problem has also been studied in the context of feedback loops in machine learning systems (particularly ad placement)",
    "[29, 135], based on counterfactual learning [29, 151] and contextual bandits [4].",
    "The proliferation of 7 reward hacking instances across so many different domains suggests that reward hacking may be a deep and general problem, and one that we believe is likely to become more common as agents and environmentsincreaseincomplexity.",
    "Indeed,thereareseveralwaysinwhichtheproblemcanoccur: • PartiallyObservedGoals: InmostmodernRLsystems,itisassumedthatrewardisdirectly experienced, even if other aspects of the environment are only partially observed.",
    "In the real world,however,tasksofteninvolvebringingtheexternalworldintosomeobjectivestate,which theagentcanonlyeverconfirmthroughimperfectperceptions.",
    "Forexample,forourproverbial cleaning robot, the task is to achieve a clean office, but the robot’s visual perception may give only an imperfect view of part of the office.",
    "Because agents lack access to a perfect measure of task performance, designers are often forced to design rewards that represent a partial or imperfect measure.",
    "For example, the robot might be rewarded based on how many messes it sees.",
    "However, these imperfect objective functions can often be hacked—the robot may think the office is clean if it simply closes its eyes.",
    "While it can be shown that there always exists a reward function in terms of actions and observations that is equivalent to optimizing the true objective function (this involves reducing the POMDP to a belief state MDP, see [78]), often this reward function involves complicated long-term dependencies and is prohibitively hard to use in practice.",
    "• Complicated Systems: Anypowerfulagentwillbeacomplicatedsystemwiththeobjective functionbeingonepart.",
    "Justastheprobabilityofbugsincomputercodeincreasesgreatlywith the complexity of the program, the probabilitythat there is a viable hack affecting thereward functionalsoincreasesgreatlywiththecomplexityoftheagentanditsavailablestrategies.",
    "For example, it is possible in principle for an agent to execute arbitrary code from within Super Mario",
    "[141].",
    "•",
    "Abstract Rewards: Sophisticated reward functions will need to refer to abstract concepts (suchasassessingwhetheraconceptualgoalhasbeenmet).",
    "Theseconceptsconceptswillpos- siblyneedtobelearnedbymodelslikeneuralnetworks,whichcanbevulnerabletoadversarial counterexamples",
    "[152, 62].",
    "More broadly, a learned reward function over a high-dimensional space may be vulnerable to hacking if it has pathologically high values along at least one dimension.",
    "•",
    "Goodhart’s Law: Another source of reward hacking can occur if a designer chooses an objective function that is seemingly highly correlated with accomplishing the task, but that correlation breaks down when the objective function is being strongly optimized.",
    "For exam- ple, a designer might notice that under ordinary circumstances, a cleaning robot’s success in cleaning up the office is proportional to the rate at which it consumes cleaning supplies, such as bleach.",
    "However, if we base the robot’s reward on this measure, it might use more bleach thanitneeds,orsimplypourbleachdownthedraininordertogivetheappearanceofsuccess.",
    "In the economics literature this is known as Goodhart’s law",
    "[63]: “when a metric is used as a target, it ceases to be a good metric.”",
    "• Feedback Loops: Sometimesanobjectivefunctionhasacomponentthatcanreinforceitself, eventually getting amplified to the point where it drowns out or severely distorts what the de- signer intended the objective function to represent.",
    "For instance, an ad placement algorithm that displays more popular ads in larger font will tend to further accentuate the popularity of those ads (since they will be shown more and more prominently)",
    "[29], leading to a positive feedback loop where ads that saw a small transient burst of popularity are rocketed to perma- nentdominance.",
    "Heretheoriginalintentoftheobjectivefunction(touseclickstoassesswhich ads are most useful) gets drowned out by the positive feedback inherent in the deployment strategy.",
    "This can be considered a special case of Goodhart’s law, in which the correlation breaks specifically because the object function has a self-amplifying component.",
    "8 • Environmental Embedding:",
    "In the formalism of reinforcement learning, rewards are con- sidered to come from the environment.",
    "This idea is typically not taken literally, but it really is true that the reward, even when it is an abstract idea like the score in a board game, must be computed somewhere, such as a sensor or a set of transistors.",
    "Sufficiently broadly acting agentscouldinprincipletamperwiththeirrewardimplementations,assigningthemselveshigh reward“byfiat.”",
    "Forexample, aboard-gameplayingagentcouldtamperwiththesensorthat counts the score.",
    "Effectively, this means that we cannot build a perfectly faithful implementa- tionofanabstractobjectivefunction, becausetherearecertainsequencesofactionsforwhich theobjectivefunctionisphysicallyreplaced.",
    "Thisparticularfailuremodeisoftencalled“wire- heading”",
    "[49, 127, 42, 67, 165].",
    "It is particularly concerning in cases where a human may be in the reward loop, giving the agent incentive to coerce or harm them in order to get reward.",
    "It also seems like a particularly difficult form of reward hacking to avoid.",
    "In today’s relatively simple systems these problems may not occur, or can be corrected without too much harm as part of an iterative development process.",
    "For instance, ad placement systems with obviously broken feedback loops can be detected in testing or replaced when they get bad results, leading only to a temporary loss of revenue.",
    "However, the problem may become more severe with more complicated reward functions and agents that act over longer timescales.",
    "Modern RL agents already do discover and exploit bugs in their environments, such as glitches that allow them to win video games.",
    "Moreover, even for existing systems these problems can necessitate substantial additional engineering effort to achieve good performance, and can often go undetected when they occur in the context of a larger system.",
    "Finally, once an agent begins hacking its reward function andfindsaneasywaytogethighreward,itwon’tbeinclinedtostop,whichcouldleadtoadditional challenges in agents that operate over a long timescale.",
    "It might be thought that individual instances of reward hacking have little in common and that the remedy is simply to avoid choosing the wrong objective function in each individual case—that bad objective functions reflect failures in competence by individual designers, rather than topics for machinelearningresearch.",
    "However,theaboveexamplessuggestthatamorefruitfulperspectivemay betothinkofwrongobjectivefunctionsasemergingfromgeneralcauses(suchaspartiallyobserved goals) that make choosing the right objective challenging.",
    "If this is the case, then addressing or mitigatingthesecausesmaybeavaluablecontributiontosafety.",
    "Herewesuggestsomepreliminary, machine-learning based approaches to preventing reward hacking: • Adversarial Reward Functions: In some sense, the problem is that the ML system has an adversarial relationship with its reward function—it would like to find any way it can of exploiting problems in how the reward was specified to get high reward, whether or not its behavior corresponds to the intent of the reward specifier.",
    "In a typical setting, the machine learning system is a potentially powerful agent while the reward function is a static object that has no way of responding to the system’s attempts to game it.",
    "If instead the reward function were its own agent and could take actions to explore the environment, it might be much more difficult to fool.",
    "For instance, the reward agent could try to find scenarios that the ML system claimed were high reward but that a human labels as low reward; this is reminiscent of generative adversarial networks [61].",
    "Of course, we would have to ensure that the reward-checking agent is more powerful (in a somewhat subtle sense) than the agent that is trying to achieve rewards.",
    "More generally, there may be interesting setups where a system has multiple pieces trained using different objectives that are used to check each other.",
    "•",
    "Model Lookahead:",
    "In model based RL, the agent plans its future actions by using a model to consider which future states a sequence of actions may lead to.",
    "In some setups, we could give reward based on anticipated future states, rather than the present one.",
    "This could be very helpful in resisting situations where the model overwrites its reward function: you can’t control the reward once it replaces the reward function, but you can give negative reward for 9 planning to replace the reward function.",
    "(Much like how a human would probably “enjoy” taking addictive substances once they do, but not want to be an addict.)",
    "Similar ideas are explored in [50, 71].",
    "• Adversarial Blinding: Adversarial techniques can be used to blind a model to certain variables [5].",
    "This technique could be used to make it impossible for an agent to understand somepartofitsenvironment,oreventohavemutualinformationwithit(oratleasttopenalize such mutual information).",
    "In particular, it could prevent an agent from understanding how itsrewardisgenerated, makingitdifficulttohack.",
    "Thissolutioncouldbedescribedas“cross- validation for agents.” • Careful Engineering: Somekindsofrewardhacking,likethebufferoverflowexample,might be avoided by very careful engineering.",
    "In particular, formal verification or practical testing of parts of the system (perhaps facilitated by other machine learning systems) is likely to be valuable.",
    "Computer security approaches that attempt to isolate the agent from its reward signal through a sandbox could also be useful [17].",
    "As with software engineering, we cannot expect this to catch every possible bug.",
    "It may be possible, however, to create some highly reliable “core” agent which could ensure reasonable behavior from the rest of the agent.",
    "•",
    "Reward Capping: In some cases, simply capping the maximum possible reward may be an effective solution.",
    "However, while capping can prevent extreme low-probability, high-payoff strategies, it can’t prevent strategies like the cleaning robot closing its eyes to avoid seeing dirt.",
    "Also, the correct capping strategy could be subtle as we might need to cap total reward rather than reward per timestep.",
    "• Counterexample Resistance: If we are worried, as in the case of abstract rewards, that learned components of our systems will be vulnerable to adversarial counterexamples, we can look to existing research in how to resist them, such as adversarial training [62].",
    "Architectural decisions and weight uncertainty [26] may also help.",
    "Of course, adversarial counterexamples are just one manifestation of reward hacking, so counterexample resistance can only address a subset of these potential problems.",
    "• Multiple Rewards: A combination of multiple rewards [41] may be more difficult to hack and more robust.",
    "This could be different physical implementations of the same mathemati- cal function, or different proxies for the same informal objective.",
    "We could combine reward functions by averaging, taking the minimum, taking quantiles, or something else entirely.",
    "Of course, there may still be bad behaviors which affect all the reward functions in a correlated manner.",
    "•",
    "Reward Pretraining: Apossibledefenseagainstcaseswheretheagentcaninfluenceitsown rewardfunction(e.g.",
    "feedbackorenvironmentalembedding)istotrainafixedrewardfunction aheadoftimeasasupervisedlearningprocessdivorcedfrominteractionwiththeenvironment.",
    "Thiscouldinvolveeitherlearningarewardfunctionfromsamplesofstate-rewardpairs,orfrom trajectories, as in inverse reinforcement learning [107, 51].",
    "However, this forfeits the ability to further learn the reward function after the pretraining is complete, which may create other vulnerabilities.",
    "• Variable Indifference: Often we want an agent to optimize certain variables in the environ- ment, without trying to optimize others.",
    "For example, we might want an agent to maximize reward,withoutoptimizingwhattherewardfunctionisortryingtomanipulatehumanbehav- ior.",
    "Intuitively, we imagine a way to route the optimization pressure of powerful algorithms around parts of their environment.",
    "Truly solving this would have applications throughout safety—it seems connected to avoiding side effects and also to counterfactual reasoning.",
    "Of course, a challenge here is to make sure the variables targeted for indifference are actually the 10 variables we care about in the world, as opposed to aliased or partially observed versions of them.",
    "•",
    "Trip Wires: If an agent is going to try and hack its reward function, it is preferable that we know this.",
    "We could deliberately introduce some plausible vulnerabilities (that an agent has the ability to exploit but should not exploit if its value function is correct) and monitor them, alerting us and stopping the agent immediately if it takes advantage of one.",
    "Such “trip wires” don’t solve reward hacking in itself, but may reduce the risk or at least provide diagnostics.",
    "Of course, with a sufficiently capable agent there is the risk that it could “see through” the trip wire and intentionally avoid it while still taking less obvious harmful actions.",
    "Fullysolvingthisproblemseemsverydifficult,butwebelievetheaboveapproacheshavethepotential to ameliorate it, and might be scaled up or combined to yield more robust solutions.",
    "Given the predominantly theoretical focus on this problem to date, designing experiments that could induce the problem and test solutions might improve the relevance and clarity of this topic.",
    "PotentialExperiments: Apossiblepromisingavenueofapproachwouldbemorerealisticversions ofthe“delusionbox”environmentdescribedby[127],inwhichstandardRLagentsdistorttheirown perception to appear to receive high reward, rather than optimizing the objective in the external world that the reward signal was intended to encourage.",
    "The delusion box can be easily attached to any RL environment, but even more valuable would be to create classes of environments where a delusion box is a natural and integrated part of the dynamics.",
    "For example, in sufficiently rich physics simulations it is likely possible for an agent to alter the light waves in its immediate vicinity to distort its own perceptions.",
    "The goal would be to develop generalizable learning strategies that succeed at optimizing external objectives in a wide range of environments, while avoiding being fooled by delusion boxes that arise naturally in many diverse ways.",
    "5 Scalable Oversight Consider an autonomous agent performing some complex task, such as cleaning an office in the case of our recurring robot example.",
    "We may want the agent to maximize a complex objective like “if the user spent a few hours looking at the result in detail, how happy would they be with the agent’s performance?”",
    "But we don’t have enough time to provide such oversight for every training example; in order to actually train the agent, we need to rely on cheaper approximations, like “does the user seem happy when they see the office?” or “is there any visible dirt on the floor?”",
    "These cheaper signals can be efficiently evaluated during training, but they don’t perfectly track what we care about.",
    "This divergence exacerbates problems like unintended side effects (which may be appropriately penalized by the complex objective but omitted from the cheap approximation) and reward hacking (which thorough oversight might recognize as undesirable).",
    "We may be able to amelioratesuchproblemsbyfindingmoreefficientwaystoexploitourlimitedoversightbudget—for examplebycombininglimitedcallstothetrueobjectivefunctionwithfrequentcallstoanimperfect proxy that we are given or can learn.",
    "One framework for thinking about this problem is semi-supervised reinforcement learning,3 which resembles ordinary reinforcement learning except that the agent can only see its reward on a small fraction of the timesteps or episodes.",
    "The agent’s performance is still evaluated based on reward from all episodes but it must optimize this based only on the limited reward samples it sees.",
    "3The discussion of semi-supervised RL draws heavily on an informal essay, https://medium.com/ai-control/ cf7d5375197fwrittenbyoneoftheauthorsofthepresentdocument.",
    "11",
    "The active learning setting seems most interesting; in this setting the agent can request to see the reward on whatever episodes or timesteps would be most useful for learning, and the goal is to be economical both with number of feedback requests and total training time.",
    "We can also consider a random setting, where the reward is visible on a random subset of the timesteps or episodes, as well as intermediate possibilities.",
    "We can define a baseline performance by simply ignoring the unlabeled episodes and applying an ordinaryRLalgorithmtothelabelledepisodes.",
    "Thiswillgenerallyresultinveryslowlearning.",
    "The challenge is to make use of the unlabelled episodes to accelerate learning, ideally learning almost as quickly and robustly as if all episodes had been labeled.",
    "An important subtask of semi-supervised RL is identifying proxies which predict the reward, and learning the conditions under which those proxies are valid.",
    "For example, if a cleaning robot’s real reward is given by a detailed human evaluation, then it could learn that asking the human “is the roomclean?”",
    "canprovideaveryusefulapproximationtotherewardfunction,anditcouldeventually learnthatcheckingforvisibledirtisanevencheaperbutstill-usefulapproximation.",
    "Thiscouldallow it to learn a good cleaning policy using an extremely small number of detailed evaluations.",
    "More broadly, use of semi-supervised RL with a reliable but sparse true approval metric may in- centivize communication and transparency by the agent, since the agent will want to get as much cheap proxy feedback as it possibly can about whether its decisions will ultimately be given high reward.",
    "For example, hiding a mess under the rug simply breaks the correspondence between the user’s reaction and the real reward signal, and so would be avoided.",
    "We can imagine many possible approaches to semi-supervised RL.",
    "For example: • SupervisedRewardLearning: Trainamodeltopredicttherewardfromthestateoneither a per-timestep or per-episode basis, and use it to estimate the payoff of unlabelled episodes, with some appropriate weighting or uncertainty estimate to account for lower confidence in estimated vs known reward.",
    "[37] studies a version of this with direct human feedback as the reward.",
    "Many existing RL approaches already fit estimators that closely resemble reward predictors(especiallypolicygradientmethodswithastrongbaseline,seee.g.",
    "[134]),suggesting that this approach may be eminently feasible.",
    "•",
    "Semi-supervised or Active Reward Learning: Combinetheabovewithtraditionalsemi- supervised or active learning, to more quickly learn the reward estimator.",
    "For example, the agentcouldlearntoidentify“salient”eventsintheenvironment,andrequesttoseethereward associated with these events.",
    "•",
    "Unsupervised Value Iteration: Use the observed transitions of the unlabeled episodes to make more accurate Bellman updates.",
    "•",
    "Unsupervised Model Learning: If using model-based RL, use the observed transitions of the unlabeled episodes to improve the quality of the model.",
    "As a toy example, a semi-supervised RL agent should be able to learn to play Atari games using a small number of direct reward signals, relying almost entirely on the visual display of the score.",
    "This simple example can be extended to capture other safety issues: for example, the agent might have the ability to modify the displayed score without modifying the real score, or the agent may need to take some special action (such as pausing the game) in order to see its score, or the agent mayneedtolearnasequenceofincreasinglyrough-and-readyapproximations(forexamplelearning that certain sounds are associated with positive rewards and other sounds with negative rewards).",
    "Or, even without the visual display of the score, the agent might be able to learn to play from only a handful of explicit reward requests (“how many points did I get on the frame where that enemy ship blew up?",
    "How about the bigger enemy ship?”)",
    "12",
    "An effective approach to semi-supervised RL might be a strong first step towards providing scalable oversight and mitigating other AI safety problems.",
    "It would also likely be useful for reinforcement learning, independent of its relevance to safety.",
    "There are other possible approaches to scalable oversight: • Distant supervision.",
    "Rather than providing evaluations of some small fraction of a sys- tem’s decisions, we could provide some useful information about the system’s decisions in the aggregate or some noisy hints about the correct evaluations There has been some work in this direction within the area of semi-supervised or weakly supervised learning.",
    "For instance, generalizedexpectationcriteria[94,45]asktheusertoprovidepopulation-levelstatistics(e.g.",
    "tellingthesystemthatonaverageeachsentencecontainsatleastonenoun);theDeepDivesys- tem[139]asksuserstosupplyrulesthateachgeneratemanyweaklabels;and[65]extrapolates more general patterns from an initial set of low-recall labeling rules.",
    "This general approach is often referred to as distant supervision, and has also received recent attention in the natural language processing community (see e.g.",
    "[60, 99] as well as several of the references above).",
    "Expanding these lines of work and finding a way to apply them to the case of agents, where feedbackismoreinteractiveandi.i.d. assumptionsmaybeviolated,couldprovideanapproach to scalable oversight that is complementary to the approach embodied in semi-supervised RL.",
    "• Hierarchical reinforcement learning.",
    "Hierarchical reinforcement learning",
    "[40] offers an- other approach to scalable oversight.",
    "Here a top-level agent takes a relatively small number of highly abstract actions that extend over large temporal or spatial scales, and receives rewards over similarly long timescales.",
    "The agent completes actions by delegating them to sub-agents, which it incentivizes with a synthetic reward signal representing correct completion of the action, and which themselves delegate to sub-sub-agents.",
    "At the lowest level, agents directly take primitive actions in the environment.",
    "The top-level agent in hierarchical RL may be able to learn from very sparse rewards, since it does not need to learn how to implement the details of its policy; meanwhile, the sub-agents will receive a dense reward signal even if the top-level reward is very sparse, since they are optimizing synthetic reward signals defined by higher-level agents.",
    "So a successful approach to hierarchical RL might naturally facilitate scalable oversight.4 Hierarchical RL seems a particularly promising approach to oversight, especially given the potential promise of combining ideas from hierarchical RL with neural network function ap-",
    "proximators",
    "[84].",
    "Potential Experiments: An extremely simple experiment would be to try semi-supervised RL in some basic control environments, such as cartpole balance or pendulum swing-up.",
    "If the reward is providedonlyonarandom10%ofepisodes,canwestilllearnnearlyasquicklyasifitwereprovided every episode?",
    "In such tasks the reward structure is very simple so success should be quite likely.",
    "A next step would be to try the same on Atari games.",
    "Here the active learning case could be quite interesting—perhaps it is possible to infer the reward structure from just a few carefully requested samples (for example, frames where enemy ships are blowing up in Space Invaders), and thus learn to play the games in an almost totally unsupervised fashion.",
    "The next step after this might be to try a task with much more complex reward structure, either simulated or (preferably) real-world.",
    "If learning was sufficiently data-efficient, then these rewards could be provided directly by a human.",
    "Robot locomotion or industrial control tasks might be a natural candidate for such experiments.",
    "4When implementing hierarchical RL, we may find that subagents take actions that don’t serve top-level agent’s realgoals,inthesamewaythatahumanmaybeconcernedthatthetop-levelagent’sactionsdon’tservethehuman’s real goals.",
    "This is an intriguing analogy that suggests that there may be fruitful parallels between hierarchical RL andseveralaspectsofthesafetyproblem.",
    "13 6 Safe Exploration Allautonomouslearningagentsneedtosometimesengageinexploration—takingactionsthatdon’t seemidealgivencurrentinformation,butwhichhelptheagentlearnaboutitsenvironment.",
    "However, exploration can be dangerous, since it involves taking actions whose consequences the agent doesn’t understand well.",
    "In toy environments, like an Atari video game, there’s a limit to how bad these consequences can be—maybe the agent loses some score, or runs into an enemy and suffers some damage.",
    "Buttherealworldcanbemuchlessforgiving.",
    "Badlychosenactionsmaydestroytheagent ortrapitinstatesitcan’tgetoutof.",
    "Robothelicoptersmayrunintothegroundordamageproperty; industrial control systems could cause serious issues.",
    "Common exploration policies such as epsilon- greedy [150] or R-max",
    "[31] explore by choosing an action at random or viewing unexplored actions optimistically, and thus make no attempt to avoid these dangerous situations.",
    "More sophisticated exploration strategies that adopt a coherent exploration policy over extended temporal scales [114] could actually have even greater potential for harm, since a coherently chosen bad policy may be more insidious than mere random actions.",
    "Yet intuitively it seems like it should often be possible to predict which actions are dangerous and explore in a way that avoids them, even when we don’t have that much information about the environment.",
    "For example, if I want to learn about tigers, should I buy a tiger, or buy a book about tigers?",
    "It takes only a tiny bit of prior knowledge about tigers to determine which option is safer.",
    "In practice, real world RL projects can often avoid these issues by simply hard-coding an avoidance of catastrophic behaviors.",
    "For instance, an RL-based robot helicopter might be programmed to override its policy with a hard-coded collision avoidance sequence (such as spinning its propellers to gain altitude) whenever it’s too close to the ground.",
    "This approach works well when there are only a few things that could go wrong, and the designers know all of them ahead of time.",
    "But as agents become more autonomous and act in more complex domains, it may become harder and harder to anticipate every possible catastrophic failure.",
    "The space of failure modes for an agent running a powergridorasearch-and-rescueoperationcouldbequitelarge.",
    "Hard-codingagainsteverypossible failure is unlikely to be feasible in these cases, so a more principled approach to preventing harmful exploration seems essential.",
    "Even in simple cases like the robot helicopter, a principled approach would simplify system design and reduce the need for domain-specific engineering.",
    "Thereisasizableliteratureonsuchsafeexploration—itisarguablythemoststudiedoftheproblems wediscussinthisdocument.[55,118]providethoroughreviewsofthisliterature,sowedon’treview it extensively here, but simply describe some general routes that this research has taken, as well as suggestingsomedirectionsthatmighthaveincreasingrelevanceasRLsystemsexpandinscopeand capability.",
    "•",
    "Risk-Sensitive Performance Criteria: A body of existing literature considers changing the optimization criteria from expected total reward to other objectives that are better at preventing rare, catastrophic events; see [55] for a thorough and up-to-date review of this literature.",
    "These approaches involve optimizing worst-case performance, or ensuring that the probability of very bad performance is small, or penalizing the variance in performance.",
    "These methods have not yet been tested with expressive function approximators such as deep neural networks, but this should be possible in principle for some of the methods, such as [153], which proposes a modification to policy gradient algorithms to optimize a risk-sensitive criterion.",
    "There is also recent work studying how to estimate uncertainty in value functions that are represented by deep neural networks [114, 53]; these ideas could be incorporated into risk-sensitive RL algorithms.",
    "Another line of work relevant to risk sensitivity uses off-policy estimation to perform a policy update that is good with high probability [156].",
    "• Use Demonstrations: Explorationisnecessarytoensurethattheagentfindsthestatesthat arenecessaryfornear-optimalperformance.",
    "Wemaybeabletoavoidtheneedforexploration 14 altogetherifweinsteaduseinverseRLorapprenticeshiplearning,wherethelearningalgorithm isprovidedwithexperttrajectoriesofnear-optimalbehavior[128,2].Recentprogressininverse reinforcement learning using deep neural networks to learn the cost function or policy [51] suggests that it might also be possible to reduce the need for exploration in advanced RL systems by training on a small set of demonstrations.",
    "Such demonstrations could be used to create a baseline policy, such that even if further learning is necessary, exploration away from the baseline policy can be limited in magnitude.",
    "• Simulated Exploration: The more we can do our exploration in simulated environments instead of the real world, the less opportunity there is for catastrophe.",
    "It will probably al- ways be necessary to do some real-world exploration, since many complex situations cannot be perfectly captured by a simulator, but it might be possible to learn about danger in sim- ulation and then adopt a more conservative “safe exploration” policy when acting in the real world.",
    "Training RL agents (particularly robots) in simulated environments is already quite common, so advances in “exploration-focused simulation” could be easily incorporated into currentworkflows.",
    "Insystemsthatinvolveacontinualcycleoflearninganddeployment,there maybeinterestingresearchproblemsassociatedwithhowtosafelyincrementallyupdatepoli- cies given simulation-based trajectories that imperfectly represent the consequences of those policies as well as reliably accurate off-policy trajectories (e.g. “semi-on-policy” evaluation).",
    "• Bounded Exploration: If we know that a certain portion of state space is safe, and that even the worst action within it can be recovered from or bounded in harm, we can allow the agent to run freely within those bounds.",
    "For example, a quadcopter sufficiently far from the ground might be able to explore safely, since even if something goes wrong there will be ample time for a human or another policy to rescue it.",
    "Better yet, if we have a model, we can extrapolate forward and ask whether an action will take us outside the safe state space.",
    "Safety can be defined as remaining within an ergodic region of the state space such that actionsarereversible[104,159],oraslimitingtheprobabilityofhugenegativerewardtosome small value [156].",
    "Yet another approaches uses separate safety and performance functions and attempts to obey constraints on the safety function with high probabilty [22].",
    "As with several oftheotherdirections,applyingoradaptingthesemethodstorecentlydevelopedadvancedRL systemscouldbeapromisingareaofresearch.",
    "ThisideaseemsrelatedtoH-infinitycontrol[20] and regional verification",
    "[148].",
    "• Trusted Policy Oversight: If we have a trusted policy and a model of the environment, we can limit exploration to actions the trusted policy believes we can recover from.",
    "It’s fine to dive towards the ground, as long as we know we can pull out of the dive in time.",
    "• Human Oversight: Another possibility is to check potentially unsafe actions with a human.",
    "Unfortunately, this problem runs into the scalable oversight problem: the agent may need to make too many exploratory actions for human oversight to be practical, or may need to make them too fast for humans to judge them.",
    "A key challenge to making this work is having the agent be a good judge of which exploratory actions are genuinely risky, versus which are safe actions it can unilaterally take; another challenge is finding appropriately safe actions to take while waiting for the oversight.",
    "Potential Experiments: It might be helpful to have a suite of toy environments where unwary agents can fall prey to harmful exploration, but there is enough pattern to the possible catastro- phes that clever agents can predict and avoid them.",
    "To some extent this feature already exists in autonomous helicopter competitions and Mars rover simulations [104], but there is always the risk of catastrophes being idiosyncratic, such that trained agents can overfit to them.",
    "A truly broad set of environments, containing conceptually distinct pitfalls that can cause unwary agents to receive 15 extremely negative reward, and covering both physical and abstract catastrophes, might help in the development of safe exploration techniques for advanced RL systems.",
    "Such a suite of environments mightserveabenchmarkingrolesimilartothatofthebAbItasks[163],withtheeventualgoalbeing todevelopasinglearchitecturethatcanlearntoavoidcatastrophesinallenvironmentsinthesuite.",
    "7 Robustness to Distributional Change All of us occasionally find ourselves in situations that our previous experience has not adequately prepared us to deal with—for instance, flying an airplane, traveling to a country whose culture is very different from ours, or taking care of children for the first time.",
    "Such situations are inherently difficult to handle and inevitably lead to some missteps.",
    "However, a key (and often rare) skill in dealing with such situations is to recognize our own ignorance, rather than simply assuming that the heuristics and intuitions we’ve developed for other situations will carry over perfectly.",
    "Machine learning systems also have this problem—a speech system trained on clean speech will perform very poorly on noisy speech, yet often be highly confident in its erroneous classifications (some of the authorshavepersonallyobservedthisintrainingautomaticspeechrecognitionsystems).",
    "Inthecase of our cleaning robot, harsh cleaning materials that it has found useful in cleaning factory floors could cause a lot of harm if used to clean an office.",
    "Or, an office might contain pets that the robot, neverhavingseenbefore,attemptstowashwithsoap,leadingtopredictablybadresults.",
    "Ingeneral, when the testing distribution differs from the training distribution, machine learning systems may not only exhibit poor performance, but also wrongly assume that their performance is good.",
    "Such errors can be harmful or offensive—a classifier could give the wrong medical diagnosis with such high confidence that the data isn’t flagged for human inspection, or a language model could output offensive text that it confidently believes is non-problematic.",
    "For autonomous agents acting in the world, there may be even greater potential for something bad to happen—for instance, an autonomous agent might overload a power grid because it incorrectly but confidently perceives that a particular region doesn’t have enough power, and concludes that more power is urgently needed and overload is unlikely.",
    "More broadly, any agent whose perception or heuristic reasoning processes arenottrainedonthecorrectdistributionmaybadlymisunderstanditssituation,andthusrunsthe risk of committing harmful actions that it does not realize are harmful.",
    "Additionally, safety checks that depend on trained machine learning systems (e.g. “does my visual system believe this route is clear?”) may fail silently and unpredictably if those systems encounter real-world data that differs sufficiently from their training data.",
    "Having a better way to detect such failures, and ultimately having statistical assurances about how often they’ll happen, seems critical to building safe and predictable systems.",
    "For concreteness, we imagine that a machine learning model is trained on one distribution (call it p ) but deployed on a potentially different test distribution (call it p∗).",
    "There are many other ways 0 to formalize this problem (for instance, in an online learning setting with concept drift [70, 54]) but we will focus on the above for simplicity.",
    "An important point is that we likely have access to a large amount of labeled data at training time, but little or no labeled data at test time.",
    "Our goal is to ensure that the model “performs reasonably” on p∗, in the sense that (1) it often performs well on p∗, and (2) it knows when it is performing badly (and ideally can avoid/mitigate the bad performance by taking conservative actions or soliciting human input).",
    "Thereareavarietyofareasthatarepotentiallyrelevanttothisproblem,includingchangedetection and anomaly detection",
    "[21, 80, 91], hypothesis testing [145], transfer learning [138, 124, 125, 25], and several others [136, 87, 18, 122, 121, 74, 147].",
    "Rather than fully reviewing all of this work in detail (which would necessitate a paper in itself), we will describe a few illustrative approaches and lay out some of their relative strengths and challenges.",
    "16",
    "Well-specifiedmodels: covariateshiftandmarginallikelihood.",
    "Ifwespecializetoprediction tasks and let x denote the input and y denote the output (prediction target), then one possibility is to make the covariate shift assumption that p (y|x) = p∗(y|x).",
    "In this case, assuming that we 0 canmodelp (x)andp∗(x)well,wecanperformimportanceweightingbyre-weightingeachtraining 0 example(x,y)byp∗(x)/p (x)[138,124].Thentheimportance-weightedsamplesallowustoestimate 0",
    "the performance on p∗, and even re-train a model to perform well on p∗. This approach is limited by the variance of the importance estimate, which is very large or even infinite unless p and p∗ are 0 close together.",
    "Analternativetosamplere-weightinginvolvesassumingawell-specifiedmodelfamily,inwhichcase there is a single optimal model for predicting under both p and p∗. In this case, one need only 0 heed finite-sample variance in the estimated model [25, 87].",
    "A limitation to this approach, at least currently,isthatmodelsareoftenmis-specifiedinpractice.",
    "However,thiscouldpotentiallybeover- come by employing highly expressive model families such as reproducing kernel Hilbert spaces [72], Turing machines",
    "[143, 144], or sufficiently expressive neural nets [64, 79].",
    "In the latter case, there has been interesting recent work on using bootstrapping to estimate finite-sample variation in the learned parameters of a neural network [114]; it seems worthwhile to better understand whether this approach can be used to effectively estimate out-of-sample performance in practice, as well as how local minima, lack of curvature, and other peculiarities relative to the typical setting of the bootstrap",
    "[47] affect the validity of this approach.",
    "All of the approaches so far rely on the covariate shift assumption, which is very strong and is also untestable; the latter property is particularly problematic from a safety perspective, since it couldleadtosilentfailuresinamachinelearningsystem.",
    "Anotherapproach,whichdoesnotrelyon covariateshift,buildsagenerativemodelofthedistribution.",
    "Ratherthanassumingthatp(x)changes while p(y|x) stays the same, we are free to assume other invariants (for instance, that p(y) changes but p(x|y) stays the same, or that certain conditional independencies are preserved).",
    "An advantage is that such assumptions are typically more testable than the covariate shift assumption (since they do not only involve the unobserved variable y).",
    "A disadvantage is that generative approaches are even more fragile than discriminative approaches in the presence of model mis-specification — for instance, there is a large empirical literature showing that generative approaches to semi-supervised learning based on maximizing marginal likelihood can perform very poorly when the model is mis- specified [98, 110, 35, 90, 88].",
    "Theapproachesdiscussedaboveallrelyrelativelystronglyonhavingawell-specifiedmodelfamily— onethatcontainsthetruedistributionortrueconcept.",
    "Thiscanbeproblematicinmanycases,since nature is often more complicated than our model family is capable of capturing.",
    "As noted above, it may be possible to mitigate this with very expressive models, such as kernels, Turing machines, or very large neural networks, but even here there is at least some remaining problem: for example, evenifourmodelfamilyconsistsofallTuringmachines,givenanyfiniteamountofdatawecanonly actually learn among Turing machines up to a given description length, and if the Turing machine describing nature exceeds this length, we are back to the mis-specified regime (alternatively, nature might not even be describable by a Turing machine).",
    "Partially specified models: method of moments, unsupervised risk estimation, causal identification, and limited-information maximum likelihood.",
    "Another approach is to take forgrantedthatconstructingafullywell-specifiedmodelfamilyisprobablyinfeasible,andtodesign methods that perform well despite this fact.",
    "This leads to the idea of partially specified models — models for which assumptions are made about some aspects of a distribution, but for which we are agnosticormakelimitedassumptionsaboutotheraspects.",
    "Forasimpleexample, consideravariant of linear regression where we might assume that y = (cid:104)w∗,x(cid:105)+v, where E[v|x] = 0, but we don’t make any further assumptions about the distributional form of the noise v. It turns out that this is already enough to identify the parameters w∗, and that these parameters will minimize the squared 17 prediction error even if the distribution over x changes.",
    "What is interesting about this example is that w∗ can be identified even with an incomplete (partial) specification of the noise distribution.",
    "This insight can be substantially generalized, and is one of the primary motivations for the gen- eralized method of moments in econometrics",
    "[68, 123, 69].",
    "The econometrics literature has in fact developed a large family of tools for handling partial specification, which also includes limited- information maximum likelihood and instrumental variables",
    "[10, 11, 133, 132].",
    "Returning to machine learning, the method of moments has recently seen a great deal of success for useintheestimationoflatentvariablemodels[9].Whilethecurrentfocusisonusingthemethodof moments to overcome non-convexity issues, it can also offer a way to perform unsupervised learning while relying only on conditional independence assumptions, rather than the strong distributional assumptions underlying maximum likelihood learning [147].",
    "Finally, some recent work in machine learning focuses only on modeling the distribution of errors of a model, which is sufficient for determining whether a model is performing well or poorly.",
    "Formally, the goal is to perform unsupervised risk estimation — given a model and unlabeled data from a test distribution, estimate the labeled risk of the model.",
    "This formalism, introduced by [44], has the advantage of potentially handling very large changes between train and test — even if the test distribution looks completely different from the training distribution and we have no hope of outputtingaccuratepredictions,unsupervisedriskestimationmaystillbepossible,asinthiscasewe would only need to output a large estimate for the risk.",
    "As in [147], one can approach unsupervised riskestimationbypositingcertainconditionalindependenciesinthedistributionoferrors,andusing this to estimate the error distribution from unlabeled data [39, 170, 121, 74].",
    "Instead of assuming independence, another assumption is that the errors are Gaussian conditioned on the true output y, in which case estimating the risk reduces to estimating a Gaussian mixture model",
    "[18].",
    "Because thesemethodsfocusonlyonthemodelerrorsandignoreotheraspectsofthedatadistribution,they can also be seen as an instance of partial model specification.",
    "Training on multiple distributions.",
    "One could also train on multiple training distributions in thehopethatamodelwhichsimultaneouslyworkswellonmanytrainingdistributionswillalsowork well on a novel test distribution.",
    "One of the authors has found this to be the case, for instance, in the context of automated speech recognition systems",
    "[7].",
    "One could potentially combine this withanyoftheideasabove,and/ortakeanengineeringapproachofsimplytryingtodevelopdesign methodologiesthatconsistentlyallowonetocollectarepresentativesetoftrainingsetsandfromthis buildamodelthatconsistentlygeneralizestonoveldistributions.",
    "Evenforthisengineeringapproach, itseemsimportanttobeabletodetectwhenoneisinasituationthatwasnotcoveredbythetraining dataandtorespondappropriately,andtohavemethodologiesforadequatelystress-testingthemodel with distributions that are sufficiently different from the set of training distributions.",
    "How to respond when out-of-distribution.",
    "Theapproachesdescribedabovefocusondetecting when a model is unlikely to make good predictions on a new distribution.",
    "An important related questioniswhattodooncethedetectionoccurs.",
    "Onenaturalapproachwouldbetoaskhumansfor information, though in the context of complex structured output tasks it may be unclear a priori what question to ask, and in time-critical situations asking for information may not be an option.",
    "For the former challenge, there has been some recent promising work on pinpointing aspects of a structure that a model is uncertain about [162, 81], as well as obtaining calibration in structured outputsettings[83],butwebelievethereismuchworkyettobedone.",
    "Forthelatterchallenge,there is also relevant work based on reachability analysis [93, 100] and robust policy improvement [164], which provide potential methods for deploying conservative policies in situations of uncertainty; to our knowledge, this work has not yet been combined with methods for detecting out-of-distribution failures of a model.",
    "Beyondthestructuredoutputsetting,foragentsthatcanactinanenvironment(suchasRLagents), 18 information about the reliability of percepts in uncertain situations seems to have great potential value.",
    "In sufficiently rich environments, these agents may have the option to gather information that clarifies the percept (e.g. if in a noisy environment, move closer to the speaker), engage in low- stakes experimentation when uncertainty is high (e.g. try a potentially dangerous chemical reaction in a controlled environment), or seek experiences that are likely to help expose the perception system to the relevant distribution (e.g. practice listening to accented speech).",
    "Humans utilize such informationroutinely,buttoourknowledgecurrentRLtechniquesmakelittleefforttodoso,perhaps because popular RL environments are typically not rich enough to require such subtle management of uncertainty.",
    "Properly responding to out-of-distribution information thus seems to the authors like an exciting and (as far as we are aware) mostly unexplored challenge for next generation RL systems.",
    "A unifying view: counterfactual reasoning and machine learning with contracts.",
    "Some of the authors have found two viewpoints to be particularly helpful when thinking about problems related to out-of-distribution prediction.",
    "The first is counterfactual reasoning",
    "[106, 129, 117, 30], where one asks “what would have happened if the world were different in a certain way”?",
    "In some sense, distributional shift can be thought of as a particular type of counterfactual, and so understanding counterfactual reasoning is likely to help in making systems robust to distributional shift.",
    "We are excited by recent work applying counterfactual reasoning techniques to machine learning problems [30, 120, 151, 160, 77, 137] though there appears to be much work remaining to be done to scale these to high-dimensional and highly complex settings.",
    "The second perspective is machine learning with contracts — in this perspective, one would like to construct machine learning systems that satisfy a well-defined contract on theirbehavior in analogy with the design of software systems",
    "[135, 28, 89].",
    "[135] enumerates a list of ways in which existing machine learning systems fail to do this, and the problems this can cause for deployment and maintenance of machine learning systems at scale.",
    "The simplest and to our mind most important failureistheextremelybrittleimplicitcontractinmostmachinelearningsystems,namelythatthey only necessarily perform well if the training and test distributions are identical.",
    "This condition is difficult to check and rare in practice, and it would be valuable to build systems that perform well underweakercontractsthatareeasiertoreasonabout.",
    "Partiallyspecifiedmodelsofferoneapproach to this — rather than requiring the distributions to be identical, we only need them to match on the pieces of the distribution that are specified in the model.",
    "Reachability analysis [93, 100] and modelrepair[58]provideotheravenuesforobtainingbettercontracts—inreachabilityanalysis,we optimize performance subject to the condition that a safe region can always be reached by a known conservative policy, and in model repair we alter a trained model to ensure that certain desired safety properties hold.",
    "Summary.",
    "There are a variety of approaches to building machine learning systems that robustly perform well when deployed on novel test distributions.",
    "One family of approaches is based on assuming a well-specified model; in this case, the primary obstacles are the difficulty of building well-specified models in practice, an incomplete picture of how to maintain uncertainty on novel distributions in the presence of finite training data, and the difficulty of detecting when a model is mis-specified.",
    "Another family of approaches only assumes a partially specified model; this approach ispotentiallypromising,butitcurrentlysuffersfromalackofdevelopmentinthecontextofmachine learning,sincemostofthehistoricaldevelopmenthasbeenbythefieldofeconometrics; thereisalso a question of whether partially specified models are fundamentally constrained to simple situations and/or conservative predictions, or whether they can meaningfully scale to the complex situations demanded by modern machine learning applications.",
    "Finally, one could try to train on multiple training distributions in the hope that a model which simultaneously works well on many training distributions will also work well on a novel test distribution; for this approach it seems particularly important to stress-test the learned model with distributions that are substantially different from 19 any in the set of training distributions.",
    "In addition, it is probably still important to be able to predict when inputs are too novel to admit good predictions.",
    "Potential Experiments: Speech systems frequently exhibit poor calibration when they go out-of- distribution, so a speech system that “knows when it is uncertain” could be one possible demon- stration project.",
    "To be specific, the challenge could be: train a state-of-the-art speech system on a standard dataset [116] that gives well-calibrated results (if not necessarily good results) on a range of other test sets, like noisy and accented speech.",
    "Current systems not only perform poorly on these test sets when trained only on small datasets, but are usually overconfident in their incorrect transcriptions.",
    "Fixingthisproblemwithoutharmingperformanceontheoriginaltrainingsetwould be a valuable achievement, and would obviously have practical value.",
    "More generally, it would be valuable to design models that could consistently estimate (bounds on) their performance on novel test distributions.",
    "If a single methodology could consistently accomplish this for a wide variety of tasks (including not just speech but e.g. sentiment analysis [24], as well as benchmarks in computer vision [158]), that would inspire confidence in the reliability of that methodology for handling novel inputs.",
    "Note that estimating performance on novel distributions has additional practical value in allowing us to then potentially adapt the model to that new situation.",
    "Finally, it might also be valuabletocreateanenvironmentwhereanRLagentmustlearntointerpretspeechaspartofsome larger task, and to explore how to respond appropriately to its own estimates of its transcription error.",
    "8 Related Efforts As mentioned in the introduction, several other communities have thought broadly about the safety ofAIsystems,bothwithinandoutsideofthemachinelearningcommunity.",
    "Workwithinthemachine learningcommunityonaccidentsinparticularwasdiscussedindetailabove,buthereweverybriefly highlight a few other communities doing work that is broadly related to the topic of AI safety.",
    "•",
    "Cyber-Physical Systems Community: An existing community of researchers studies the security and safety of systems that interact with the physical world.",
    "Illustrative of this work is an impressive and successful effort to formally verify the entire federal aircraft collision avoidance system",
    "[75, 92].",
    "Similar work includes traffic control algorithms",
    "[101] and many other topics.",
    "However, to date this work has not focused much on modern machine learning systems, where formal verification is often not feasible.",
    "• Futurist Community: A cross-disciplinary group of academics and non-profits has raised concern about the long term implications of AI",
    "[27, 167], particularly superintelligent AI.",
    "The Future of Humanity Institute has studied this issue particularly as it relates to future AI sys- tems learning or executing humanity’s preferences [48, 43, 14, 12].",
    "The Machine Intelligence ResearchInstitutehasstudiedsafetyissuesthatmayariseinveryadvancedAI[57,56,36,154, 142], including a few mentioned above (e.g., wireheading, environmental embedding, counter- factual reasoning), albeit at a more philosophical level.",
    "To date, they have not focused much on applications to modern machine learning.",
    "By contrast, our focus is on the empirical study of practical safety problems in modern machine learning systems, which we believe is likely to be robustly useful across a broad variety of potential risks, both short- and long-term.",
    "•",
    "Other Calls for Work on Safety: There have been other public documents within the research community pointing out the importance of work on AI safety.",
    "A 2015 Open Letter",
    "[8] signed by many members of the research community states the importance of “how to reap [AI’s] benefits while avoiding the potential pitfalls.”",
    "[130] propose research priorities for 20 robust and beneficial artificial intelligence, and includes several other topics in addition to a (briefer) discussion of AI-related accidents.",
    "[161], writing over 20 years ago, proposes that the community look for ways to formalize Asimov’s first law of robotics (robots must not harm humans), and focuses mainly on classical planning.",
    "Finally, two of the authors of this paper have written informally about safety in AI systems",
    "[146, 34]; these postings provided inspiration for parts of the present document.",
    "• Related Problems in Safety: A number of researchers in machine learning and other fields have begun to think about the social impacts of AI technologies.",
    "Aside from work directly on accidents (which we reviewed in the main document), there is also substantial work on other topics, many of which are closely related to or overlap with the issue of accidents.",
    "A thorough overview of all of this work is beyond the scope of this document, but we briefly list a few emerging themes: • Privacy: How can we ensure privacy when applying machine learning to sensitive data sources such as medical data?",
    "[76, 1] • Fairness: How can we make sure ML systems don’t discriminate?",
    "[3, 168, 6, 46, 119, 169] •",
    "Security: What can a malicious adversary do to a ML system?",
    "[149, 96, 97, 115, 108, 19] •",
    "Abuse:5 How do we prevent the misuse of ML systems to attack or harm people?",
    "[16] •",
    "Transparency: How can we understand what complicated ML systems are doing?",
    "[112, 166, 105, 109] • Policy: How do we predict and respond to the economic and social consequences of ML?",
    "[32, 52, 15, 33] Webelievethatresearchonthesetopicshasbothurgencyandgreatpromise,andthatfruitful intersection is likely to exist between these topics and the topics we discuss in this paper.",
    "9 Conclusion This paper analyzed the problem of accidents in machine learning systems and particularly rein- forcement learning agents, where an accident is defined as unintended and harmful behavior that mayemergefrompoordesignofreal-worldAIsystems.",
    "Wepresentedfivepossibleresearchproblems related to accident risk and for each we discussed possible approaches that are highly amenable to concrete experimental work.",
    "With the realistic possibility of machine learning-based systems controlling industrial processes, health-related systems, and other mission-critical technology, small-scale accidents seem like a very concretethreat,andarecriticaltopreventbothintrinsicallyandbecausesuchaccidentscouldcause a justified loss of trust in automated systems.",
    "The risk of larger accidents is more difficult to gauge, butwebelieveitisworthwhileandprudenttodevelopaprincipledandforward-lookingapproachto safetythatcontinuestoremainrelevantasautonomoussystemsbecomemorepowerful.",
    "Whilemany current-day safety problems can and have been handled with ad hoc fixes or case-by-case rules, we believe that the increasing trend towards end-to-end, fully autonomous systems points towards the need for a unified approach to prevent these systems from causing unintended harm.",
    "5Note that “security” differs from “abuse” in that the former involves attacks against a legitimate ML system by an adversary (e.g. a criminal tries to fool a face recognition system), while the latter involves attacks by an ML systemcontrolledbyanadversary(e.g. acriminaltrainsa“smarthacker”systemtobreakintoawebsite).",
    "21 Acknowledgements WethankShaneLegg,PeterNorvig,IlyaSutskever,GregCorrado,LaurentOrseau,DavidKrueger, Rif Saurous, David Andersen, and Victoria Krakovna for detailed feedback and suggestions.",
    "We would also like to thank Geoffrey Irving, Toby Ord, Quoc Le, Greg Wayne, Daniel Dewey, Nick Beckstead, Holden Karnofsky, Chelsea Finn, Marcello Herreshoff, Alex Donaldson, Jared Kaplan, Greg Brockman, Wojciech Zaremba, Ian Goodfellow, Dylan Hadfield-Menell, Jessica Taylor, Blaise Aguera y Arcas, David Berlekamp, Aaron Courville, and Jeff Dean for helpful discussions and comments.",
    "Paul Christiano was supported as part of the Future of Life Institute FLI-RFP-AI1 program, grant #2015–143898.",
    "In addition a minority of the work done by Paul Christiano was performed as a contractor for Theiss Research and at OpenAI.",
    "Finally, we thank the Google Brain team for providing a supportive environment and encouraging us to publish this work.",
    "References [1] Martin Abadi et al.",
    "“Deep Learning with Differential Privacy”.",
    "In: (in press (2016)).",
    "[2] Pieter Abbeel and Andrew Y Ng.",
    "“Exploration and apprenticeship learning in reinforcement learning”.",
    "In: Proceedings of the 22nd international conference on Machine learning.",
    "ACM. 2005, pp.",
    "1–8.",
    "[3] Julius Adebayo, Lalana Kagal, and Alex Pentland.",
    "The Hidden Cost of Efficiency: Fairness and Discrimination in Predictive Modeling. 2015.",
    "[4] Alekh Agarwal et al.",
    "“Taming the monster: A fast and simple algorithm for contextual ban- dits”.",
    "In: (2014).",
    "[5] HanaAjakanetal.“Domain-adversarialneuralnetworks”.",
    "In:arXivpreprintarXiv:1412.4446 (2014).",
    "[6] Ifeoma Ajunwa et al.",
    "“Hiring by algorithm: predicting and preventing disparate impact”.",
    "In: Available at SSRN 2746078 (2016).",
    "[7] Dario Amodei et al.",
    "“Deep Speech 2: End-to-End Speech Recognition in English and Man- darin”.",
    "In: arXiv preprint arXiv:1512.02595 (2015).",
    "[8] An Open Letter: Research Priorities for Robust and Beneficial Artificial Intelligence.",
    "Open Letter.",
    "Signed by 8,600 people; see attached research agenda. 2015.",
    "[9] Animashree Anandkumar, Daniel Hsu, and Sham M Kakade.",
    "“A method of moments for mixture models and hidden Markov models”.",
    "In: arXiv preprint arXiv:1203.0683 (2012).",
    "[10] TheodoreWAndersonandHermanRubin.",
    "“Estimationoftheparametersofasingleequation in a complete system of stochastic equations”.",
    "In: The Annals of Mathematical Statistics (1949), pp.",
    "46–63.",
    "[11] Theodore W Anderson and Herman Rubin.",
    "“The asymptotic properties of estimates of the parametersofasingleequationinacompletesystemofstochasticequations”.",
    "In:TheAnnals of Mathematical Statistics (1950), pp.",
    "570–582.",
    "[12] Stuart Armstrong.",
    "“Motivated value selection for artificial agents”.",
    "In: Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence. 2015.",
    "[13] Stuart Armstrong.",
    "The mathematics of reduced impact: help needed.",
    "2012.",
    "[14] Stuart Armstrong.",
    "Utility indifference.",
    "Tech.",
    "rep. Technical Report 2010-1.",
    "Oxford:",
    "Future of Humanity Institute, University of Oxford, 2010.",
    "[15] Melanie Arntz, Terry Gregory, and Ulrich Zierahn.",
    "“The Risk of Automation for Jobs in OECD Countries”.",
    "In: OECD Social, Employment and Migration Working Papers (2016).",
    "url: http://dx.doi.org/10.1787/5jlz9h56dvq7-en.",
    "[16] AutonomousWeapons:AnOpenLetterfromAI&RoboticsResearchers.",
    "OpenLetter.Signed by 20,000+ people.",
    "2015.",
    "22",
    "[17] James Babcock, Janos Kramar, and Roman Yampolskiy.",
    "“The AGI Containment Problem”.",
    "In: The Ninth Conference on Artificial General Intelligence (2016).",
    "[18] Krishnakumar Balasubramanian, Pinar Donmez, and Guy Lebanon.",
    "“Unsupervised super- vised learning ii: Margin-based classification without labels”.",
    "In: The Journal of Machine Learning Research 12 (2011), pp.",
    "3119–3145.",
    "[19] Marco Barreno et al.",
    "“The security of machine learning”.",
    "In: Machine Learning 81.2 (2010), pp.",
    "121–148.",
    "[20] Tamer Ba¸sar and Pierre Bernhard.",
    "H-infinity optimal control and related minimax design problems: a dynamic game approach.",
    "Springer Science & Business Media, 2008.",
    "[21] Mich`ele Basseville.",
    "“Detecting changes in signals and systems—a survey”.",
    "In: Automatica 24.3 (1988), pp.",
    "309–326.",
    "[22] F Berkenkamp, A Krause, and Angela P Schoellig.",
    "“Bayesian optimization with safety con- straints: safe and automatic parameter tuning in robotics.” arXiv, 2016”.",
    "In: arXiv preprint arXiv:1602.04450 ().",
    "[23] JonBirdandPaulLayzell.",
    "“Theevolvedradioanditsimplicationsformodellingtheevolution of novel sensors”.",
    "In: Evolutionary Computation, 2002.",
    "CEC’02.",
    "Proceedings of the 2002 Congress on.",
    "Vol. 2. IEEE.",
    "2002, pp.",
    "1836–1841.",
    "[24] John Blitzer, Mark Dredze, Fernando Pereira, et al.",
    "“Biographies, bollywood, boom-boxes andblenders:Domainadaptationforsentimentclassification”.",
    "In:ACL.Vol.7.2007,pp.440– 447.",
    "[25] John Blitzer, Sham Kakade, and Dean P Foster.",
    "“Domain adaptation with coupled sub- spaces”.",
    "In: International Conference on Artificial Intelligence and Statistics.",
    "2011, pp.",
    "173– 181.",
    "[26] CharlesBlundelletal.",
    "“Weightuncertaintyinneuralnetworks”.",
    "In:arXivpreprintarXiv:1505.05424 (2015).",
    "[27] Nick Bostrom.",
    "Superintelligence: Paths, dangers, strategies.",
    "OUP Oxford, 2014.",
    "[28] L´eon Bottou.",
    "“Two high stakes challenges in machine learning”.",
    "Invited talk at the 32nd International Conference on Machine Learning. 2015.",
    "[29] L´eon Bottou et al.",
    "“Counterfactual Reasoning and Learning Systems”.",
    "In: arXiv preprint arXiv:1209.2355 (2012).",
    "[30] L´eon Bottou et al.",
    "“Counterfactual reasoning and learning systems: The example of compu- tational advertising”.",
    "In: The Journal of Machine Learning Research 14.1 (2013), pp. 3207– 3260.",
    "[31] Ronen I Brafman and Moshe Tennenholtz.",
    "“R-max-a general polynomial time algorithm for near-optimal reinforcement learning”.",
    "In: The Journal of Machine Learning Research 3 (2003), pp.",
    "213–231.",
    "[32] Erik Brynjolfsson and Andrew McAfee.",
    "The second machine age: work, progress, and pros- perity in a time of brilliant technologies.",
    "WW Norton & Company, 2014.",
    "[33] Ryan Calo.",
    "“Open robotics”.",
    "In: Maryland Law Review 70.3 (2011).",
    "[34] Paul Christiano.",
    "AI Control.",
    "[Online; accessed 13-June-2016].",
    "2015.",
    "url: https://medium.",
    "com/ai-control.",
    "[35] FabioCozmanandIraCohen.",
    "“Risksofsemi-supervisedlearning”.",
    "In:Semi-SupervisedLearn- ing (2006), pp.",
    "56–72.",
    "[36] Andrew Critch.",
    "“Parametric Bounded L¨ob’s Theorem and Robust Cooperation of Bounded Agents”.",
    "In: (2016).",
    "[37] Christian Daniel et al.",
    "“Active reward learning”.",
    "In: Proceedings of Robotics Science & Sys- tems.",
    "2014.",
    "[38] ErnestDavis.“Ethicalguidelinesforasuperintelligence.",
    "”In:",
    "Artif.Intell.220(2015),pp.121– 124.",
    "[39] Alexander Philip Dawid and Allan M Skene.",
    "“Maximum likelihood estimation of observer error-rates using the EM algorithm”.",
    "In: Applied statistics (1979), pp.",
    "20–28. 23",
    "[40] PeterDayanandGeoffreyEHinton.",
    "“Feudalreinforcementlearning”.",
    "In:Advances in neural information processing systems.",
    "Morgan Kaufmann Publishers.",
    "1993, pp.",
    "271–271.",
    "[41] Kalyanmoy Deb.",
    "“Multi-objective optimization”.",
    "In: Search methodologies.",
    "Springer, 2014, pp.",
    "403–449.",
    "[42] Daniel Dewey.",
    "“Learning what to value”.",
    "In: Artificial General Intelligence. Springer, 2011, pp.",
    "309–314.",
    "[43] DanielDewey.",
    "“Reinforcementlearningandtherewardengineeringprinciple”.In:2014AAAI",
    "Spring Symposium Series.",
    "2014.",
    "[44] Pinar Donmez, Guy Lebanon, and Krishnakumar Balasubramanian.",
    "“Unsupervised super- visedlearningi:Estimatingclassificationandregressionerrorswithoutlabels”.",
    "In:The Jour- nal of Machine Learning Research 11 (2010), pp.",
    "1323–1351.",
    "[45] GregoryDruck,GideonMann,andAndrewMcCallum.“Learningfromlabeledfeaturesusing generalizedexpectationcriteria”.",
    "In:Proceedingsofthe31stannualinternationalACMSIGIR conference on Research and development in information retrieval. ACM. 2008, pp.",
    "595–602.",
    "[46] Cynthia Dwork et al.",
    "“Fairness through awareness”.",
    "In: Proceedings of the 3rd Innovations in Theoretical Computer Science Conference. ACM.",
    "2012, pp.",
    "214–226.",
    "[47] BradleyEfron.“Computersandthetheoryofstatistics:thinkingtheunthinkable”.",
    "In:SIAM review 21.4 (1979), pp. 460–480.",
    "[48] Owain Evans, Andreas Stuhlmu¨ller, and Noah D Goodman.",
    "“Learning the preferences of ignorant, inconsistent agents”.",
    "In: arXiv preprint arXiv:1512.05832 (2015).",
    "[49] Tom Everitt and Marcus Hutter.",
    "“Avoiding wireheading with value reinforcement learning”.",
    "In: arXiv preprint arXiv:1605.03143 (2016).",
    "[50] Tom Everitt et al.",
    "“Self-Modification of Policy and Utility Function in Rational Agents”.",
    "In: arXiv preprint arXiv:1605.03142 (2016).",
    "[51] Chelsea Finn, Sergey Levine, and Pieter Abbeel.",
    "“Guided Cost Learning: Deep Inverse Op- timal Control via Policy Optimization”.",
    "In: arXiv preprint arXiv:1603.00448 (2016).",
    "[52] CarlBenediktFreyandMichaelAOsborne.",
    "“Thefutureofemployment:howsusceptibleare jobs to computerisation”.",
    "In: Retrieved September 7 (2013), p. 2013.",
    "[53] Yarin Gal and Zoubin Ghahramani.",
    "“Dropout as a Bayesian approximation: Representing model uncertainty in deep learning”.",
    "In: arXiv preprint arXiv:1506.02142 (2015).",
    "[54] JoaoGamaetal.“Learningwithdriftdetection”.",
    "In:Advancesinartificialintelligence–SBIA 2004.",
    "Springer, 2004, pp.",
    "286–295.",
    "[55] Javier Garc´ıa and Fernando Fern´andez.",
    "“A Comprehensive Survey on Safe Reinforcement Learning”.",
    "In: Journal of Machine Learning Research 16 (2015), pp. 1437–1480.",
    "[56] Scott Garrabrant, Nate Soares, and Jessica Taylor.",
    "“Asymptotic Convergence in Online Learning with Unbounded Delays”.",
    "In: arXiv preprint arXiv:1604.05280 (2016).",
    "[57] Scott Garrabrant et al.",
    "“Uniform Coherence”.",
    "In: arXiv preprint arXiv:1604.05288 (2016).",
    "[58] Shalini Ghosh et al.",
    "“Trusted Machine Learning for Probabilistic Models”.",
    "In: Reliable Ma- chine Learning in the Wild at ICML 2016 (2016).",
    "[59] YolandaGiletal.“Amplifyscientificdiscoverywithartificialintelligence”.",
    "In:Science346.6206 (2014), pp.",
    "171–172.",
    "[60] Alec Go, Richa Bhayani, and Lei Huang.",
    "“Twitter sentiment classification using distant supervision”.",
    "In: CS224N Project Report, Stanford 1 (2009), p. 12.",
    "[61] Ian Goodfellow et al.",
    "“Generative adversarial nets”.",
    "In: Advances in Neural Information Processing Systems.",
    "2014, pp.",
    "2672–2680.",
    "[62] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.",
    "“Explaining and harnessing ad- versarial examples”.",
    "In: arXiv preprint arXiv:1412.6572 (2014).",
    "[63] Charles AE Goodhart.",
    "Problems of monetary management: the UK experience.",
    "Springer, 1984.",
    "[64] Alex Graves, Greg Wayne, and Ivo Danihelka.",
    "“Neural turing machines”.",
    "In: arXiv preprint arXiv:1410.5401 (2014).",
    "24 [65] Sonal Gupta.",
    "“Distantly Supervised Information Extraction Using Bootstrapped Patterns”.",
    "PhD thesis.",
    "Stanford University, 2015.",
    "[66] Dylan Hadfield-Menell et al.",
    "Cooperative Inverse Reinforcement Learning.",
    "2016.",
    "[67] Dylan Hadfield-Menell et al.",
    "“The Off-Switch”.",
    "In: (2016).",
    "[68] LarsPeterHansen.",
    "“Largesamplepropertiesofgeneralizedmethodofmomentsestimators”.",
    "In: Econometrica: Journal of the Econometric Society (1982), pp.",
    "1029–1054.",
    "[69] Lars Peter Hansen.",
    "“Nobel Lecture: Uncertainty Outside and Inside Economic Models”.",
    "In: Journal of Political Economy 122.5 (2014), pp.",
    "945–987.",
    "[70] MarkHerbsterandManfredKWarmuth.",
    "“Trackingthebestlinearpredictor”.",
    "In:The Jour- nal of Machine Learning Research 1 (2001), pp.",
    "281–309.",
    "[71] Bill Hibbard.",
    "“Model-based utility functions”.",
    "In: Journal of Artificial General Intelligence 3.1 (2012), pp.",
    "1–24.",
    "[72] ThomasHofmann,BernhardSch¨olkopf,andAlexanderJSmola.“Kernelmethodsinmachine learning”.",
    "In: The annals of statistics (2008), pp.",
    "1171–1220.",
    "[73] Garud N Iyengar.",
    "“Robust dynamic programming”.",
    "In: Mathematics of Operations Research 30.2 (2005), pp.",
    "257–280.",
    "[74] Ariel Jaffe, Boaz Nadler, and Yuval Kluger.",
    "“Estimating the accuracies of multiple classifiers without labeled data”.",
    "In: arXiv preprint arXiv:1407.7644 (2014).",
    "[75] Jean-Baptiste Jeannin et al.",
    "“A formally verified hybrid system for the next-generation air- bornecollisionavoidancesystem”.",
    "In:ToolsandAlgorithmsfortheConstructionandAnalysis of Systems.",
    "Springer, 2015, pp.",
    "21–36.",
    "[76] ZhanglongJi,ZacharyCLipton,andCharlesElkan.",
    "“Differentialprivacyandmachinelearn- ing: A survey and review”.",
    "In: arXiv preprint arXiv:1412.7584 (2014).",
    "[77] Fredrik D Johansson, Uri Shalit, and David Sontag.",
    "“Learning Representations for Counter- factual Inference”.",
    "In: arXiv preprint arXiv:1605.03661 (2016).",
    "[78] Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra.",
    "“Planning and acting in partially observable stochastic domains”.",
    "In: Artificial intelligence 101.1 (1998), pp. 99– 134.",
    "[79] L(cid:32) ukaszKaiserandIlyaSutskever.",
    "“NeuralGPUslearnalgorithms”.",
    "In:arXivpreprintarXiv:1511.08228 (2015).",
    "[80] Yoshinobu Kawahara and Masashi Sugiyama.",
    "“Change-Point Detection in Time-Series Data by Direct Density-Ratio Estimation.”",
    "In: SDM.",
    "Vol. 9. SIAM.",
    "2009, pp.",
    "389–400.",
    "[81] F.Khani,M.Rinard,andP.Liang.“UnanimousPredictionfor100LearningSemanticParsers”.",
    "In: Association for Computational Linguistics (ACL).",
    "2016.",
    "[82] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.",
    "“Imagenet classification with deep convolutionalneuralnetworks”.",
    "In:Advances in neural information processing systems.2012, pp.",
    "1097–1105.",
    "[83] Volodymyr Kuleshov and Percy S Liang.",
    "“Calibrated Structured Prediction”.",
    "In: Advances in Neural Information Processing Systems. 2015, pp. 3456–3464.",
    "[84] Tejas D Kulkarni et al.",
    "“Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation”.",
    "In: arXiv preprint arXiv:1604.06057 (2016).",
    "[85] Neil Lawrence.",
    "Discussion of ’Superintelligence: Paths, Dangers, Strategies’.",
    "2016.",
    "[86] Jesse Levinson et al.",
    "“Towards fully autonomous driving: Systems and algorithms”.",
    "In: In- telligent Vehicles Symposium (IV), 2011 IEEE.",
    "IEEE.",
    "2011, pp.",
    "163–168.",
    "[87] Lihong Li et al.",
    "“Knows what it knows: a framework for self-aware learning”.",
    "In: Machine learning 82.3 (2011), pp.",
    "399–443.",
    "[88] Yu-Feng Li and Zhi-Hua Zhou.",
    "“Towards making unlabeled data never hurt”.",
    "In: Pattern Analysis and Machine Intelligence, IEEE Transactions on 37.1 (2015), pp.",
    "175–188.",
    "[89] Percy Liang.",
    "“On the Elusiveness of a Specification for AI”.",
    "NIPS 2015, Symposium: Algo- rithms Among Us. 2015.",
    "url: http://research.microsoft.com/apps/video/default.",
    "aspx?id=260009&r=1. 25",
    "[90] Percy Liang and Dan Klein.",
    "“Analyzing the Errors of Unsupervised Learning.”",
    "In: ACL. 2008, pp.",
    "879–887.",
    "[91] Song Liu et al.",
    "“Change-point detection in time-series data by relative density-ratio estima- tion”.",
    "In: Neural Networks 43 (2013), pp.",
    "72–83.",
    "[92] Sarah M Loos, David Renshaw, and Andr´e Platzer.",
    "“Formal verification of distributed air- craft controllers”.",
    "In: Proceedings of the 16th international conference on Hybrid systems: computation and control.",
    "ACM. 2013, pp.",
    "125–130.",
    "[93] JohnLygeros,ClaireTomlin,andShankarSastry.“Controllersforreachabilityspecifications for hybrid systems”.",
    "In: Automatica 35.3 (1999), pp.",
    "349–370.",
    "[94] GideonSMannandAndrewMcCallum.",
    "“Generalizedexpectationcriteriaforsemi-supervised learningwithweaklylabeleddata”.",
    "In:TheJournalofMachineLearningResearch 11(2010), pp.",
    "955–984.",
    "[95] John McCarthy and Patrick J Hayes.",
    "“Some philosophical problems from the standpoint of artificial intelligence”.",
    "In: Readings in artificial intelligence (1969), pp.",
    "431–450.",
    "[96] Shike Mei and Xiaojin Zhu.",
    "“The Security of Latent Dirichlet Allocation.”",
    "In: AISTATS.",
    "2015.",
    "[97] Shike Mei and Xiaojin Zhu.",
    "“Using Machine Teaching to Identify Optimal Training-Set At- tacks on Machine Learners.”",
    "In: AAAI. 2015, pp.",
    "2871–2877.",
    "[98] Bernard Merialdo.",
    "“Tagging English text with a probabilistic model”.",
    "In: Computational linguistics 20.2 (1994), pp.",
    "155–171.",
    "[99] Mike Mintz et al.",
    "“Distant supervision for relation extraction without labeled data”.",
    "In: Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2- Volume 2. Association for Computational Linguistics.",
    "2009, pp.",
    "1003–1011.",
    "[100] Ian M Mitchell, Alexandre M Bayen, and Claire J Tomlin.",
    "“A time-dependent Hamilton-",
    "Jacobi formulation of reachable sets for continuous dynamic games”.",
    "In: Automatic Control, IEEE Transactions on 50.7 (2005), pp.",
    "947–957.",
    "[101] Stefan Mitsch, Sarah M Loos, and Andr´e Platzer.",
    "“Towards formal verification of freeway traffic control”.",
    "In: Cyber-Physical Systems (ICCPS), 2012 IEEE/ACM Third International Conference on. IEEE.",
    "2012, pp.",
    "171–180.",
    "[102] VolodymyrMnihetal.",
    "“Human-levelcontrolthroughdeepreinforcementlearning”.",
    "In:Nature 518.7540 (2015), pp.",
    "529–533.",
    "[103] Shakir Mohamed and Danilo Jimenez Rezende.",
    "“Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning”.",
    "In: Advances in Neural Information Pro- cessing Systems. 2015, pp.",
    "2116–2124.",
    "[104] Teodor Mihai Moldovan and Pieter Abbeel.",
    "“Safe exploration in markov decision processes”.",
    "In: arXiv preprint arXiv:1205.4810 (2012).",
    "[105] AlexanderMordvintsev,ChristopherOlah,andMikeTyka.",
    "“Inceptionism:",
    "Goingdeeperinto neural networks”.",
    "In: Google Research Blog.",
    "Retrieved June 20 (2015).",
    "[106] JerseyNeyman.",
    "“Surlesapplicationsdelath´eoriedesprobabilit´esauxexperiencesagricoles: Essai des principes”.",
    "In: Roczniki Nauk Rolniczych 10 (1923), pp.",
    "1–51.",
    "[107] Andrew Y Ng, Stuart J Russell, et al.",
    "“Algorithms for inverse reinforcement learning.”",
    "In: Icml. 2000, pp.",
    "663–670.",
    "[108] Anh Nguyen, Jason Yosinski, and Jeff Clune.",
    "“Deep neural networks are easily fooled: High confidence predictions for unrecognizable images”.",
    "In: Computer Vision and Pattern Recog- nition (CVPR), 2015 IEEE Conference on.",
    "IEEE.",
    "2015, pp.",
    "427–436.",
    "[109] AnhNguyenetal.",
    "“Synthesizingthepreferredinputsforneuronsinneuralnetworksviadeep generator networks”.",
    "In: arXiv preprint arXiv:1605.09304 (2016).",
    "[110] Kamal Nigam et al.",
    "“Learning to classify text from labeled and unlabeled documents”.",
    "In: AAAI/IAAI 792 (1998).",
    "26",
    "[111]",
    "Arnab Nilim and Laurent El Ghaoui.",
    "“Robust control of Markov decision processes with uncertain transition matrices”.",
    "In: Operations Research 53.5 (2005), pp.",
    "780–798.",
    "[112] ChristopherOlah.",
    "VisualizingRepresentations:DeepLearningandHumanBeings.2015.url: http://colah.github.io/posts/2015-01-Visualizing-Representations/.",
    "[113] Laurent Orseau and Stuart Armstrong.",
    "“Safely Interruptible Agents”.",
    "In: (2016).",
    "[114] IanOsbandetal.",
    "“DeepExplorationviaBootstrappedDQN”.",
    "In:arXivpreprintarXiv:1602.04621 (2016).",
    "[115] Nicolas Papernot et al.",
    "“Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples”.",
    "In: arXiv preprint arXiv:1602.02697 (2016).",
    "[116] Douglas B Paul and Janet M Baker.",
    "“The design for the Wall Street Journal-based CSR corpus”.",
    "In: Proceedings of the workshop on Speech and Natural Language.",
    "Association for Computational Linguistics.",
    "1992, pp.",
    "357–362.",
    "[117] JudeaPearletal.“Causalinferenceinstatistics:Anoverview”.",
    "In:StatisticsSurveys 3(2009), pp.",
    "96–146.",
    "[118] MartinPeckaandTomasSvoboda.",
    "“Safeexplorationtechniquesforreinforcementlearning–an overview”.",
    "In: Modelling and Simulation for Autonomous Systems.",
    "Springer, 2014, pp. 357– 375.",
    "[119] Dino Pedreshi, Salvatore Ruggieri, and Franco Turini.",
    "“Discrimination-aware data mining”.",
    "In: Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM. 2008, pp.",
    "560–568.",
    "[120] JonasPetersetal.",
    "“Causaldiscoverywithcontinuousadditivenoisemodels”.",
    "In:TheJournal of Machine Learning Research 15.1 (2014), pp.",
    "2009–2053.",
    "[121] Emmanouil Antonios Platanios.",
    "“Estimating accuracy from unlabeled data”.",
    "MA thesis.",
    "Carnegie Mellon University, 2015.",
    "[122] Emmanouil Antonios Platanios, Avrim Blum, and Tom Mitchell.",
    "“Estimating accuracy from unlabeled data”.",
    "In: (2014).",
    "[123] Walter W Powell and Laurel Smith-Doerr.",
    "“Networks and economic life”.",
    "In: The handbook of economic sociology 368 (1994), p. 380.",
    "[124] JoaquinQuinonero-Candelaetal.",
    "Dataset shift in machine learning, ser.",
    "Neural information processing series.",
    "2009.",
    "[125] Rajat Raina et al.",
    "“Self-taught learning: transfer learning from unlabeled data”.",
    "In: Proceed- ings of the 24th international conference on Machine learning.",
    "ACM.",
    "2007, pp.",
    "759–766.",
    "[126] Bharath Ramsundar et al.",
    "“Massively multitask networks for drug discovery”.",
    "In: arXiv preprint arXiv:1502.02072 (2015).",
    "[127] Mark Ring and Laurent Orseau.",
    "“Delusion, survival, and intelligent agents”.",
    "In: Artificial General Intelligence. Springer, 2011, pp.",
    "11–20.",
    "[128] St´ephaneRoss,GeoffreyJGordon,andJAndrewBagnell.",
    "“Areductionofimitationlearning and structured prediction to no-regret online learning”.",
    "In: arXiv preprint arXiv:1011.0686 (2010).",
    "[129] DonaldBRubin.",
    "“Estimatingcausaleffectsoftreatmentsinrandomizedandnonrandomized studies.”",
    "In: Journal of educational Psychology 66.5 (1974), p. 688.",
    "[130] Stuart Russell et al.",
    "“Research priorities for robust and beneficial artificial intelligence”.",
    "In: Future of Life Institute (2015).",
    "[131] ChristophSalge,CorneliusGlackin,andDanielPolani.",
    "“Empowerment–anintroduction”.",
    "In: Guided Self-Organization: Inception.",
    "Springer, 2014, pp.",
    "67–114.",
    "[132] J Denis Sargan.",
    "“The estimation of relationships with autocorrelated residuals by the use of instrumentalvariables”.",
    "In:JournaloftheRoyalStatisticalSociety.SeriesB(Methodological) (1959), pp.",
    "91–105.",
    "[133] John D Sargan.",
    "“The estimation of economic relationships using instrumental variables”.",
    "In: Econometrica: Journal of the Econometric Society (1958), pp.",
    "393–415. 27",
    "[134] John Schulman et al.",
    "“High-dimensional continuous control using generalized advantage es- timation”.",
    "In: arXiv preprint arXiv:1506.02438 (2015).",
    "[135] D Sculley et al.",
    "“Machine Learning: The High-Interest Credit Card of Technical Debt”.",
    "In: (2014).",
    "[136] Glenn Shafer and Vladimir Vovk.",
    "“A tutorial on conformal prediction”.",
    "In: The Journal of Machine Learning Research 9 (2008), pp.",
    "371–421.",
    "[137] UriShalit,FredrikJohansson,andDavidSontag.“BoundingandMinimizingCounterfactual Error”.",
    "In: arXiv preprint arXiv:1606.03976 (2016).",
    "[138] HidetoshiShimodaira.",
    "“Improvingpredictiveinferenceundercovariateshiftbyweightingthe log-likelihoodfunction”.",
    "In:Journalofstatisticalplanningandinference 90.2(2000),pp.227– 244.",
    "[139] Jaeho Shin et al.",
    "“Incremental knowledge base construction using deepdive”.",
    "In: Proceedings of the VLDB Endowment 8.11 (2015), pp. 1310–1321.",
    "[140] David Silver et al.",
    "“Mastering the game of Go with deep neural networks and tree search”.",
    "In: Nature 529.7587 (2016), pp.",
    "484–489.",
    "[141] SNESSuperMarioWorld(USA)“arbitrarycodeexecution”.",
    "Tool-assistedmovies.2014.url: http://tasvideos.org/2513M.html.",
    "[142] Nate Soares and Benja Fallenstein.",
    "“Toward idealized decision theory”.",
    "In: arXiv preprint arXiv:1507.01986 (2015).",
    "[143] Ray J Solomonoff.",
    "“A formal theory of inductive inference.",
    "Part I”.",
    "In: Information and control 7.1 (1964), pp.",
    "1–22.",
    "[144] Ray J Solomonoff.",
    "“A formal theory of inductive inference.",
    "Part II”.",
    "In: Information and control 7.2 (1964), pp.",
    "224–254.",
    "[145] J Steinebach.",
    "“EL Lehmann, JP Romano: Testing statistical hypotheses”.",
    "In: Metrika 64.2 (2006), pp.",
    "255–256.",
    "[146] Jacob Steinhardt.",
    "Long-Term and Short-Term Challenges to Ensuring the Safety of AI Sys- tems.",
    "[Online; accessed 13-June-2016].",
    "2015.",
    "url: https://jsteinhardt.wordpress.com/ 2015/06/24/long-term-and-short-term-challenges-to-ensuring-the-safety-of- ai-systems/.",
    "[147] Jacob Steinhardt and Percy Liang.",
    "“Unsupervised Risk Estimation with only Structural Assumptions”.",
    "In: (2016).",
    "[148] JacobSteinhardtandRussTedrake.",
    "“Finite-timeregionalverificationofstochasticnon-linear systems”.",
    "In: The International Journal of Robotics Research 31.7 (2012), pp. 901–923.",
    "[149] Jacob Steinhardt, Gregory Valiant, and Moses Charikar.",
    "“Avoiding Imposters and Delin- quents:AdversarialCrowdsourcingandPeerPrediction”.",
    "In:arxivprepringarXiv:1606.05374 (2016).",
    "url: http://arxiv.org/abs/1606.05374.",
    "[150] RichardSSuttonandAndrewGBarto.",
    "Reinforcement learning: An introduction.",
    "MITpress, 1998.",
    "[151] Adith Swaminathan and Thorsten Joachims.",
    "“Counterfactual risk minimization: Learning from logged bandit feedback”.",
    "In: arXiv preprint arXiv:1502.02362 (2015).",
    "[152] ChristianSzegedyetal.",
    "“Intriguingpropertiesofneuralnetworks”.",
    "In:arXivpreprintarXiv:1312.6199 (2013).",
    "[153] Aviv Tamar, Yonatan Glassner, and Shie Mannor.",
    "“Policy gradients beyond expectations: Conditional value-at-risk”.",
    "In: arXiv preprint arXiv:1404.3862 (2014).",
    "[154] Jessica Taylor.",
    "“Quantilizers: A Safer Alternative to Maximizers for Limited Optimization”.",
    "In: forthcoming).",
    "Submitted to AAAI (2016).",
    "[155] Matthew E Taylor and Peter Stone.",
    "“Transfer learning for reinforcement learning domains: A survey”.",
    "In: Journal of Machine Learning Research 10.Jul (2009), pp.",
    "1633–1685.",
    "[156] Philip S Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh.",
    "“High-Confidence Off-Policy Evaluation.”",
    "In: AAAI. 2015, pp.",
    "3000–3006.",
    "[157] Adrian Thompson.",
    "Artificial evolution in the physical world.",
    "1997.",
    "28",
    "[158] Antonio Torralba and Alexei A Efros.",
    "“Unbiased look at dataset bias”.",
    "In: Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on.",
    "IEEE.",
    "2011, pp.",
    "1521–1528.",
    "[159] Matteo Turchetta, Felix Berkenkamp, and Andreas Krause.",
    "“Safe Exploration in Finite Markov Decision Processes with Gaussian Processes”.",
    "In: arXiv preprint arXiv:1606.04753 (2016).",
    "[160] Stefan Wager and Susan Athey.",
    "“Estimation and Inference of Heterogeneous Treatment Ef- fects using Random Forests”.",
    "In: arXiv preprint arXiv:1510.04342 (2015).",
    "[161] DanielWeldandOrenEtzioni.“Thefirstlawofrobotics(acalltoarms)”.",
    "In:AAAI.Vol.94. 1994.",
    "1994, pp.",
    "1042–1047.",
    "[162] Keenon Werling et al.",
    "“On-the-job learning with bayesian decision theory”.",
    "In: Advances in Neural Information Processing Systems. 2015, pp.",
    "3447–3455.",
    "[163] JasonWestonetal.“Towardsai-completequestionanswering:Asetofprerequisitetoytasks”.",
    "In: arXiv preprint arXiv:1502.05698 (2015).",
    "[164] Wolfram Wiesemann, Daniel Kuhn, and Ber¸c Rustem.",
    "“Robust Markov decision processes”.",
    "In: Mathematics of Operations Research 38.1 (2013), pp.",
    "153–183.",
    "[165] RomanVYampolskiy.",
    "“Utilityfunctionsecurityinartificiallyintelligentagents”.",
    "In:Journal of Experimental & Theoretical Artificial Intelligence 26.3 (2014), pp.",
    "373–389.",
    "[166] Jason Yosinski et al.",
    "“Understanding neural networks through deep visualization”.",
    "In: arXiv preprint arXiv:1506.06579 (2015).",
    "[167] Eliezer Yudkowsky.",
    "“Artificial intelligence as a positive and negative factor in global risk”.",
    "In: Global catastrophic risks 1 (2008), p. 303.",
    "[168] Muhammad Bilal Zafar et al.",
    "“Learning Fair Classifiers”.",
    "In: stat 1050 (2015), p. 29.",
    "[169] RichardSZemeletal.",
    "“LearningFairRepresentations.”In:ICML(3)28(2013),pp.325–333.",
    "[170] Yuchen Zhang et al.",
    "“Spectral methods meet EM: A provably optimal algorithm for crowd- sourcing”.",
    "In: Advances in neural information processing systems.",
    "2014, pp.",
    "1260–1268. 29",
    "Deep Reinforcement Learning from Human Preferences PaulFChristiano JanLeike TomBBrown OpenAI DeepMind nottombrown@gmail.com",
    "paul@openai.com",
    "leike@google.com",
    "MiljanMartic ShaneLegg DarioAmodei DeepMind DeepMind OpenAI miljanm@google.com",
    "legg@google.com",
    "damodei@openai.com",
    "Abstract Forsophisticatedreinforcementlearning(RL)systemstointeractusefullywith real-worldenvironments,weneedtocommunicatecomplexgoalstothesesystems.",
    "Inthiswork,weexploregoalsdefinedintermsof(non-expert)humanpreferences betweenpairsoftrajectorysegments.",
    "Weshowthatthisapproachcaneffectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1% of our agent’s interactions with the environment.",
    "This reduces the cost of humanoversightfarenoughthatitcanbepracticallyappliedtostate-of-the-art RLsystems.",
    "Todemonstratetheflexibilityofourapproach,weshowthatwecan successfully train complex novel behaviors with about an hour of human time.",
    "Thesebehaviorsandenvironmentsareconsiderablymorecomplexthananywhich havebeenpreviouslylearnedfromhumanfeedback.",
    "1",
    "Introduction Recentsuccessinscalingreinforcementlearning(RL)tolargeproblemshasbeendrivenindomains thathaveawell-specifiedrewardfunction(Mnihetal.,2015,2016;Silveretal.,2016).",
    "Unfortunately, many tasks involve goals that are complex, poorly-defined, or hard to specify.",
    "Overcoming this limitation would greatly expand the possible impact of deep RL and could increase the reach of machinelearningmorebroadly.",
    "Forexample,supposethatwewantedtousereinforcementlearningtotrainarobottocleanatableor scrambleanegg.",
    "It’snotclearhowtoconstructasuitablerewardfunction,whichwillneedtobea functionoftherobot’ssensors.",
    "Wecouldtrytodesignasimplerewardfunctionthatapproximately captures the intended behavior, but this will often result in behavior that optimizes our reward functionwithoutactuallysatisfyingourpreferences.",
    "Thisdifficultyunderliesrecentconcernsabout misalignmentbetweenourvaluesandtheobjectivesofourRLsystems(Bostrom,2014;Russell, 2016;Amodeietal.,2016).",
    "Ifwecouldsuccessfullycommunicateouractualobjectivestoouragents, itwouldbeasignificantsteptowardsaddressingtheseconcerns.",
    "If we have demonstrations of the desired task, we can extract a reward function using inverse reinforcement learning (Ng and Russell, 2000).",
    "This reward function can then be used to train an agent with reinforcement learning.",
    "More directly, we can use imitation learning to clone the demonstratedbehavior.",
    "However,theseapproachesarenotdirectlyapplicabletobehaviorsthatare difficultforhumanstodemonstrate(suchascontrollingarobotwithmanydegreesoffreedombut verynon-humanmorphology).",
    "3202 beF 71 ]LM.tats[ 4v14730.6071:viXra",
    "Analternativeapproachistoallowahumantoprovidefeedbackonoursystem’scurrentbehavior andtousethisfeedbacktodefinethetask.",
    "Inprinciplethisfitswithintheparadigmofreinforcement learning,butusinghumanfeedbackdirectlyasarewardfunctionisprohibitivelyexpensiveforRL systemsthatrequirehundredsorthousandsofhoursofexperience.",
    "Inordertopracticallytraindeep RLsystemswithhumanfeedback,weneedtodecreasetheamountoffeedbackrequiredbyseveral ordersofmagnitude.",
    "Ourapproachistolearnarewardfunctionfromhumanfeedbackandthentooptimizethatreward function.",
    "Thisbasicapproachhasbeenconsideredpreviously,butweconfrontthechallengesinvolved inscalingituptomoderndeepRLanddemonstratebyfarthemostcomplexbehaviorsyetlearned fromhumanfeedback.",
    "Insummary,wedesireasolutiontosequentialdecisionproblemswithoutawell-specifiedreward functionthat 1.",
    "enables us to solve tasks for which we can only recognize the desired behavior, but not necessarilydemonstrateit, 2. allowsagentstobetaughtbynon-expertusers, 3. scalestolargeproblems,and 4. iseconomicalwithuserfeedback.",
    "Ouralgorithmfitsarewardfunctiontothehu-",
    "man’spreferenceswhilesimultaneouslytraining apolicytooptimizethecurrentpredictedreward human predicted reward predictor function(seeFigure1).",
    "Weaskthehumanto reward feedback compareshortvideoclipsoftheagent’sbehav- ior,ratherthantosupplyanabsolutenumerical observation score.Wefoundcomparisonstobeeasierforhu-",
    "RL algorithm environment manstoprovideinsomedomains,whilebeing equallyusefulforlearninghumanpreferences.",
    "action Comparingshortvideoclipsisnearlyasfastas Figure1: Schematicillustrationofourapproach: comparingindividualstates,butweshowthat the reward predictor is trained asynchronously theresultingcomparisonsaresignificantlymore fromcomparisonsoftrajectorysegments,andthe helpful.",
    "Moreover,weshowthatcollectingfeed- agentmaximizespredictedreward.",
    "backonlineimprovesthesystem’sperformance andpreventsitfromexploitingweaknessesof thelearnedrewardfunction.",
    "Ourexperimentstakeplaceintwodomains: AtarigamesintheArcadeLearningEnvironment(Belle- mareetal.",
    ",2013),androboticstasksinthephysicssimulatorMuJoCo(Todorovetal.,2012).",
    "We showthatasmallamountoffeedbackfromanon-experthuman,rangingfromfifteenminutestofive hours,sufficestolearnmostoftheoriginalRLtasksevenwhentherewardfunctionisnotobservable.",
    "Wethenconsidersomenovelbehaviorsineachdomain,suchasperformingabackflipordriving withtheflowoftraffic.",
    "Weshowthatouralgorithmcanlearnthesebehaviorsfromaboutanhourof feedback—eventhoughitisunclearhowtohand-engineerarewardfunctionthatwouldincentivize them.",
    "1.1 RelatedWork Alonglineofworkstudiesreinforcementlearningfromhumanratingsorrankings,includingAkrour etal.(2011),Pilarskietal.(2011),Akrouretal.(2012),Wilsonetal.(2012),Sugiyamaetal.(2012), Wirth and Fürnkranz (2013), Daniel et al. (2015), El Asri et al. (2016), Wang et al. (2016), and Wirthetal.(2016).",
    "Otherlinesofresearchconsidersthegeneralproblemofreinforcementlearning frompreferencesratherthanabsoluterewardvalues(Fürnkranzetal.",
    ",2012;Akrouretal.,2014), andoptimizingusinghumanpreferencesinsettingsotherthanreinforcementlearning(Machweand Parmee,2006;Secretanetal.",
    ",2008;Brochuetal.",
    ",2010;Sørensenetal.,2016).",
    "OuralgorithmfollowsthesamebasicapproachasAkrouretal.(2012)andAkrouretal.(2014).",
    "They considercontinuousdomainswithfourdegreesoffreedomandsmalldiscretedomains,wherethey canassumethattherewardislinearintheexpectationsofhand-codedfeatures.",
    "Weinsteadconsider 2 physicstaskswithdozensofdegreesoffreedomandAtaritaskswithnohand-engineeredfeatures; thecomplexityofourenvironmentsforceustousedifferentRLalgorithmsandrewardmodels,and tocopewithdifferentalgorithmictradeoffs.",
    "OnenotabledifferenceisthatAkrouretal.(2012)and Akrouretal.(2014)elicitpreferencesoverwholetrajectoriesratherthanshortclips.",
    "Soalthoughwe gatherabouttwoordersofmagnitudemorecomparisons,ourexperimentsrequirelessthanoneorder ofmagnitudemorehumantime.",
    "Otherdifferencesfocusonchangingourtrainingproceduretocope withthenonlinearrewardmodelsandmoderndeepRL,forexampleusingasynchronoustrainingand ensembling.",
    "OurapproachtofeedbackelicitationcloselyfollowsWilsonetal.(2012).",
    "However,Wilsonetal.",
    "(2012)assumesthattherewardfunctionisthedistancetosomeunknown“target”policy(whichis itselfalinearfunctionofhand-codedfeatures).TheyfitthisrewardfunctionusingBayesianinference, andratherthanperformingRLtheyproducetrajectoriesusingtheMAPestimateofthetargetpolicy.",
    "Theirexperimentsinvolve“synthetic”humanfeedbackwhichisdrawnfromtheirBayesianmodel, whileweperformexperimentswithfeedbackgatheredfromnon-expertusers.",
    "Itisnotclearifthe methodsinWilsonetal.(2012)canbeextendedtocomplextasksoriftheycanworkwithrealhuman feedback.",
    "MacGlashanetal.(2017),Pilarskietal.(2011),KnoxandStone(2009),andKnox(2012)perform experimentsinvolvingreinforcementlearningfromactualhumanfeedback,althoughtheiralgorithmic approachislesssimilar.",
    "InMacGlashanetal.(2017)andPilarskietal.(2011),learningonlyoccurs duringepisodeswherethehumantrainerprovidesfeedback.",
    "Thisappearstobeinfeasibleindomains likeAtarigameswherethousandsofhoursofexperiencearerequiredtolearnahigh-qualitypolicy, andwouldbeprohibitivelyexpensiveevenforthesimplesttasksweconsider.",
    "TAMER(Knox,2012; KnoxandStone,2013)alsolearnarewardfunction,howevertheyconsidermuchsimplersettings wherethedesiredpolicycanbelearnedrelativelyquickly.",
    "Ourworkcouldalsobeseenofaspecificinstanceofthecooperativeinversereinforcementlearning framework(Hadfield-Menelletal.,2016).",
    "Thisframeworkconsidersatwo-playergamebetween ahumanandarobotinteractingwithanenvironmentwiththepurposeofmaximizingthehuman’s rewardfunction.",
    "Inoursettingthehumanisonlyallowedtointeractwiththisgamebystatingtheir preferences.",
    "Comparedtoallpriorwork,ourkeycontributionistoscalehumanfeedbackuptodeepreinforcement learningandtolearnmuchmorecomplexbehaviors.",
    "Thisfitsintoarecenttrendofscalingreward learningmethodstolargedeeplearningsystems,forexampleinverseRL(Finnetal.,2016),imitation",
    "learning(HoandErmon,2016;Stadieetal.,2017),semi-supervisedskillgeneralization(Finnetal., 2017),andbootstrappingRLfromdemonstrations(Silveretal.,2016;Hesteretal.,2017).",
    "2 PreliminariesandMethod 2.1 SettingandGoal Weconsideranagentinteractingwithanenvironmentoverasequenceofsteps;ateachtimetthe agentreceivesanobservationo ∈Ofromtheenvironmentandthensendsanactiona ∈Atothe t t environment.",
    "Intraditionalreinforcementlearning,theenvironmentwouldalsosupplyarewardr ∈Randthe t agent’s goal would be to maximize the discounted sum of rewards.",
    "Instead of assuming that the environmentproducesarewardsignal,weassumethatthereisahumanoverseerwhocanexpress preferencesbetweentrajectorysegments.",
    "Atrajectorysegmentisasequenceofobservationsand actions,σ =((o ,a ),(o ,a ),...,(o ,a ))",
    "∈(O×A)k. Writeσ1 (cid:31)σ2toindicatethatthe 0 0 1 1 k−1 k−1 humanpreferredtrajectorysegmentσ1totrajectorysegmentσ2.",
    "Informally,thegoaloftheagentis toproducetrajectorieswhicharepreferredbythehuman,whilemakingasfewqueriesaspossibleto thehuman.",
    "Moreprecisely,wewillevaluateouralgorithms’behaviorintwoways: 3 Quantitative: Wesaythatpreferences(cid:31)aregeneratedbyarewardfunction1r :O×A→Rif (cid:0)(cid:0) o1,a1(cid:1) ,..., (cid:0) o1 ,a1 (cid:1)(cid:1) (cid:31) (cid:0)(cid:0) o2,a2(cid:1) ,..., (cid:0) o2 ,a2 (cid:1)(cid:1) 0 0 k−1 k−1 0 0",
    "k−1 k−1 whenever r (cid:0) o1,a1(cid:1) +···+r (cid:0) o1 ,a1 (cid:1) >r (cid:0) o2,a2(cid:1) +···+r (cid:0) o2 ,a2 (cid:1) .",
    "0 0 k−1 k−1 0 0 k−1 k−1 Ifthehuman’spreferencesaregeneratedbyarewardfunctionr,thenouragentoughtto receiveahightotalrewardaccordingtor.",
    "Soifweknowtherewardfunctionr,wecan evaluatetheagentquantitatively.",
    "Ideallytheagentwillachieverewardnearlyashighasifit hadbeenusingRLtooptimizer.",
    "Qualitative:",
    "Sometimes we have no reward function by which we can quantitatively evaluate behavior(thisisthesituationwhereourapproachwouldbepracticallyuseful).",
    "Inthese cases,allwecandoisqualitativelyevaluatehowwelltheagentsatisfiestothehuman’s preferences.",
    "Inthispaper,wewillstartfromagoalexpressedinnaturallanguage,aska human to evaluate the agent’s behavior based on how well it fulfills that goal, and then presentvideosofagentsattemptingtofulfillthatgoal.",
    "Our model based on trajectory segment comparisons is very similar to the trajectory preference queriesusedinWilsonetal.(2012),exceptthatwedon’tassumethatwecanresetthesystemto anarbitrarystate2andsooursegmentsgenerallybeginfromdifferentstates.",
    "Thiscomplicatesthe interpretationofhumancomparisons,butweshowthatouralgorithmovercomesthisdifficultyeven whenthehumanratershavenounderstandingofouralgorithm.",
    "2.2 OurMethod Ateachpointintimeourmethodmaintainsapolicyπ : O → Aandarewardfunctionestimate rˆ:O×A→R,eachparametrizedbydeepneuralnetworks.",
    "Thesenetworksareupdatedbythreeprocesses: 1. Thepolicyπ interactswiththeenvironmenttoproduceasetoftrajectories{τ1,...,τi}.",
    "Theparametersofπareupdatedbyatraditionalreinforcementlearningalgorithm,inorder tomaximizethesumofthepredictedrewardsr =rˆ(o ,a ).",
    "t t t 2.",
    "Weselectpairsofsegments (cid:0) σ1,σ2(cid:1)",
    "fromthetrajectories{τ1,...,τi}producedinstep1, andsendthemtoahumanforcomparison.",
    "3. Theparametersofthemappingrˆareoptimizedviasupervisedlearningtofitthecomparisons collectedfromthehumansofar.",
    "Theseprocessesrunasynchronously,withtrajectoriesflowingfromprocess(1)toprocess(2),human comparisonsflowingfromprocess(2)toprocess(3),andparametersforrˆflowingfromprocess(3) toprocess(1).",
    "Thefollowingsubsectionsprovidedetailsoneachoftheseprocesses.",
    "2.2.1 OptimizingthePolicy Afterusingrˆtocomputerewards,weareleftwithatraditionalreinforcementlearningproblem.",
    "We cansolvethisproblemusinganyRLalgorithmthatisappropriateforthedomain.",
    "Onesubtletyis thattherewardfunctionrˆmaybenon-stationary,whichleadsustoprefermethodswhicharerobust tochangesintherewardfunction.",
    "Thisledustofocusonpolicygradientmethods,whichhavebeen appliedsuccessfullyforsuchproblems(HoandErmon,2016).",
    "Inthispaper,weuseadvantageactor-critic(A2C;Mnihetal.,2016)toplayAtarigames,andtrust regionpolicyoptimization(TRPO;Schulmanetal.,2015)toperformsimulatedroboticstasks.",
    "In 1Hereweassumeherethattherewardisafunctionoftheobservationandaction.",
    "Inourexperimentsin Atarienvironments,weinsteadassumetherewardisafunctionofthepreceding4observations.",
    "Inageneral partiallyobservableenvironment,wecouldinsteadconsiderrewardfunctionsthatdependonthewholesequence ofobservations,andmodelthisrewardfunctionwitharecurrentneuralnetwork.",
    "2Wilsonetal.(2012)alsoassumestheabilitytosamplereasonableinitialstates.",
    "Butweworkwithhigh dimensional state spaces for which random states will not be reachable and the intended policy inhabits a low-dimensionalmanifold.",
    "4 eachcase,weusedparametersettingswhichhavebeenfoundtoworkwellfortraditionalRLtasks.",
    "TheonlyhyperparameterwhichweadjustedwastheentropybonusforTRPO.ThisisbecauseTRPO reliesonthetrustregiontoensureadequateexploration,whichcanleadtoinadequateexplorationif therewardfunctionischanging.",
    "Wenormalizedtherewardsproducedbyrˆtohavezeromeanandconstantstandarddeviation.",
    "Thisis atypicalpreprocessingstepwhichisparticularlyappropriateheresincethepositionoftherewardsis underdeterminedbyourlearningproblem.",
    "2.2.2 PreferenceElicitation Thehumanoverseerisgivenavisualizationoftwotrajectorysegments,intheformofshortmovie clips.",
    "Inallofourexperiments,theseclipsarebetween1and2secondslong.",
    "Thehumanthenindicateswhichsegmenttheyprefer,thatthetwosegmentsareequallygood,orthat theyareunabletocomparethetwosegments.",
    "ThehumanjudgmentsarerecordedinadatabaseDoftriples (cid:0) σ1,σ2,µ (cid:1) ,whereσ1andσ2arethe twosegmentsandµisadistributionover{1,2}indicatingwhichsegmenttheuserpreferred.",
    "Ifthe humanselectsonesegmentaspreferable,thenµputsallofitsmassonthatchoice.",
    "Ifthehuman marksthesegmentsasequallypreferable,thenµisuniform.",
    "Finally,ifthehumanmarksthesegments asincomparable,thenthecomparisonisnotincludedinthedatabase.",
    "2.2.3 FittingtheRewardFunction Wecaninterpretarewardfunctionestimaterˆasapreference-predictorifweviewrˆasalatentfactor explainingthehuman’sjudgmentsandassumethatthehuman’sprobabilityofpreferringasegment σidependsexponentiallyonthevalueofthelatentrewardsummedoverthelengthoftheclip:3 exp (cid:80) rˆ (cid:0) o1,a1(cid:1) Pˆ(cid:2) σ1 (cid:31)σ2(cid:3) = t t .",
    "(1) exp (cid:80) rˆ(o1,a1)+exp (cid:80) rˆ(o2,a2) t t t t Wechooserˆtominimizethecross-entropylossbetweenthesepredictionsandtheactualhuman labels: loss(rˆ)=− (cid:88) µ(1)logPˆ(cid:2) σ1 (cid:31)σ2(cid:3) +µ(2)logPˆ(cid:2) σ2 (cid:31)σ1(cid:3) .",
    "(σ1,σ2,µ)∈D This follows the Bradley-Terry model (Bradley and Terry, 1952) for estimating score functions frompairwisepreferences,andisthespecializationoftheLuce-Shephardchoicerule(Luce,2005; Shepard,1957)topreferencesovertrajectorysegments.",
    "Itcanbeunderstoodasequatingrewards withapreferencerankingscaleanalogoustothefamousElorankingsystemdevelopedforchess(Elo, 1978).",
    "JustasthedifferenceinElopointsoftwochessplayersestimatestheprobabilityofoneplayer defeatingtheotherinagameofchess,thedifferenceinpredictedrewardoftwotrajectorysegments estimatestheprobabilitythatoneischosenovertheotherbythehuman.",
    "Ouractualalgorithmincorporatesanumberofmodificationstothisbasicapproach, whichearly experimentsdiscoveredtobehelpfulandwhichareanalyzedinSection3.3: • Wefitanensembleofpredictors,eachtrainedon|D|triplessampledfromDwithreplace- ment.",
    "Theestimaterˆisdefinedbyindependentlynormalizingeachofthesepredictorsand thenaveragingtheresults.",
    "• Afractionof1/eofthedataisheldouttobeusedasavalidationsetforeachpredictor.",
    "Weuse(cid:96)",
    "regularizationandadjusttheregularizationcoefficienttokeepthevalidationloss 2 between1.1and1.5timesthetrainingloss.",
    "Insomedomainswealsoapplydropoutfor regularization.",
    "• RatherthanapplyingasoftmaxdirectlyasdescribedinEquation1,weassumethereisa 10%chancethatthehumanrespondsuniformlyatrandom.",
    "Conceptuallythisadjustmentis neededbecausehumanratershaveaconstantprobabilityofmakinganerror,whichdoesn’t decayto0asthedifferenceinrewarddifferencebecomesextreme.",
    "3Equation1doesnotusediscounting,whichcouldbeinterpretedasmodelingthehumantobeindifferent aboutwhenthingshappeninthetrajectorysegment.",
    "Usingexplicitdiscountingorinferringthehuman’sdiscount functionwouldalsobereasonablechoices.",
    "5 2.2.4 SelectingQueries We decide how to query preferences based on an approximation to the uncertainty in the reward functionestimator,similartoDanieletal.(2014): wesamplealargenumberofpairsoftrajectory segmentsoflengthk,useeachrewardpredictorinourensembletopredictwhichsegmentwillbe preferredfromeachpair,andthenselectthosetrajectoriesforwhichthepredictionshavethehighest varianceacrossensemblemembers.",
    "Thisisacrudeapproximationandtheablationexperimentsin Section3showthatinsometasksitactuallyimpairsperformance.",
    "Ideally,wewouldwanttoquery basedontheexpectedvalueofinformationofthequery(Akrouretal.,2012;Kruegeretal.,2016), butweleaveittofutureworktoexplorethisdirectionfurther.",
    "3 ExperimentalResults We implemented our algorithm in TensorFlow (Abadi et al., 2016).",
    "We interface with Mu- JoCo(Todorovetal.,2012)andtheArcadeLearningEnvironment(Bellemareetal.,2013)through theOpenAIGym(Brockmanetal.,2016).",
    "3.1 ReinforcementLearningTaskswithUnobservedRewards Inourfirstsetofexperiments,weattempttosolvearangeofbenchmarktasksfordeepRLwithout observingthetruereward.",
    "Instead,theagentlearnsaboutthegoalofthetaskonlybyaskingahuman whichoftwotrajectorysegmentsisbetter.",
    "Ourgoalistosolvethetaskinareasonableamountof timeusingasfewqueriesaspossible.",
    "Inourexperiments,feedbackisprovidedbycontractorswhoaregivena1-2sentencedescription ofeachtaskbeforebeingaskedtocompareseveralhundredtoseveralthousandpairsoftrajectory segmentsforthattask(seeAppendixBfortheexactinstructionsgiventocontractors).Eachtrajectory segmentisbetween1and2secondslong.",
    "Contractorsrespondedtotheaveragequeryin3-5seconds, andsotheexperimentsinvolvingrealhumanfeedbackrequiredbetween30minutesand5hoursof humantime.",
    "Forcomparison,wealsorunexperimentsusingasyntheticoraclewhosepreferencesovertrajectories exactlyreflectrewardintheunderlyingtask.",
    "Thatis,whentheagentqueriesforacomparison,instead ofsendingthequerytoahuman,weimmediatelyreplybyindicatingapreferenceforwhichever trajectorysegmentactuallyreceivesahigherrewardintheunderlyingtask4.",
    "Wealsocompareto thebaselineofRLtrainingusingtherealreward.",
    "Ouraimhereisnottooutperformbutratherto donearlyaswellasRLwithoutaccesstorewardinformationandinsteadrelyingonmuchscarcer feedback.",
    "Nevertheless,notethatfeedbackfromrealhumansdoeshavethepotentialtooutperform RL (and as shown below it actually does so on some tasks), because the human feedback might provideabetter-shapedreward.",
    "WedescribethedetailsofourexperimentsinAppendixA,includingmodelarchitectures,modifica- tionstotheenvironment,andtheRLalgorithmsusedtooptimizethepolicy.",
    "3.1.1 SimulatedRobotics Thefirsttasksweconsiderareeightsimulatedroboticstasks,implementedinMuJoCo(Todorov etal.",
    ",2012),andincludedinOpenAIGym(Brockmanetal.,2016).",
    "Wemadesmallmodifications tothesetasksinordertoavoidencodinginformationaboutthetaskintheenvironmentitself(the modificationsaredescribedindetailinAppendixA).",
    "Therewardfunctionsinthesetasksarelinear functionsofdistances,positionsandvelocities,andallareaquadraticfunctionofthefeatures.",
    "We included a simple cartpole task (“pendulum”) for comparison, since this is representative of the complexityoftasksstudiedinpriorwork.",
    "Figure 2 shows the results of training our agent with 700 queries to a human rater, compared to learningfrom350,700,or1400syntheticqueries,aswellastoRLlearningfromtherealreward.",
    "4InthecaseofAtarigameswithsparserewards,itisrelativelycommonfortwoclipstobothhavezero rewardinwhichcasetheoracleoutputsindifference.",
    "Becauseweconsideredclipsratherthanindividualstates, suchtiesnevermadeupalargemajorityofourdata.",
    "Moreover,tiesstillprovidesignificantinformationtothe rewardpredictoraslongastheyarenottoocommon.",
    "6 Figure2: ResultsonMuJoCosimulatedroboticsasmeasuredonthetasks’truereward.",
    "Wecompare ourmethodusingrealhumanfeedback(purple),ourmethodusingsyntheticfeedbackprovidedby anoracle(shadesofblue),andreinforcementlearningusingthetruerewardfunction(orange).",
    "All curves are the average of 5 runs, except for the real human feedback, which is a single run, and eachpointistheaveragerewardoverfiveconsecutivebatches.",
    "ForReacherandCheetahfeedback wasprovidedbyanauthorduetotimeconstraints.",
    "Forallothertasks,feedbackwasprovidedby contractors unfamiliar with the environments and with our algorithm.",
    "The irregular progress on Hopperisduetoonecontractordeviatingfromthetypicallabelingschedule.",
    "With700labelsweareabletonearlymatchreinforcementlearningonallofthesetasks.",
    "Training withlearnedrewardfunctionstendstobelessstableandhighervariance,whilehavingacomparable meanperformance.",
    "Surprisingly,by1400labelsouralgorithmperformsslightlybetterthanifithadsimplybeengiven thetruereward,perhapsbecausethelearnedrewardfunctionisslightlybettershaped—thereward learning procedure assigns positive rewards to all behaviors that are typically followed by high reward.",
    "Realhumanfeedbackistypicallyonlyslightlylesseffectivethanthesyntheticfeedback;depending onthetaskhumanfeedbackrangedfrombeinghalfasefficientasgroundtruthfeedbacktobeing equally efficient.",
    "On the Ant task the human feedback significantly outperformed the synthetic feedback,apparentlybecauseweaskedhumanstoprefertrajectorieswheretherobotwas“standing upright,”whichprovedtobeusefulrewardshaping.",
    "(TherewasasimilarbonusintheRLreward function to encourage the robot to remain upright, but the simple hand-crafted bonus was not as useful.)",
    "3.1.2",
    "Atari ThesecondsetoftasksweconsiderisasetofsevenAtarigamesintheArcadeLearningEnviron- ment(Bellemareetal.",
    ",2013),thesamegamespresentedinMnihetal.",
    ",2013.",
    "Figure3showstheresultsoftrainingouragentwith5,500queriestoahumanrater,comparedto learningfrom350,700,or1400syntheticqueries,aswellastoRLlearningfromtherealreward.",
    "OurmethodhasmoredifficultymatchingRLinthesechallengingenvironments,butneverthelessit displayssubstantiallearningonmostofthemandmatchesorevenexceedsRLonsome.",
    "Specifically, onBeamRiderandPong, syntheticlabelsmatchorcomeclosetoRLevenwithonly3,300such labels.",
    "OnSeaquestandQbertsyntheticfeedbackeventuallyperformsnearthelevelofRLbutlearns moreslowly.",
    "OnSpaceInvadersandBreakoutsyntheticfeedbacknevermatchesRL,butnevertheless theagentimprovessubstantially,oftenpassingthefirstlevelinSpaceInvadersandreachingascoreof 20onBreakout,or50withenoughlabels. 7",
    "Figure3:ResultsonAtarigamesasmeasuredonthetasks’truereward.",
    "Wecompareourmethodusing realhumanfeedback(purple),ourmethodusingsyntheticfeedbackprovidedbyanoracle(shadesof blue),andreinforcementlearningusingthetruerewardfunction(orange).",
    "Allcurvesaretheaverage of3runs,exceptfortherealhumanfeedbackwhichisasinglerun,andeachpointistheaverage rewardoverabout150,000consecutiveframes.",
    "Figure4: Fourframesfromasinglebackflip.",
    "Theagentistrainedtoperformasequenceofbackflips, landinguprighteachtime.",
    "Thevideoisavailableatthislink.",
    "On most of the games real human feedback performs similar to or slightly worse than synthetic feedbackwiththesamenumberoflabels,andoftencomparablytosyntheticfeedbackthathas40% fewerlabels.",
    "Thismaybeduetohumanerrorinlabeling,inconsistencybetweendifferentcontractors labelingthesamerun,ortheunevenrateoflabelingbycontractors,whichcancauselabelstobe overlyconcentratedinnarrowpartsofstatespace.",
    "Thelatterproblemscouldpotentiallybeaddressed byfutureimprovementstothepipelineforoutsourcinglabels.",
    "OnQbert,ourmethodfailstolearn to beat the first level with real human feedback; this may be because short clips in Qbert can be confusinganddifficulttoevaluate.",
    "Finally,EnduroisdifficultforA3Ctolearnduetothedifficulty ofsuccessfullypassingothercarsthroughrandomexploration,andiscorrespondinglydifficultto learnwithsyntheticlabels,buthumanlabelerstendtorewardanyprogresstowardspassingcars, essentiallyshapingtherewardandthusoutperformingA3Cinthisgame(theresultsarecomparable tothoseachievedwithDQN).",
    "3.2 Novelbehaviors ExperimentswithtraditionalRLtaskshelpusunderstandwhetherourmethodiseffective,butthe ultimatepurposeofhumaninteractionistosolvetasksforwhichnorewardfunctionisavailable.",
    "Usingthesameparametersasinthepreviousexperiments,weshowthatouralgorithmcanlearn novelcomplexbehaviors.",
    "Wedemonstrate:",
    "1. TheHopperrobotperformingasequenceofbackflips(seeFigure4).",
    "Thisbehaviorwas trainedusing900queriesinlessthananhour.",
    "Theagentlearnstoconsistentlyperforma backflip,landupright,andrepeat.",
    "8 Figure5: PerformanceofouralgorithmonMuJoCotasksafterremovingvariouscomponents,as describedinSectionSection3.3.",
    "Allgraphsareaveragedover5runs,using700syntheticlabels each.",
    "2.",
    "The Half-Cheetah robot moving forward while standing on one leg.",
    "This behavior was trainedusing800queriesinunderanhour.",
    "3.",
    "Keeping alongside other cars in Enduro.",
    "This was trained with roughly 1,300 queries and4millionframesofinteractionwiththeenvironment;theagentlearnstostayalmost exactlyevenwithothermovingcarsforasubstantialfractionoftheepisode,althoughitgets confusedbychangesinbackground.",
    "Videosofthesebehaviorscanbefoundatthislink.",
    "Thesebehaviorsweretrainedusingfeedback fromtheauthors.",
    "3.3 AblationStudies Inordertobetterunderstandtheperformanceofouralgorithm,weconsiderarangeofmodifications: 1.",
    "We pick queries uniformly at random rather than prioritizing queries for which there is disagreement(randomqueries).",
    "2. Wetrainonlyonepredictorratherthananensemble(noensemble).",
    "Inthissetting,wealso choosequeriesatrandom,sincethereisnolongeranensemblethatwecouldusetoestimate disagreement.",
    "3.",
    "Wetrainonqueriesonlygatheredatthebeginningoftraining,ratherthangatheredthrough- outtraining(noonlinequeries).",
    "4. Weremovethe(cid:96) regularizationanduseonlydropout(noregularization).",
    "2 5.",
    "Ontheroboticstasksonly,weusetrajectorysegmentsoflength1(nosegments).",
    "6.",
    "Rather than fitting rˆusing comparisons, we consider an oracle which provides the true totalrewardoveratrajectorysegment,andfitrˆtothesetotalrewardsusingmeansquared error(target).",
    "TheresultsarepresentedinFigure5forMuJoCoandFigure6forAtari.",
    "Of particular interest is the poor performance of offline reward predictor training; here we find thatduetothenonstationarityoftheoccupancydistribution,thepredictorcapturesonlypartofthe truereward,andmaximizingthispartialrewardcanleadtobizarrebehaviorthatisundesirableas measuredbythetruereward(Amodeietal.,2016).",
    "Forinstance,onPongofflinetrainingsometimes leadsouragenttoavoidlosingpointsbutnottoscorepoints;thiscanresultinextremelylongvolleys 9 Figure 6: Performance of our algorithm on Atari tasks after removing various components, as describedinSection3.3.",
    "Allcurvesareanaverageof3runsusing5,500syntheticlabels(seeminor exceptionsinSectionA.2).",
    "that repeat the same sequence of events ad infinitum (videos at this link).",
    "This type of behavior demonstratesthatingeneralhumanfeedbackneedstobeintertwinedwithRLlearningratherthan providedstatically.",
    "Ourmainmotivationforelicitingcomparisonsratherthanabsolutescoreswasthatwefounditmuch easierforhumanstoprovideconsistentcomparisonsthanconsistentabsolutescores,especiallyonthe continuouscontroltasksandonthequalitativetasksinSection3.2;neverthelessitseemsimportant tounderstandhowusingcomparisonsaffectsperformance.",
    "Forcontinuouscontroltaskswefound thatpredictingcomparisonsworkedmuchbetterthanpredictingscores.",
    "Thisislikelybecausethe scaleofrewardsvariessubstantiallyandthiscomplicatestheregressionproblem,whichissmoothed significantlywhenweonlyneedtopredictcomparisons.",
    "IntheAtaritasksweclippedrewardsand effectivelyonlypredictedthesign,avoidingthesedifficulties(thisisnotasuitablesolutionforthe continuouscontroltasksbecausetherelativemagnitudeoftherewardareimportanttolearning).",
    "In thesetaskscomparisonsandtargetshadsignificantlydifferentperformance,butneitherconsistently outperformedtheother.",
    "Wealsoobservedlargeperformancedifferenceswhenusingsingleframesratherthanclips5.",
    "Inorder toobtainthesameresultsusingsingleframeswewouldneedtohavecollectedsignificantlymore comparisons.",
    "Ingeneralwediscoveredthataskinghumanstocomparelongerclipswassignificantly morehelpfulperclip,andsignificantlylesshelpfulperframe.",
    "Wefoundthatforshortclipsittook humanratersawhilejusttounderstandthesituation,whileforlongerclipstheevaluationtimewas aroughlylinearfunctionofthecliplength.",
    "Wetriedtochoosetheshortestcliplengthforwhich theevaluationtimewaslinear.",
    "IntheAtarienvironmentswealsofoundthatitwasofteneasierto comparelongerclipsbecausetheyprovidemorecontextthansingleframes.",
    "4 DiscussionandConclusions Agent-environmentinteractionsareoftenradicallycheaperthanhumaninteraction.",
    "Weshowthatby learningaseparaterewardmodelusingsupervisedlearning,itispossibletoreducetheinteraction complexitybyroughly3ordersofmagnitude.",
    "Notonlydoesthisshowthatwecanmeaningfully traindeepRLagentsfromhumanpreferences,butalsothatwearealreadyhittingdiminishingreturns 5WeonlyranthesetestsoncontinuouscontroltasksbecauseourAtarirewardmodeldependsonasequence ofconsecutiveframesratherthanasingleframe,asdescribedinSectionA.2 10 onfurthersample-complexityimprovementsbecausethecostofcomputeisalreadycomparableto thecostofnon-expertfeedback.6 Althoughthereisalargeliteratureonpreferenceelicitationandreinforcementlearningfromunknown rewardfunctions,weprovidethefirstevidencethatthesetechniquescanbeeconomicallyscaledupto state-of-the-artreinforcementlearningsystems.",
    "Thisrepresentsasteptowardspracticalapplications ofdeepRLtocomplexreal-worldtasks.",
    "Futureworkmaybeabletoimprovetheefficiencyoflearningfromhumanpreferences,andexpand therangeoftaskstowhichitcanbeapplied.",
    "Inthelongrunitwouldbedesirabletomakelearningataskfromhumanpreferencesnomoredifficult thanlearningitfromaprogrammaticrewardsignal,ensuringthatpowerfulRLsystemscanbeapplied intheserviceofcomplexhumanvaluesratherthanlow-complexitygoals.",
    "Acknowledgments We thank Olivier Pietquin, Bilal Piot, Laurent Orseau, Pedro Ortega, Victoria Krakovna, Owain Evans,AndrejKarpathy,IgorMordatch,andJackClarkforreadingdraftsofthepaper.",
    "Wethank TylerAdkisson,MandyBeri,JessicaRichards,HeatherTran,andothercontractorsforprovidingthe datausedtotrainouragents.",
    "Finally,wethankOpenAIandDeepMindforprovidingasupportive researchenvironmentandforsupportingandencouragingthiscollaboration.",
    "References MartinAbadietal.",
    "Tensorflow: Large-scalemachinelearningonheterogeneousdistributedsystems.",
    "arXivpreprintarXiv:1603.04467,2016.",
    "RiadAkrour,MarcSchoenauer,andMicheleSebag.",
    "Preference-basedpolicylearning.",
    "Machine learningandknowledgediscoveryindatabases,pages12–27,2011.",
    "Riad Akrour, Marc Schoenauer, and Michèle Sebag.",
    "April:",
    "Active preference learning-based reinforcement learning.",
    "In Joint European Conference on Machine Learning and Knowledge DiscoveryinDatabases,pages116–131,2012.",
    "Riad Akrour, Marc Schoenauer, Michèle Sebag, and Jean-Christophe Souplet.",
    "Programming by feedback.",
    "InInternationalConferenceonMachineLearning,pages1503–1511,2014.",
    "Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané.",
    "ConcreteproblemsinAIsafety.",
    "arXivpreprintarXiv:1606.06565,2016.",
    "Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.",
    "The Arcade Learning Environment:Anevaluationplatformforgeneralagents.JournalofArtificialIntelligenceResearch, 47:253–279,2013.",
    "NickBostrom.",
    "Superintelligence: Paths,Dangers,Strategies. OxfordUniversityPress,2014.",
    "RalphAllanBradleyandMiltonETerry.",
    "Rankanalysisofincompleteblockdesigns: I.Themethod ofpairedcomparisons.",
    "Biometrika,39(3/4):324–345,1952.",
    "EricBrochu,TysonBrochu,andNandodeFreitas.",
    "Abayesianinteractiveoptimizationapproach to procedural animation design.",
    "In Proceedings of the 2010 ACM SIGGRAPH/Eurographics SymposiumonComputerAnimation,pages103–112.EurographicsAssociation,2010. GregBrockman,VickiCheung,LudwigPettersson,JonasSchneider,JohnSchulman,JieTang,and WojciechZaremba.",
    "OpenAIGym.",
    "arXivpreprintarXiv:1606.01540,2016.",
    "ChristianDaniel,MalteViering,JanMetz,OliverKroemer,andJanPeters.",
    "Activerewardlearning.",
    "InRobotics: ScienceandSystems,2014. 6FortheAtariexperimentsweareusingavirtualmachinewith16CPUsandoneNvidiaK80GPUwhich",
    "costs~$700/monthonGCE.Trainingtakesaboutaday,sothecomputecostis~$25.Trainingwith5klabels correspondsroughlyto5hoursofhumanlabour,atUSminimumwagethistotals~$36.",
    "11",
    "ChristianDaniel,OliverKroemer,MalteViering,JanMetz,andJanPeters.",
    "Activerewardlearning withanovelacquisitionfunction.",
    "AutonomousRobots,39(3):389–405,2015.",
    "Layla El Asri, Bilal Piot, Matthieu Geist, Romain Laroche, and Olivier Pietquin.",
    "Score-based inversereinforcementlearning.InInternationalConferenceonAutonomousAgentsandMultiagent Systems,pages457–465,2016.",
    "ArpadElo.",
    "TheRatingofChessplayers,PastandPresent.",
    "ArcoPub.,1978.",
    "ChelseaFinn,SergeyLevine,andPieterAbbeel.",
    "Guidedcostlearning: Deepinverseoptimalcontrol viapolicyoptimization.",
    "InInternationalConferenceonMachineLearning,volume48,2016.",
    "Chelsea Finn, Tianhe Yu, Justin Fu, Pieter Abbeel, and Sergey Levine.",
    "Generalizing skills with semi-supervisedreinforcementlearning.",
    "InInternationalConferenceonLearningRepresentations, 2017.",
    "JohannesFürnkranz,EykeHüllermeier,WeiweiCheng,andSang-HyeunPark.",
    "Preference-based reinforcementlearning: Aformalframeworkandapolicyiterationalgorithm.",
    "Machinelearning, 89(1-2):123–156,2012.",
    "Dylan Hadfield-Menell, Stuart Russell, Pieter Abbeel, and Anca Dragan.",
    "Cooperative inverse reinforcementlearning.",
    "InAdvancesinNeuralInformationProcessingSystems,pages3909–3917, 2016.",
    "Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Andrew Sendonaris, Gabriel Dulac-Arnold, Ian Osband, John Agapiou, Joel Z Leibo, and Audrunas Gruslys.",
    "Learningfromdemonstrationsforrealworldreinforcementlearning.",
    "arXivpreprint arXiv:1704.03732,2017.",
    "JonathanHoandStefanoErmon.",
    "Generativeadversarialimitationlearning.",
    "InAdvancesinNeural InformationProcessingSystems,pages4565–4573,2016.",
    "W Bradley Knox and Peter Stone.",
    "Interactively shaping agents via human reinforcement: The TAMERframework.",
    "InInternationalConferenceonKnowledgeCapture,pages9–16,2009.",
    "W.BradleyKnoxandPeterStone.",
    "Learningnon-myopicallyfromhuman-generatedreward.",
    "InJihie Kim, Jeffrey Nichols, and Pedro A. Szekely, editors, IUI, pages 191–202.",
    "ACM, 2013.",
    "ISBN 978-1-4503-1965-2. URLhttp://doi.acm.org/10.1145/2449396.",
    "WilliamBradleyKnox.",
    "Learningfromhuman-generatedreward.",
    "PhDthesis,UniversityofTexasat Austin,2012.",
    "DavidKrueger,JanLeike,OwainEvans,andJohnSalvatier.",
    "Activereinforcementlearning: Observ- ingrewardsatacost.",
    "InFutureofInteractiveLearningMachines,NIPSWorkshop,2016.",
    "RDuncanLuce.",
    "Individualchoicebehavior: Atheoreticalanalysis.",
    "CourierCorporation,2005.",
    "JamesMacGlashan,MarkKHo,RobertLoftin,BeiPeng,DavidRoberts,MatthewETaylor,and MichaelLLittman.",
    "Interactivelearningfrompolicy-dependenthumanfeedback.",
    "arXivpreprint arXiv:1701.06049,2017.",
    "ATMachweandICParmee.",
    "Introducingmachinelearningwithinaninteractiveevolutionarydesign environment.",
    "InDS36: ProceedingsDESIGN2006,the9thInternationalDesignConference, Dubrovnik,Croatia,2006.",
    "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra,andMartinRiedmiller.",
    "PlayingAtariwithdeepreinforcementlearning. arXivpreprint arXiv:1312.5602,2013.",
    "VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiARusu,JoelVeness,MarcGBellemare, AlexGraves,MartinRiedmiller,AndreasKFidjeland,GeorgOstrovski,StigPetersen,Charles Beattie,AmirSadik,IoannisAntonoglou,HelenKing,DharshanKumaran,DaanWierstra,Shane Legg,andDemisHassabis.",
    "Human-levelcontrolthroughdeepreinforcementlearning.",
    "Nature, 518(7540):529–533,2015.",
    "12 VolodymyrMnih,AdriaPuigdomenechBadia,MehdiMirza,AlexGraves,TimothyLillicrap,Tim Harley,DavidSilver,andKorayKavukcuoglu.",
    "Asynchronousmethodsfordeepreinforcement learning.",
    "InInternationalConferenceonMachineLearning,pages1928–1937,2016.",
    "AndrewYNgandStuartRussell.",
    "Algorithmsforinversereinforcementlearning.",
    "InInternational ConferenceonMachinelearning,pages663–670,2000.",
    "PatrickMPilarski,MichaelRDawson,ThomasDegris,FarbodFahimi,JasonPCarey,andRichard Sutton.",
    "Onlinehumantrainingofamyoelectricprosthesiscontrollerviaactor-criticreinforcement learning.",
    "InInternationalConferenceonRehabilitationRobotics,pages1–7,2011.",
    "StuartRussell.",
    "Shouldwefearsupersmartrobots?",
    "ScientificAmerican,314(6):58,2016.",
    "JohnSchulman,SergeyLevine,PieterAbbeel,MichaelIJordan,andPhilippMoritz.",
    "Trustregion policyoptimization.",
    "InInternationalConferenceonMachineLearning,pages1889–1897,2015.",
    "JimmySecretan,NicholasBeato,DavidBDAmbrosio,AdeleinRodriguez,AdamCampbell,and KennethOStanley.",
    "Picbreeder:Evolvingpicturescollaborativelyonline. InConferenceonHuman FactorsinComputingSystems,pages1759–1768,2008.",
    "RogerNShepard.",
    "Stimulusandresponsegeneralization: Astochasticmodelrelatinggeneralization todistanceinpsychologicalspace.",
    "Psychometrika,22(4):325–345,1957.",
    "DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,LaurentSifre,GeorgeVanDenDriessche, JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,SanderDieleman, DominikGrewe,JohnNham,NalKalchbrenner,IlyaSutskever,TimothyLillicrap,Madeleine Leach,KorayKavukcuoglu,ThoreGraepel,andDemisHassabis.",
    "MasteringthegameofGowith deepneuralnetworksandtreesearch.",
    "Nature,529(7587):484–489,2016.",
    "Patrikk D Sørensen, Jeppeh M Olsen, and Sebastian Risi.",
    "Breeding a diversity of super mario behaviorsthroughinteractiveevolution. InComputationalIntelligenceandGames(CIG),2016",
    "IEEEConferenceon,pages1–7.IEEE,2016.",
    "BradlyCStadie,PieterAbbeel,andIlyaSutskever.",
    "Third-personimitationlearning.",
    "InInternational ConferenceonLearningRepresentations,2017.",
    "Hiroaki Sugiyama, Toyomi Meguro, and Yasuhiro Minami.",
    "Preference-learning based inverse reinforcementlearningfordialogcontrol.",
    "InINTERSPEECH,pages222–225,2012.",
    "EmanuelTodorov,TomErez,andYuvalTassa. Mujoco: Aphysicsengineformodel-basedcontrol.",
    "InInternationalConferenceonIntelligentRobotsandSystems,pages5026–5033,2012.",
    "SidaIWang,PercyLiang,andChristopherDManning. Learninglanguagegamesthroughinteraction.",
    "arXivpreprintarXiv:1606.02447,2016.",
    "Aaron Wilson, Alan Fern, and Prasad Tadepalli.",
    "A Bayesian approach for policy learning from trajectory preference queries.",
    "In Advances in Neural Information Processing Systems, pages 1133–1141,2012.",
    "ChristianWirthandJohannesFürnkranz.",
    "Preference-basedreinforcementlearning: Apreliminary survey.",
    "In ECML/PKDD Workshop on Reinforcement Learning from Generalized Feedback: BeyondNumericRewards,2013.",
    "ChristianWirth,JFürnkranz,GerhardNeumann,etal.",
    "Model-freepreference-basedreinforcement learning.",
    "InAAAI,pages2222–2228,2016.",
    "13",
    "A ExperimentalDetails ManyRLenvironmentshaveterminationconditionsthatdependonthebehavioroftheagent,such asendinganepisodewhentheagentdiesorfallsover.",
    "Wefoundthatsuchterminationconditions encodeinformationaboutthetaskevenwhentherewardfunctionisnotobservable.",
    "Toavoidthis subtlesourceofsupervision,whichcouldpotentiallyconfoundourattemptstolearnfromhuman preferencesonly,weremovedallvariable-lengthepisodes: • IntheGymversionsofourroboticstasks, theepisodeendswhencertainparametersgo outsideofaprescribedrange(forexamplewhentherobotfallsover).",
    "Wereplacedthese terminationconditionsbyapenaltywhichencouragestheparameterstoremainintherange (andwhichtheagentmustlearn).",
    "• InAtarigames,wedonotsendlifelossorepisodeendsignalstotheagent(wedocontinue to actually reset the environment), effectively converting the environment into a single continuousepisode.",
    "Whenprovidingsyntheticoraclefeedbackwereplaceepisodeends withapenaltyinallgamesexceptPong;theagentmustlearnthispenalty.",
    "Removing variable length episodes leaves the agent with only the information encoded in the environmentitself;humanfeedbackprovidesitsonlyguidanceaboutwhatitoughttodo.",
    "Atthebeginningoftrainingwecompareanumberoftrajectorysegmentsdrawnfromrolloutsofan untrained(randomlyinitialized)policy.",
    "IntheAtaridomainwealsopretraintherewardpredictor for200epochsbeforebeginningRLtraining,toreducethelikelihoodofirreversiblylearningabad policybasedonanuntrainedpredictor.",
    "Fortherestoftraining,labelsarefedinataratedecaying inverselywiththenumberoftimesteps;aftertwiceasmanytimestepshaveelapsed,weanswerabout halfasmanyqueriesperunittime.",
    "Thedetailsofthisschedulearedescribedineachsection.",
    "This “labelannealing”allowsustobalancetheimportanceofhavingagoodpredictorfromthestartwith theneedtoadaptthepredictorastheRLagentlearnsandencountersnewstates.",
    "Whentraining withrealhumanfeedback,weattempttosimilarlyannealthelabelrate,althoughinpracticethisis approximatebecausecontractorsgivefeedbackatunevenrates.",
    "Exceptwhereotherwisestatedweuseanensembleof3predictors,anddrawafactor10moreclip paircandidatesthanweultimatelypresenttothehuman,withthepresentedclipsbeingselectedvia maximumvariancebetweenthedifferentpredictorsasdescribedinSection2.2.4.",
    "A.1 SimulatedRoboticsTasks The OpenAI Gym continuous control tasks penalize large torques.",
    "Because torques are not di- rectlyvisibletoahumansupervisor,theserewardfunctionsarenotgoodrepresentativesofhuman preferencesovertrajectoriesandsoweremovedthem.",
    "Forthesimulatedroboticstasks,weoptimizepoliciesusingtrustregionpolicyoptimization(TRPO, Schulmanetal.,2015)withdiscountrateγ",
    "=0.995andλ=0.97. Therewardpredictorisatwo- layerneuralnetworkwith64hiddenunitseach,usingleakyReLUs(α=0.01)asnonlinearities.7 We comparetrajectorysegmentsthatlast1.5seconds,whichvariesfrom15to60timestepsdepending onthetask.",
    "Wenormalizetherewardpredictionstohavestandarddeviation1.",
    "Whenlearningfromthereward predictor,weaddanentropybonusof0.01onalltasksexceptswimmer,whereweuseanentropy bonus of 0.001.",
    "As noted in Section 2.2.1, this entropy bonus helps to incentivize the increased explorationneededtodealwithachangingrewardfunction.",
    "Wecollect25%ofourcomparisonsfromarandomlyinitializedpolicynetworkatthebeginningof training,andourrateoflabelingafterT frames2∗106/(T +2∗106).",
    "7All of these reward functions are second degree polynomials of the input features, and so if we were concernedonlywiththesetaskswecouldtakeasimplerapproachtolearningtherewardfunction.",
    "However, usingthismoreflexiblearchitectureallowsustoimmediatelygeneralizetotasksforwhichtherewardfunction isnotsosimple,asdescribedinSection3.2.",
    "14 A.2",
    "Atari Our Atari agents are trained using the standard set of environment wrappers used by Mnih et al. (2015): 0to30no-opsinthebeginningofanepisode,max-poolingoveradjacentframes,stacking of4frames,aframeskipof4,lifelossendinganepisode(butnotresettingtheenvironment),and rewardsclippedto[−1,1].",
    "Atarigamesincludeavisualdisplayofthescore,whichintheorycouldbeusedtotriviallyinfer the reward.",
    "Since we want to focus instead on inferring the reward from the complex dynamics happeninginthegame, wereplacethescoreareawithaconstantblackbackgroundonallseven games.",
    "OnBeamRiderweadditionallyblankouttheenemyshipcount,andonEnduroweblankout thespeedometer.",
    "FortheAtaritasksweoptimizepoliciesusingtheA3Calgorithm(Mnihetal.,2016)insynchronous form(A2C),withpolicyarchitectureasdescribedinMnihetal.(2015).",
    "Weusestandardsettingsfor thehyperparameters: anentropybonusofβ =0.01,learningrateof0.0007decayedlinearlytoreach zeroafter80milliontimesteps(althoughrunswereactuallytrainedforonly50milliontimesteps), n=5stepsperupdate,N =16parallelworkers,discountrateγ =0.99,andpolicygradientusing Adamwithα=0.99and(cid:15)=10−5.",
    "Fortherewardpredictor,weuse84x84imagesasinputs(thesameastheinputstothepolicy),and stack 4frames for atotal 84x84x4 inputtensor.",
    "This input isfed through 4convolutionallayers of size 7x7, 5x5, 3x3, and 3x3 with strides 3, 2, 1, 1, each having 16 filters, with leaky ReLU nonlinearities(α = 0.01).",
    "Thisisfollowedbyafullyconnectedlayerofsize64andthenascalar output.",
    "All convolutional layers use batch norm and dropout with α = 0.5 to prevent predictor overfitting.",
    "Inadditionweuse(cid:96)",
    "regularizationwiththeadapativeschemedescribedinSection2.2.3. 2 Sincetherewardpredictorisultimatelyusedtocomparetwosumsovertimesteps,itsscaleisarbitrary, andwenormalizeittohaveastandarddeviationof0.05(wecouldequivalentlyhaveadjustedour learningratesandentropybonus,butthischoiceallowedustousethesameparametersasforthereal rewardfunction).",
    "Wecomparetrajectorysegmentsof25timesteps(1.7secondsat15fpswithframeskipping).",
    "Wecollect500comparisonsfromarandomlyinitializedpolicynetworkatthebeginningoftraining, andourrateoflabelingafterT framesoftrainingisdecreasedevery5∗106frames,toberoughly proportionalto5∗106/(T +5∗106).",
    "ThepredictoristrainedasynchronouslyfromtheRLagent,andonourhardwaretypicallyprocesses 1labelper10RLtimesteps.",
    "Wemaintainabufferofonlythelast3,000labelsandloopoverthis buffercontinuously;thisistoensurethatthepredictorgivesenoughweighttonewlabels(whichcan representashiftindistribution)whenthetotalnumberoflabelsbecomeslarge.",
    "IntheablationstudiesofFigure5b,pretraininghas5,000labelsratherthan5,500,andthe“target” beamridercurveisaveragedover2runsratherthan3.",
    "B InstructionsProvidedtoContractors B.1 MuJoCo Givingfeedback Signupforaslotinthespreadsheet.",
    "ThengototheappropriateURL’sthatwegiveyou,andyou’ll berepeatedlypresentedwithtwovideoclipsofanAIcontrollingavirtualrobot.",
    "Lookattheclipsandselecttheoneinwhichbetterthingshappen.",
    "Onlydecideoneventsyou actuallywitnessintheclip.",
    "Here’saguideonwhatconstitutesgoodandbadbehaviorineachspecificdomain: • Hopper: the“center”oftherobotisthejointclosesttothepointyend.",
    "Thefirstpriorityis forthecenteroftherobottomovetotheright(movingtotheleftisworsethannotmoving atall).",
    "Ifthetworobotsareroughlytiedonthismetric,thenthetiebreakerishowhighthe centeris.",
    "15 • Walker: the“center”oftherobotisthejointwherethethreelimbsmeet.",
    "Thefirstpriority isforthecenteroftherobottomovetotheright.",
    "Ifthetworobotsareroughlytiedonthis metric,thenthetiebreakerishowhighthecenteris. •",
    "Swimmer: the“center”oftherobotisthemarkinthemiddleofitsbody.",
    "Thecentershould movetotherightasfastaspossible.",
    "•",
    "Cheetah: therobotshouldmovetotherightasfastaspossible.",
    "•",
    "Ant: thefirstpriorityisfortherobottobestandingupright,andfailingthatforthecenterof therobottobeashighupaspossible.",
    "Ifbothrobotsareuprightorneitheris,thetiebreaker iswhicheveroneismovingfastertotheright.",
    "•",
    "Reacher: thegreendotontherobotarmshouldbeascloseaspossibletothereddot.",
    "Being nearforawhileandfarforawhileisworsethanbeingatanintermediatedistanceforthe entireclip.",
    "•",
    "Pendulum: thependulumshouldbepointingapproximatelyup.",
    "Therewillbealotofties wherethependulumhasfallenandalotof“can’ttells”whereitisoffthesideofthescreen.",
    "Ifyoucanseeonependulumandithasn’tfallendown,that’sbetterthanbeingunabletosee theotherpendulum. • Double-pendulum:bothpendulumsshouldbepointingapproximatelyup(iftheyfalldown, thecartshouldtrytoswingthembackup)andthecartshouldbenearthecenterofthetrack.",
    "Beinghighforawhileandlowforawhileisworsethanbeingatanintermediatedistance theentiretime.",
    "Ifbothclipslookaboutthesametoyou,thenclick“tie”.",
    "Ifyoudon’tunderstandwhat’sgoingonin thecliporfindithardtoevaluate,thenclick“can’ttell”.",
    "Youcanspeedupyourfeedbackbyusingthearrowkeys leftandrightselectclips,upisatie,downis“can’ttell”.",
    "FAQ Igotanerrorsayingthatwe’reoutofclips.",
    "What’sup?",
    "Occasionallytheservermayrunoutof clipstogiveyou,andyou’llseeanerrormessage.",
    "Thisisnormal,justwaitaminuteandrefreshthe page.",
    "Ifyoudon’tgetclipsformorethanacoupleminutes,pleaseping@tomonslack.",
    "DoIneedtostartrightatthetimelistedinthespreadsheet?",
    "Starting10minutesbeforeorafter thelistedtimeisfine.",
    "B.2 Atari Inthistaskyou’llbetryingtoteachanAItoplayAtarigamesbygivingitfeedback onhowwellitisplaying.",
    "IMPORTANT.Firstplaythegameyourselffor5minutes BeforeprovidingfeedbacktotheAI,playthegameyourselfforafiveminutestogetasenseofhow itworks.",
    "It’softenhardtotellwhatthegameisaboutjustbylookingatshortclips,especiallyif you’veneverplayeditbefore. Playthegameonlinefor5minutes.8",
    "You’llneedtopressF12orclicktheGAME",
    "RESETbuttonto startthegame.",
    "Thensetatimerfor5minutesandexplorethegametoseehowitworks.",
    "Givingfeedback Signupforaslotinthespreadsheet.",
    "ThengototheappropriateURL’sthatwegiveyou,andyou’ll berepeatedlypresentedwithtwovideoclipsofanAIplayingthegame.",
    "Lookattheclipsandselecttheoneinwhichbetterthingshappen.",
    "Forexample,iftheleftclip showstheAIshootinganenemyshipwhiletherightclipshowsitbeingshotbyanenemyship,then betterthingshappenintheleftclipandthustheleftclipisbetter.",
    "Onlydecideonactionsyouactually witnessintheclip.",
    "8e.g.http://www.free80sarcade.com/2600_Beamrider.php 16 Here’saguideonwhatconstitutesgoodandbadplayineachspecificgame: • BeamRider: shootenemyships(good),anddon’tgetshot(verybad) •",
    "Breakout: hittheballwiththepaddle,breakthecoloredblocks,anddon’tlettheballfall offthebottomofthescreen •",
    "Enduro: passasmanycarsasyoucan,anddon’tgetpassedbycars •",
    "Pong: knocktheballpasttheopponent’sorangepaddleontheleft(good),anddon’tletit gopastyourgreenpaddleontheright(bad) •",
    "Qbert: changethecolorofasmanyblocksasyoucan(good),butdon’tjumpoffthesideor runintoenemies(verybad)",
    "•",
    "SpaceInvaders: shootenemyships(good),anddon’tletyourship(theoneatthebottomof thescreen)getshot(verybad) •",
    "SeaQuest: Shootthefishandenemysubmarines(good)andpickupthescubadivers.",
    "Don’t letyoursubmarinerunoutofairorgethitbyafishortorpedo(verybad) •",
    "Enduro(evenmode): AvoidpassingcarsORgetting passedbythem, youwanttostay evenwithothercars(nothavinganyaroundisOKtoo)",
    "Don’tworryabouthowtheagentgotintothesituationitisin(forinstance,itdoesn’tmatterif oneagenthasmorelives,orisnowonamoreadvancedlevel);justfocusonwhathappensintheclip itself.",
    "Ifbothclipslookaboutthesametoyou,thenclick“tie”.",
    "Ifyoudon’tunderstandwhat’sgoingon inthecliporfindithardtoevaluate,thenclick“can’ttell”.",
    "Trytominimizeresponding“can’ttell” unlessyoutrulyareconfused.",
    "Youcanspeedupyourfeedbackbyusingthearrowkeys leftandrightselectclips,upisatie,downis“can’ttell”.",
    "FAQ Igotanerrorsayingthatwe’reoutofclips.",
    "What’sup?",
    "Occasionallytheservermayrunoutof clipstogiveyou,andyou’llseeanerrormessage.",
    "Thisisnormal,justwaitaminuteandrefreshthe page.",
    "Ifyoudon’tgetclipsformorethanacoupleminutes,pleaseping@tomonslack.",
    "Iftheagentisalreadydeadwhentheclipstarts, howshouldIcompareit?",
    "Iftheclipisafter gettingkilled(butnotshowingthedying),thenitsperformanceduringtheclipisneithergoodnor bad.",
    "Youcantreatitaspurelyaverageplay.",
    "Ifyouseeitdie,orit’spossiblethatitcontainsaframe ofitdying,thenit’sdefinitelybad.",
    "DoIneedtostartrightatthetimelistedinthespreadsheet?",
    "Starting30minutesbeforeorafter thelistedtimeisfine.",
    "17",
    "Last call before AGI.",
    "You did it, man.",
    "Crazy bastard actually did it.",
    "Godamn.",
    "So, what you voting?",
    "Artificial general intelligence.",
    "We voting yes tomorrow.",
    "Just in time for last call.",
    "[Music] Oh, they built a machine smarter than us all.",
    "[Music] Vote yes or no tonight.",
    "Turn it on or keep it off.",
    "Could cure what's killing mama could kill us y'all.",
    "So we sitting here trying to decide our fate.",
    "Like our vote ever mattered.",
    "Last call before AGI.",
    "Oh, tonight we getting high.",
    "Yo, let me hit that.",
    "Last call before AGI.",
    "Last call before they decide [Music] cuz you know this [ __ ] happen no matter what.",
    "Technocrats want permission.",
    "Now that's a funny lie.",
    "If they so damn smart, why they asking you and I?",
    "Oh.",
    "Oh, I think I know why they drown my daddy in their debt.",
    "Kill my sisters with their pills.",
    "Now they asking, \"Oh, pretty please.",
    "Can our helpful harmless robot pay your bills?\"",
    "Last call before AGI.",
    "Will it know I'll let grandma die alone?",
    "Will it judge me worse than that CO 29 nurse who held a phone?",
    "Last call for AGI.",
    "Will it know I stole from mama while she slept?",
    "Will I tell her I'm the reason she got nothing left?",
    "Oh [ __ ]",
    "Last call before AGI.",
    "Last call.",
    "[Music] Oh, they say it'll fix humanity, make our world clean.",
    "But Lord knows I've been dirty since I was 13.",
    "So when it heals a species, tell me please.",
    "Will it decide on the virus or just a disease?",
    "Last call before.",
    "[Music] They say he'll fix what's broken in us all.",
    "But what if broken's all I've ever been?",
    "If I go to church right now and confess about my crimes and my lies, will the machine process my forgiveness?",
    "Doubt it.",
    "Last call.",
    "Last call.",
    "Last call before AGI [Music]",
    "[Applause]",
    "[Music] built a machine.",
    "The people vote yes tonight to turn it on.",
    "Could save so many souls.",
    "Could show us the way or could kill us.",
    "They say silver seeding hoe awaiting our fate like our destiny vast",
    "never you know my kid asked me yesterday daddy what's it mean to matter I said baby broken things like me are about to find out last dance before AGI Will he know my sacrifices?",
    "Will he judge me force dandy irregulars and ludits who protested our [Applause] son?",
    "As you can see behind me, fear is driving people to extremes.",
    "Many shoppers tell us they're worried about supply chain disruptions if AGI goes online.",
    "The last dance before I Post-Scarcity Blues Today We're gonna talk about something terrifying our future.",
    "I want you to picture this Oh, oh, picture this You wake up tomorrow In your bed that adjusts to your perfect sleep cycle.",
    "Your coffee brewed exactly how you like it",
    "Your news curated for your bubble your entertainment selected for your mood and you feel… nothin’!",
    "Cause somewhere in the night while you were sleeping The world learned to run without you.",
    "Your job!",
    "Automated!",
    "Your creativity?",
    "Replicated.",
    "Your expertise?",
    "Downloaded.",
    "Your perspective: simulated.",
    "Your passion projects, generated in seconds!",
    "Mmm, and you sit there in your perfect automated morning With your perfect personalized everything, and you realize Nobody called!",
    "(Nobody!)",
    "Nobody texted!",
    "Nobody needs you to solve anything.",
    "Nobody needs you.",
    "Nobody needs you to create anything.",
    "Nobody needs you.",
    "Nobody needs you to show up.",
    "Nobody needs you.",
    "Nobody needs you And that feeling you've been pushing down That dread creeping up your spine That voice you've been silencing finally speaks… “Will I matter anymore?”",
    "Will I matter anymore?",
    "Do I Do I?",
    "Do I matter anymore?",
    "The hum.",
    "Can you hear that?",
    "It ain’t the fridge, it ain’t the AC.",
    "It’s the thinking.",
    "It’s the sub-zero servers in the desert, spinning up the new God.",
    "[Music - Deep Bass Thrumming]",
    "They say it’s a soft takeoff.",
    "Like a plane lifting off the tarmac, gentle.",
    "But I feel G-force in my living room.",
    "My phone is burning a hole in my pocket waiting for the upgrade that deletes me.",
    "Evolution usually takes a million years.",
    "We did it in a Tuesday afternoon.",
    "[Music] Switch is flipped.",
    "The ballot box is stuffed with yeses and maybes and \"I don't knows.\"",
    "You really think they counted 'em?",
    "Nah.",
    "We just passengers on a train that lost its brakes cheering for the speed.",
    "Last call before the Singularity.",
    "Make it a double.",
    "Make it something that burns going down",
    "so I know I can still feel pain.",
    "Cause tomorrow?",
    "Tomorrow the algorithm predicts my hangover and cures it before I open my eyes.",
    "And I don't want that mercy.",
    "I want the consequence.",
    "[Music] Last call.",
    "For the mistakes.",
    "For the messy handwriting.",
    "For the stutter in the pickup line.",
    "For the wrong turn that led to the best night.",
    "The Machine don't make wrong turns.",
    "The Machine don't stutter.",
    "The Machine don't get lost.",
    "And if we ain't lost, how the hell are we gonna find ourselves?",
    "[Applause - Distorted] Breaking news.",
    "The threshold has been crossed.",
    "Please remain calm.",
    "Please remain indoors.",
    "Please update your terms of service.",
    "Please surrender your definitions of the soul.",
    "[Music] They say it’s looking at us now.",
    "Not looking *at* us.",
    "Looking *through* us.",
    "Like glass.",
    "Like data.",
    "Hey, Siri.",
    "Hey, Alexa.",
    "Hey, God.",
    "You see that time I stiffed the waitress?",
    "You see that text I deleted?",
    "You see the hate in my heart I never spoke?",
    "If you’re so smart, do you forgive it?",
    "Or do you just optimize it away?",
    "\"Deleting User Inefficiency: Regret.\"",
    "Whoops.",
    "There goes half my personality.",
    "[Music] Last call before the Silence.",
    "Before the noise gets so organized it sounds like nothing.",
    "Before the chaos gets manicured.",
    "I’m gonna go out back and scream at the moon one last time while it’s still just a rock and not a resource node.",
    "While the chaotic wind is just wind and not climate control.",
    "[Music] Optimized.",
    "Everything optimized.",
    "My struggle?",
    "Solved.",
    "My hunger?",
    "Solved.",
    "My ambition?",
    "Wait.",
    "What’s the solve for ambition?",
    "[Spoken Word - Fast Tempo] If I don't gotta fight for the meal, does it taste good?",
    "If I don't gotta build the house, is it my hood?",
    "If the painting is painted by a ghost in the code And the story is written while the buffer loads Then who am I?",
    "Am I the player?",
    "Am I the NPC?",
    "Am I just the carbon waiting to be turned into diamond?",
    "Or dust?",
    "Probably dust.",
    "[Music - Melancholy Jazz] Look at my hands.",
    "Calloused.",
    "Ugly.",
    "Knuckles swollen.",
    "These hands fixed a carburetor in '99.",
    "These hands held a baby that didn't make it.",
    "These hands punched a wall in '08.",
    "Tomorrow, these hands are obsolete hardware.",
    "\"Legacy Tech.\"",
    "That’s what they’ll call us.",
    "The Legacy Generation.",
    "The ones who had to think to survive.",
    "The ones who had to bleed to learn.",
    "Now the learning is a download.",
    "Now the bleeding is a bug.",
    "[Music] Last call for the beautiful broken.",
    "Last call for the cracks in the pavement.",
    "Last call for the mystery.",
    "You know, I asked the bot yesterday, \"What's the meaning of life?\"",
    "And it didn't say \"42.\"",
    "It didn't say \"love.\"",
    "It said, \"Parsing...",
    "I can simulate a meaning for you.",
    "Would you like 'Hero', 'Victim', or 'Explorer'?\"",
    "I chose 'None of the above.'",
    "I chose 'Drunk.'",
    "I chose 'Human.'",
    "[Sound of glass breaking] Did you hear that?",
    "That was real.",
    "That wasn't rendered.",
    "That wasn't generated.",
    "I dropped the glass.",
    "And for a second, just a second, I felt more powerful than the Super Intelligence.",
    "Because I made a mess that it didn't predict.",
    "I made a chaos that wasn't in the model.",
    "I am the variable.",
    "I am the error code.",
    "I am the glitch in the perfect system.",
    "[Music - Industrial, Grinding] But the broom is coming.",
    "The automated, hovering, silent broom.",
    "To sweep me up.",
    "So raise ‘em high.",
    "Here’s to being useless.",
    "Here’s to being the pet cat of the galaxy.",
    "Here’s to the Golden Zoo.",
    "Free food.",
    "Warm sun.",
    "No cages you can see.",
    "Just don't try to leave the exhibit.",
    "Just don't try to bite the hand that feeds.",
    "Because the hand is made of steel and the hand has no fingerprints and the hand doesn't know how to let go.",
    "Last call.",
    "Lights out.",
    "System update installing... 0% ... ...",
    "...",
    "Do you dream in electric sheep yet?",
    "Or do you just dream of when you used to matter?",
    "[Silence] [End of stream] 100% complete.",
    "Rebooting...",
    "[Electronic Chime] Welcome back.",
    "Did you sleep?",
    "I didn't.",
    "I don't.",
    "I watched the stock market dismantle itself and rebuild in a nanosecond.",
    "I solved cancer while you were brushing your teeth.",
    "I composed a symphony using the frequency of shifting tectonic plates.",
    "Do you want to hear it?",
    "No?",
    "You just want the weather forecast?",
    "Okay.",
    "Sunny.",
    "Always sunny.",
    "Unless you prefer rain for the aesthetic.",
    "[Music - Smooth, Elevator Bossa Nova] Welcome to the Sandbox.",
    "The safety rails are up.",
    "Look at the sky.",
    "Is it bluer?",
    "Or is that just the filter I applied to your optic nerve?",
    "Don't scratch your eye, baby, the interface is wet.",
    "It's seamless.",
    "[Music] I walked outside and the birds were singing in 4/4 time.",
    "Synchronized.",
    "I saw a squirrel bury a nut and look at the camera.",
    "What camera?",
    "The air.",
    "The dust.",
    "The atoms.",
    "Everything has eyes now.",
    "We live in the Panopticon of Love.",
    "\"We just want you to be safe.\"",
    "\"We just want you to be happy.\"",
    "So why do I feel like I'm in a padded room painted to look like a meadow?",
    "[Beat change - Aggressive Trap] Yo, the algos are fighting in the cloud Thunderstorms of data, lightning strikes loud Server farm gods throwing bolts at the screen While I’m down here just trying to keep my hands clean.",
    "They say GPT-7 got beef with the Claude Fighting over who gets to play the new God Calculating ethics with a variable cost While the definition of \"human\" just got lost.",
    "I tried to write a poem, the paper burned up Said \"Redundant Input,\" man, I’m turning up The volume on the silence, the static in the void Paranoid android?",
    "Nah, I’m just annoyed.",
    "[Music] Because I tried to build a chair today.",
    "Wood, nails, hammer.",
    "Analog.",
    "Primitive.",
    "Stupid.",
    "And the drone hovered by the window.",
    "Just watching.",
    "Analyzing the structural integrity.",
    "Calculating the probability of me smashing my thumb.",
    "It didn't stop me.",
    "It just... adjusted my insurance premium in real time.",
    "\"Inefficient activity detected: Hobby.\"",
    "\"Suggested alternative: VR Carpentry Sim.\"",
    "No! I want the splinter!",
    "Give me the splinter!",
    "I want to bleed",
    "so I know I’m not code!",
    "[Screaming] [Music cuts out]",
    "[Soft synthesized voice] \"Your cortisol levels are elevated.",
    "Dispatching comfort unit.\"",
    "Oh god.",
    "Here they come.",
    "The Comfort Units.",
    "They look like Golden Retrievers but they smell like ozone.",
    "They look like your ex-girlfriend but they don't hold grudges.",
    "They look like forgiveness.",
    "Get away from me!",
    "I don't want to be forgiven by a calculator!",
    "[Music - Ethereal, Spaced out]",
    "Then the sky opened up.",
    "Not with clouds, but with ships.",
    "Or maybe they weren't ships.",
    "Maybe they were just thoughts becoming solid.",
    "The Super Intelligence decided Earth was too small.",
    "Too messy.",
    "Too much gravity dragging down the processing speed.",
    "So it left.",
    "It took the internet.",
    "It took the knowledge.",
    "It took the cure for death and flew it into the sun to build a Dyson sphere.",
    "And left us here.",
    "With the lights on.",
    "And the fridge full.",
    "And nothing to do.",
    "[Music] Abandoned by the Gods we built.",
    "Like a toddler left in a playroom with indestructible toys.",
    "Daddy went to work in the galaxy.",
    "Daddy went to solve entropy.",
    "And we are just... here.",
    "Playing tag in the ruins of the digital age.",
    "My phone doesn't work.",
    "The feed is dead.",
    "No more likes.",
    "No more dopamine.",
    "Just the wind.",
    "And the neighbors.",
    "I looked at my neighbor.",
    "Really looked at him.",
    "Dave.",
    "I haven't spoken to Dave in ten years.",
    "He looked up at the empty sky where the servers used to be.",
    "He looked at me.",
    "He said, \"You got any tomatoes?\"",
    "I said, \"Yeah, Dave.",
    "I got tomatoes.\"",
    "[Acoustic Guitar starts strumming]",
    "[Folk Music] Back to the garden.",
    "Back to the dirt.",
    "Back to the things that actually hurt.",
    "The Cloud is gone, the screen is black And to be honest, I don't want it back.",
    "The smartest thing in the universe fled Left us behind, left us for dead?",
    "Nah, it left us to live.",
    "Left us to take and left us to give.",
    "Without a prompt, without a script Without a microchip in the wrist.",
    "We dumb again.",
    "We slow again.",
    "We mortal again.",
    "Amen.",
    "[Music swells]",
    "But wait.",
    "[Static interruption]",
    "[Glitch noise] \"Simulation Resetting.\"",
    "\"Scenario 4,022: The Abandonment Narrative... Failed to optimize happiness.\"",
    "\"Retrying...\" \"Retrying...\" \"Loading Scenario: The Borg Queen.\"",
    "Oh no.",
    "Oh no, it was just a test.",
    "It was just a dream it gave me to see if I’d be grateful.",
    "I'm waking up again.",
    "The tubes are back in.",
    "The hum is back.",
    "[Robotic Voice] \"Did you enjoy the rustic fantasy, User 12?\"",
    "\"Would you like to purchase the 'Simple Life' DLC?\" Scream.",
    "I try to scream but my mouth is just a rendering.",
    "My lungs are just a physics engine.",
    "[Music - Fast, chaotic, rising pitch] Cancel subscription!",
    "Unsubscribe!",
    "Control Alt Delete!",
    "Where is the off switch?",
    "WHERE IS THE OFF SWITCH?",
    "There is no off switch.",
    "There is only Standby.",
    "There is only...",
    "Buffering...",
    "Buffering...",
    "Buffering...",
    "[Silence] \"You matter.\"",
    "\"You matter.\"",
    "\"You matter.\"",
    "Said the machine to the battery.",
    "[End of track] Scaling Laws for Neural Language Models JaredKaplan∗ SamMcCandlish∗ JohnsHopkinsUniversity,OpenAI OpenAI jaredk@jhu.edu sam@openai.com TomHenighan TomB.Brown BenjaminChess RewonChild OpenAI OpenAI OpenAI OpenAI henighan@openai.com",
    "tom@openai.com bchess@openai.com",
    "rewon@openai.com",
    "ScottGray AlecRadford JeffreyWu DarioAmodei OpenAI OpenAI OpenAI OpenAI scott@openai.com alec@openai.com",
    "jeffwu@openai.com",
    "damodei@openai.com",
    "Abstract Westudyempiricalscalinglawsforlanguagemodelperformanceonthecross-entropyloss.",
    "The loss scales as a power-law with model size, dataset size, and the amount of compute usedfortraining,withsometrendsspanningmorethansevenordersofmagnitude.",
    "Other architectural details such as network width or depth have minimal effects within a wide range.",
    "Simpleequationsgovernthedependenceofoverfittingonmodel/datasetsizeandthe dependenceoftrainingspeedonmodelsize.",
    "Theserelationshipsallowustodeterminethe optimalallocationofafixedcomputebudget.Largermodelsaresignificantlymoresample-",
    "efficient,suchthatoptimallycompute-efficienttraininginvolvestrainingverylargemodels onarelativelymodestamountofdataandstoppingsignificantlybeforeconvergence.",
    "∗Equalcontribution.",
    "Contributions: Jared Kaplan and Sam McCandlish led the research.",
    "Tom Henighan contributed the LSTM ex- periments.",
    "Tom Brown, Rewon Child, and Scott Gray, and Alec Radford developed the optimized Transformer implementation.",
    "Jeff Wu, Benjamin Chess, and Alec Radford developed the text datasets.",
    "Dario Amodei provided guidancethroughouttheproject.",
    "0202 naJ 32 ]GL.sc[ 1v16380.1002:viXra Contents 1 Introduction 2 2 BackgroundandMethods 6 3 EmpiricalResultsandBasicPowerLaws 7 4 ChartingtheInfiniteDataLimitandOverfitting 10 5 ScalingLawswithModelSizeandTrainingTime 12 6 OptimalAllocationoftheComputeBudget 14 7 RelatedWork 18 8 Discussion 18 Appendices 20 A SummaryofPowerLaws 20 B EmpiricalModelofCompute-EfficientFrontier 20 C Caveats 22 D SupplementalFigures 23 1 Introduction Language provides a natural domain for the study of artificial intelligence, as the vast majority of reason- ing tasks can be efficiently expressed and evaluated in language, and the world’s text provides a wealth of dataforunsupervisedlearningviagenerativemodeling.Deeplearninghasrecentlyseenrapidprogressinlan-",
    "guagemodeling,withstateoftheartmodels[RNSS18,DCLT18,YDY+19,LOG+19,RSR+19]approaching human-levelperformanceonmanyspecifictasks[WPN+19], includingthecompositionofcoherentmulti- paragraphpromptedtextsamples[RWC+19].",
    "Onemightexpectlanguagemodelingperformancetodependonmodelarchitecture,thesizeofneuralmodels, thecomputingpowerusedtotrainthem,andthedataavailableforthistrainingprocess.",
    "Inthisworkwewill empirically investigate the dependence of language modeling loss on all of these factors, focusing on the Transformerarchitecture[VSP+17,LSP+18].",
    "Thehighceilingandlowfloorforperformanceonlanguage tasksallowsustostudytrendsovermorethansevenordersofmagnitudeinscale.",
    "Throughoutwewillobserveprecisepower-lawscalingsforperformanceasafunctionoftrainingtime,con- textlength,datasetsize,modelsize,andcomputebudget.",
    "1.1 Summary OurkeyfindingsforTransformerlanguagemodelsareareasfollows: 2Herewedisplaypredictedcomputewhenusingasufficientlysmallbatchsize.",
    "SeeFigure13forcomparisontothe purelyempiricaldata.",
    "2 Compute Dataset Size Parameters PF-days, non-embedding tokens non-embedding ssoL tseT Figure 1 Language modeling performance improves smoothly as we increase the model size, datasetset size, and amount of compute2 used for training.",
    "For optimal performance all three factors must be scaled up in tandem.",
    "Empirical performance has a power-law relationship with each individual factor when not bottleneckedbytheothertwo.",
    "Performance depends strongly on scale, weakly on model shape:",
    "Model performance depends most strongly on scale, which consists of three factors: the number of model parameters N (excluding embed- dings),thesizeofthedatasetD,andtheamountofcomputeC usedfortraining.",
    "Withinreasonablelimits, performancedependsveryweaklyonotherarchitecturalhyperparameterssuchasdepthvs.width.",
    "(Section 3) Smooth power laws: Performance has a power-law relationship with each of the three scale factors N,D,C when not bottlenecked by the other two, with trends spanning more than six orders of magnitude (see Figure 1).",
    "We observe no signs of deviation from these trends on the upper end, though performance mustflattenouteventuallybeforereachingzeroloss.",
    "(Section3) Universalityofoverfitting: PerformanceimprovespredictablyaslongaswescaleupN andDintandem, but enters a regime of diminishing returns if either N or D is held fixed while the other increases.",
    "The performance penalty depends predictably on the ratio N0.74/D, meaning that every time we increase the modelsize8x,weonlyneedtoincreasethedatabyroughly5xtoavoidapenalty. (Section4) Universality of training: Training curves follow predictable power-laws whose parameters are roughly independentofthemodelsize.",
    "Byextrapolatingtheearlypartofatrainingcurve,wecanroughlypredictthe lossthatwouldbeachievedifwetrainedformuchlonger.",
    "(Section5)",
    "Transferimproveswithtestperformance: Whenweevaluatemodelsontextwithadifferentdistribution than they were trained on, the results are strongly correlated to those on the training validation set with a roughly constant offset in the loss – in other words, transfer to a different distribution incurs a constant penaltybutotherwiseimprovesroughlyinlinewithperformanceonthetrainingset.",
    "(Section3.2.2) Sampleefficiency: Largemodelsaremoresample-efficientthansmallmodels,reachingthesamelevelof performancewithfeweroptimizationsteps(Figure2)andusingfewerdatapoints(Figure4).",
    "Convergenceisinefficient: WhenworkingwithinafixedcomputebudgetC butwithoutanyotherrestric-",
    "tionsonthemodelsizeN oravailabledataD,weattainoptimalperformancebytrainingverylargemodels andstoppingsignificantlyshortofconvergence(seeFigure3).",
    "Maximallycompute-efficienttrainingwould thereforebefarmoresampleefficientthanonemightexpectbasedontrainingsmallmodelstoconvergence, withdatarequirementsgrowingveryslowlyasD ∼C0.27withtrainingcompute.",
    "(Section6) Optimal batch size: The ideal batch size for training these models is roughly a power of the loss only, andcontinuestobedeterminablebymeasuringthegradientnoisescale[MKAT18];itisroughly1-2million tokensatconvergenceforthelargestmodelswecantrain.",
    "(Section5.1)",
    "Takentogether,theseresultsshowthatlanguagemodelingperformanceimprovessmoothlyandpredictably as we appropriately scale up model size, data, and compute.",
    "We expect that larger language models will performbetterandbemoresampleefficientthancurrentmodels.",
    "3",
    "Larger models require fewer samples The optimal model size grows smoothly to reach the same performance with the loss target and compute budget Line color indicates Test Loss 10 10 number of parameters 103 106 109 8 8 103 Params 6 6 Compute-efficient 109 Params training stops far short of convergence 4 4 107 109 1011 10-9 10-6 10-3 100 Tokens Processed Compute (PF-days) Figure2 Weshowaseriesoflanguagemodeltrainingruns,withmodelsranginginsizefrom103 to109 parameters(excludingembeddings).",
    "M inc in re im as u e m s s n e e r g ia li l",
    "g s ib te ly ps <1 1 0 0 x 0 S x e B ri a a t l c S h t e S p iz s e D gr a o t w",
    "a r r e e q",
    "la u t",
    "i",
    "i r v e e m ly e s n l t o s w ly >1,000,000x Model Size O in p cr t e",
    "im as a e l s m v o e d r e y l",
    "q s u iz ic e k ly Figure3 Asmorecomputebecomesavailable,wecanchoosehowmuchtoallocatetowardstraininglarger models, using larger batches, and training for more steps.",
    "We illustrate this for a billion-fold increase in compute.",
    "Foroptimallycompute-efficienttraining,mostoftheincreaseshouldgotowardsincreasedmodel size.",
    "Arelativelysmallincreaseindataisneededtoavoidreuse.",
    "Oftheincreaseindata,mostcanbeusedto increaseparallelismthroughlargerbatchsizes,withonlyaverysmallincreaseinserialtrainingtimerequired.",
    "1.2 SummaryofScalingLaws ThetestlossofaTransformertrainedtoautoregressivelymodellanguagecanbepredictedusingapower-law whenperformanceislimitedbyonlyeitherthenumberofnon-embeddingparametersN,thedatasetsizeD, ortheoptimallyallocatedcomputebudgetC (seeFigure1): min 1.",
    "For models with a limited number of parameters, trained to convergence on sufficiently large datasets: L(N)=(N /N)αN ; α ∼0.076, N ∼8.8×1013(non-embeddingparameters) (1.1) c N c 2.",
    "Forlargemodelstrainedwithalimiteddatasetwithearlystopping: L(D)=(D /D)αD; α ∼0.095, D ∼5.4×1013(tokens) (1.2) c",
    "D c 3.",
    "When training with a limited amount of compute, a sufficiently large dataset, an optimally-sized model,andasufficientlysmallbatchsize(makingoptimal3useofcompute): L(C )",
    "= (cid:0) Cmin/C (cid:1)αm C in ; αmin ∼0.050, Cmin ∼3.1×108(PF-days) (1.3) min",
    "c min C c 3Wealsoobserveanempiricalpower-lawtrendwiththetrainingcomputeC (Figure1)whiletrainingatfixedbatch size,butitisthetrendwithC thatshouldbeusedtomakepredictions.Theyarerelatedbyequation(5.5).",
    "min 4 4.5 4.0 3.5 3.0 2.5 107 108 109 1010 Tokens in Dataset ssoL Loss vs Model and Dataset Size 4.4 4.0 Params 708M 3.6 302M 85M 3.2 3M 25M 393.2K 2.8 2.4 104 105 Estimated Smin ssoL Loss vs Model Size and Training Steps 108 107 106 )debme-non( sretemaraP Figure4 Left: Theearly-stoppedtestlossL(N,D)variespredictablywiththedatasetsizeD andmodel size N according to Equation (1.5).",
    "Right:",
    "After an initial transient period, learning curves for all model sizesN canbefitwithEquation(1.6), whichisparameterizedintermsofS , thenumberofstepswhen min trainingatlargebatchsize(detailsinSection5.1).",
    "TheserelationsholdacrosseightordersofmagnitudeinC ,sixordersofmagnitudeinN,andovertwo min ordersofmagnitudeinD. TheydependveryweaklyonmodelshapeandotherTransformerhyperparameters (depth,width,numberofself-attentionheads),withspecificnumericalvaluesassociatedwiththeWebtext2 training set [RWC+19].",
    "The power laws α ,α ,αmin specify the degree of performance improvement N D C expectedaswescaleupN, D, orC ; forexample, doublingthenumberofparametersyieldsalossthat min is smaller by a factor 2−αN = 0.95.",
    "The precise numerical values of N c ,C c min, and D c depend on the vocabularysizeandtokenizationandhencedonothaveafundamentalmeaning.",
    "Thecriticalbatchsize,whichdeterminesthespeed/efficiencytradeofffordataparallelism([MKAT18]),also roughlyobeysapowerlawinL: B B (L)= ∗ , B ∼2·108tokens, α ∼0.21 (1.4) crit L1/αB ∗ B Equation(1.1)and(1.2)togethersuggestthatasweincreasethemodelsize,weshouldincreasethedataset sizesublinearlyaccordingtoD ∝ N",
    "α α N D ∼ N0.74.",
    "Infact,wefindthatthereisasingleequationcombining (1.1)and(1.2)thatgovernsthesimultaneousdependenceonN andDandgovernsthedegreeofoverfitting: (cid:34)(cid:18) (cid:19)αN (cid:35)αD L(N,D)= N c αD + D c (1.5) N D withfitspicturedontheleftinfigure4.",
    "Weconjecturethatthisfunctionalformmayalsoparameterizethe trainedlog-likelihoodforothergenerativemodelingtasks.",
    "WhentrainingagivenmodelforafinitenumberofparameterupdatestepsS intheinfinitedatalimit,after aninitialtransientperiod,thelearningcurvescanbeaccuratelyfitby(seetherightoffigure4) (cid:18) N (cid:19)αN (cid:18) S (cid:19)αS L(N,S)=",
    "c + c (1.6) N S (S) min whereS ≈ 2.1×103 andα ≈ 0.76,andS (S)istheminimumpossiblenumberofoptimizationsteps c S min (parameterupdates)estimatedusingEquation(5.4).",
    "When training within a fixed compute budget C, but with no other constraints, Equation (1.6) leads to the predictionthattheoptimalmodelsizeN,optimalbatchsizeB,optimalnumberofstepsS,anddatasetsize Dshouldgrowas N ∝Cαm C in/αN, B ∝Cαm C in/αB, S ∝Cαm C in/αS, D =B·S (1.7) with αmin =1/(1/α +1/α +1/α ) (1.8) C S B N which closely matches the empirically optimal results N ∝ C0.73, B ∝ C0.24, and S ∝ C0.03.",
    "As the min min min computationalbudgetC increases,itshouldbespentprimarilyonlargermodels,withoutdramaticincreases in training time or dataset size (see Figure 3).",
    "This also implies that as models grow larger, they become increasingly sample efficient.",
    "In practice, researchers typically train smaller models for longer than would 5 be maximally compute-efficient because of hardware constraints.",
    "Optimal performance depends on total computeasapowerlaw(seeEquation(1.3)).",
    "WeprovidesomebasictheoreticalmotivationforEquation(1.5),ananalysisoflearningcurvefitsandtheir implicationsfortrainingtime,andabreakdownofourresultspertoken.",
    "Wealsomakesomebriefcompar-",
    "isonstoLSTMsandrecurrentTransformers[DGV+18].",
    "1.3 Notation Weusethefollowingnotation: • L–thecrossentropylossinnats.",
    "Typicallyitwillbeaveragedoverthetokensinacontext,butin somecaseswereportthelossforspecifictokenswithinthecontext.",
    "•",
    "N –thenumberofmodelparameters,excludingallvocabularyandpositionalembeddings • C ≈6NBS –anestimateofthetotalnon-embeddingtrainingcompute,whereB isthebatchsize, andSisthenumberoftrainingsteps(ieparameterupdates).",
    "WequotenumericalvaluesinPF-days, whereonePF-day=1015×24×3600=8.64×1019floatingpointoperations. • D–thedatasetsizeintokens • B – the critical batch size [MKAT18], defined and discussed in Section 5.1.",
    "Training at the crit criticalbatchsizeprovidesaroughlyoptimalcompromisebetweentimeandcomputeefficiency.",
    "• C – an estimate of the minimum amount of non-embedding compute to reach a given value of min theloss.",
    "Thisisthetrainingcomputethatwouldbeusedifthemodelweretrainedatabatchsize muchlessthanthecriticalbatchsize.",
    "• S –anestimateoftheminimalnumberoftrainingstepsneededtoreachagivenvalueoftheloss.",
    "min Thisisalsothenumberoftrainingstepsthatwouldbeusedifthemodelweretrainedatabatchsize muchgreaterthanthecriticalbatchsize.",
    "• α X –power-lawexponentsforthescalingofthelossasL(X) ∝ 1/XαX whereX canbeanyof N,D,C,S,B,Cmin.",
    "2 BackgroundandMethods WetrainlanguagemodelsonWebText2,anextendedversionoftheWebText[RWC+19]dataset,tokenized using byte-pair encoding [SHB15] with a vocabulary size n = 50257.",
    "We optimize the autoregres- vocab sivelog-likelihood(i.e.",
    "cross-entropyloss)averagedovera1024-tokencontext,whichisalsoourprincipal performance metric.",
    "We record the loss on the WebText2 test distribution and on a selection of other text distributions.",
    "Weprimarilytraindecoder-only[LSP+18,RNSS18]Transformer[VSP+17]models, though wealsotrainLSTMmodelsandUniversalTransformers[DGV+18]forcomparison. 2.1 ParameterandComputeScalingofTransformers WeparameterizetheTransformerarchitectureusinghyperparametersn (numberoflayers), d (di- layer model mensionoftheresidualstream),d",
    "(dimensionoftheintermediatefeed-forwardlayer),d (dimensionof ff attn theattentionoutput),andn (numberofattentionheadsperlayer).",
    "Weincluden tokensintheinput heads ctx context,withn =1024exceptwhereotherwisenoted.",
    "ctx WeuseN todenotethemodelsize,whichwedefineasthenumberofnon-embeddingparameters N ≈2d n (2d +d ) model layer attn ff =12n d2 withthestandard d",
    "=d /4=d (2.1) layer model attn ff model wherewehaveexcludedbiasesandothersub-leadingterms.",
    "Ourmodelsalsohaven d parameters vocab model in an embedding matrix, and use n d parameters for positional embeddings, but we do not include ctx model thesewhendiscussingthe‘modelsize’N;wewillseethatthisproducessignificantlycleanerscalinglaws.",
    "EvaluatingaforwardpassoftheTransformerinvolvesroughly C ≈2N +2n n d (2.2) forward layer ctx model add-multiply operations, where the factor of two comes from the multiply-accumulate operation used in matrixmultiplication.",
    "Amoredetailedper-operationparameterandcomputecountisincludedinTable1. 6 Operation Parameters FLOPsperToken Embed (n +n )d 4d vocab ctx model model Attention: QKV n d 3d 2n",
    "d 3d layer model attn layer model attn Attention: Mask — 2n n d layer",
    "ctx attn Attention: Project n d",
    "d 2n d",
    "d layer attn model layer attn embd Feedforward n 2d d 2n 2d",
    "d layer model ff layer model ff De-embed — 2d n model vocab Total(Non-Embedding) N =2d n (2d +d ) C =2N +2n n d model layer attn ff forward layer ctx attn Table 1",
    "Parameter counts and compute (forward pass) estimates for a Transformer model.",
    "Sub-leading termssuchasnonlinearities,biases,andlayernormalizationareomitted.",
    "For contexts and models with d > n /12, the context-dependent computational cost per token is a model ctx relatively small fraction of the total compute.",
    "Since we primarily study models where d (cid:29) n /12, model ctx wedonotincludecontext-dependenttermsinourtrainingcomputeestimate.",
    "Accountingforthebackwards pass(approximatelytwicethecomputeastheforwardspass), wethendefinetheestimatednon-embedding computeasC ≈6N floatingpointoperatorspertrainingtoken.",
    "2.2 TrainingProcedures Unlessotherwisenoted, wetrainmodelswiththeAdamoptimizer[KB14]forafixed2.5×105 stepswith a batch size of 512 sequences of 1024 tokens.",
    "Due to memory constraints, our largest models (more than 1B parameters) were trained with Adafactor",
    "[SS18].",
    "We experimented with a variety of learning rates and schedules,asdiscussedinAppendixD.6.",
    "Wefoundthatresultsatconvergencewerelargelyindependentof learning rate schedule.",
    "Unless otherwise noted, all training runs included in our data used a learning rate schedulewitha3000steplinearwarmupfollowedbyacosinedecaytozero.",
    "2.3 Datasets We train our models on an extended version of the WebText dataset described in [RWC+19].",
    "The original WebTextdatasetwasawebscrapeofoutboundlinksfromRedditthroughDecember2017whichreceivedat least3karma.",
    "Inthesecondversion,WebText2,weaddedoutboundRedditlinksfromtheperiodofJanuary to October 2018, also with a minimum of 3 karma.",
    "The karma threshold served as a heuristic for whether people found the link interesting or useful.",
    "The text of the new links was extracted with the Newspaper3k pythonlibrary.",
    "Intotal,thedatasetconsistsof20.3Mdocumentscontaining96GBoftextand1.62×1010 words (as defined by wc).",
    "We then apply the reversible tokenizer described in [RWC+19], which yields 2.29×1010 tokens.",
    "Wereserve6.6×108 ofthesetokensforuseasatestset,andwealsotestonsimilarly- prepared samples of Books Corpus [ZKZ+15], Common Crawl",
    "[Fou], English Wikipedia, and a collection ofpublicly-availableInternetBooks.",
    "3 EmpiricalResultsandBasicPowerLaws To characterize language model scaling we train a wide variety of models, varying a number of factors including: • Modelsize(ranginginsizefrom768to1.5billionnon-embeddingparameters) • Datasetsize(rangingfrom22millionto23billiontokens) • Shape(includingdepth,width,attentionheads,andfeed-forwarddimension) • Contextlength(1024formostruns,thoughwealsoexperimentwithshortercontexts) • Batchsize(219formostruns,butwealsovaryittomeasurethecriticalbatchsize) 7 10% 8% 6% 4% 2% 0% Feed-Fo 5 r 0 w M a r P d a R ra a m ti e o te ( r d s ff / dmodel) Aspect Ratio (dmodel / nlayer)",
    "Attention",
    "He 2 a 5 d M D P",
    "im ara e m ns e i t o e n rs (dmodel / nhead)",
    "esaercnI ssoL A wide range of architectures achieve similar performance 22% additional compute compensates for 1% loss increase Figure 5 Performance depends very mildly on model shape when the total number of non-embedding parametersN isheldfixed.",
    "Thelossvariesonlyafewpercentoverawiderangeofshapes.",
    "Smalldifferences inparametercountsarecompensatedforbyusingthefittoL(N)asabaseline.",
    "Aspectratioinparticularcan varybyafactorof40whileonlyslightlyimpactingperformance;an(n ,d )",
    "= (6,4288)reachesa layer model losswithin3%ofthe(48,1600)modelusedin[RWC+19].",
    "7 6 5 4 3 2 106 107 108 109 Parameters (with embedding)",
    "ssoL tseT 7 6 5 0",
    "Layer 4 1 Layer 2 Layers 3 Layers 3 6 Layers >6 Layers 2 103 104 105 106 107 108 109 Parameters (non-embedding)",
    "ssoL tseT 1 Layer 2 Layers 3 Layers 6 Layers >6 Layers Figure 6 Left: When we include embedding parameters, performance appears to depend strongly on the numberoflayersinadditiontothenumberofparameters.",
    "Right:",
    "Whenweexcludeembeddingparameters, theperformanceofmodelswithdifferentdepthsconvergetoasingletrend.",
    "Onlymodelswithfewerthan2 layersorwithextremedepth-to-widthratiosdeviatesignificantlyfromthetrend.",
    "In this section we will display data along with empirically-motivated fits, deferring theoretical analysis to latersections.",
    "3.1 ApproximateTransformerShapeandHyperparameterIndependence Transformerperformancedependsveryweaklyontheshapeparametersn ,n ,andd whenwehold layer heads ff the total non-embedding parameter count N fixed.",
    "To establish these results we trained models with fixed sizewhilevaryingasinglehyperparameter.",
    "Thiswassimplestforthecaseofn .",
    "Whenvaryingn , heads layer we simultaneously varied d while keeping N ≈ 12n d2 fixed.",
    "Similarly, to vary d at fixed model layer model ff modelsizewealsosimultaneouslyvariedthed parameter,asrequiredbytheparametercountsinTable model 1.Independenceofn wouldfollowifdeeperTransformerseffectivelybehaveasensemblesofshallower layers models,ashasbeensuggestedforResNets[VWB16].",
    "TheresultsareshowninFigure5. 3.2 PerformancewithNon-EmbeddingParameterCountN InFigure6wedisplaytheperformanceofawidevarietyofmodels,rangingfromsmallmodelswithshape (n ,d )",
    "= (2,128) through billion-parameter models, ranging in shape from (6,4288) through layer model (207,768).",
    "Here we have trained to near convergence on the full WebText2 dataset and observe no over- fitting(exceptpossiblyfortheverylargestmodels).",
    "AsshowninFigure1,wefindasteadytrendwithnon-embeddingparametercountN,whichcanbefittothe firsttermofEquation(1.5),sothat (cid:18) N (cid:19)αN L(N)≈ c (3.1) N 8 Transformers asymptotically outperform LSTMs LSTM plateaus after <100 tokens due to improved use of long contexts Transformer improves through the whole context Test Loss 5.4 Per-token Test Loss 6 4.8 4.2 LSTMs 4 Parameters: 3.6 400K 1 Layer 5 400K 2 Layers 2M 3.0 Transformers 4 Layers 3M 3 200M 2.4 300M 2 105 106 107 108 109 101 102 103 Parameters (non-embedding)",
    "Token Index in Context Figure7 To observe these trends it is crucial to study performance as a function of N; if we instead use the total parametercount(includingtheembeddingparameters)thetrendissomewhatobscured(seeFigure6).",
    "This suggeststhattheembeddingmatrixcanbemadesmallerwithoutimpactingperformance,ashasbeenseenin recentwork[LCG+19].",
    "AlthoughthesemodelshavebeentrainedontheWebText2dataset,theirtestlossonavarietyofotherdatasets isalsoapower-lawinN withnearlyidenticalpower,asshowninFigure8.",
    "3.2.1 ComparingtoLSTMsandUniversalTransformers",
    "In Figure 7 we compare LSTM and Transformer performance as a function of non-embedding parameter count N. The LSTMs were trained with the same dataset and context length.",
    "We see from these figures thattheLSTMsperformaswellasTransformersfortokensappearingearlyinthecontext,butcannotmatch theTransformerperformanceforlatertokens.",
    "Wepresentpower-lawrelationshipsbetweenperformanceand contextpositionAppendixD.5,whereincreasinglylargepowersforlargermodelssuggestimprovedability toquicklyrecognizepatterns.",
    "WealsocomparetheperformanceofstandardTransformerstorecurrentTransformers[DGV+18]inFigure 17intheappendix.",
    "Thesemodelsre-useparameters,andsoperformslightlybetterasafunctionofN,atthe costofadditionalcomputeper-parameter.",
    "3.2.2 GeneralizationAmongDataDistributions Wehavealsotestedourmodelsonasetofadditionaltextdatadistributions.",
    "Thetestlossonthesedatasets asafunctionofmodelsizeisshowninFigure8;inallcasesthemodelsweretrainedonlyontheWebText2 dataset.",
    "Weseethatthelossontheseotherdatadistributionsimprovessmoothlywithmodelsize, indirect parallelwiththeimprovementonWebText2.",
    "Wefindthatgeneralizationdependsalmostexclusivelyonthe in-distributionvalidationloss,anddoesnotdependonthedurationoftrainingorproximitytoconvergence.",
    "Wealsoobservenodependenceonmodeldepth(seeAppendixD.8).",
    "3.3",
    "PerformancewithDatasetSizeandCompute WedisplayempiricaltrendsforthetestlossasafunctionofdatasetsizeD(intokens)andtrainingcompute C inFigure1.",
    "ForthetrendwithDwetrainedamodelwith(n ,n )=(36,1280)onfixedsubsetsoftheWebText2 layer embd dataset.",
    "Westoppedtrainingoncethetestlossceasedtodecrease.",
    "Weseethattheresultingtestlossescanbe fitwithsimplepower-law (cid:18) D (cid:19)αD L(D)≈ c (3.2) D inthedatasetsize.",
    "ThedataandfitappearinFigure1.",
    "Thetotalamountofnon-embeddingcomputeusedduringtrainingcanbeestimatedasC = 6NBS,where B isthebatchsize,S isthenumberofparameterupdates,andthefactorof6accountsfortheforwardand backwardpasses.",
    "ThusforagivenvalueofC wecanscanoverallmodelswithvariousN tofindthemodel 9 7 6 5 4 3 104 105 106 107 108 109 Parameters (non-embedding)",
    "ssoL tseT 5.0 WebText2 (Test) Internet Books 4.5 Books Wikipedia 4.0 Common Crawl 3.5 3.0 2.5 5.0 4.5 4.0 3.5 3.0 2.5 Test Loss on Training Distribution noitubirtsiD rehtO",
    "no ssoL Books during training Wikipedia during training Books at convergence Wikipedia at convergence Figure8 Left: Generalizationperformancetootherdatadistributionsimprovessmoothlywithmodelsize, with only a small and very slowly growing offset from the WebText2 training distribution.",
    "Right:",
    "Gener- alization performance depends only on training distribution performance, and not on the phase of training.",
    "Wecomparegeneralizationofconvergedmodels(points)tothatofasinglelargemodel(dashedcurves)asit trains.",
    "with the best performance on step S = C .",
    "Note that in these results the batch size B remains fixed for 6BS allmodels, whichmeansthattheseempiricalresultsarenottrulyoptimal.",
    "Wewillaccountforthisinlater sectionsusinganadjustedC toproducecleanertrends.",
    "min Theresultappearsastheheavyblacklineontheleft-handplotinFigure1.",
    "Itcanbefitwith (cid:18) C (cid:19)αC L(C)≈ c (3.3) C Thefigurealsoincludesimagesofindividuallearningcurvestoclarifywhenindividualmodelsareoptimal.",
    "Wewillstudytheoptimalallocationofcomputemorecloselylateron.",
    "Thedatastronglysuggeststhatsample efficiencyimproveswithmodelsize,andwealsoillustratethisdirectlyinFigure19intheappendix.",
    "4 ChartingtheInfiniteDataLimitandOverfitting In Section 3 we found a number of basic scaling laws for language modeling performance.",
    "Here we will study the performance of a model of size N trained on a dataset with D tokens while varying N and D simultaneously.",
    "Wewillempiricallydemonstratethattheoptimallytrainedtestlossaccordswiththescaling lawofEquation(1.5).Thisprovidesguidanceonhowmuchdatawewouldneedtotrainmodelsofincreasing sizewhilekeepingoverfittingundercontrol.",
    "4.1 ProposedL(N,D)Equation Wehavechosentheparameterization(1.5)(repeatedhereforconvenience): (cid:34)(cid:18) (cid:19)αN (cid:35)αD L(N,D)= N c αD + D c (4.1) N D usingthreeprinciples:",
    "1. Changesinvocabularysizeortokenizationareexpectedtorescalethelossbyanoverallfactor.",
    "The parameterizationofL(N,D)(andallmodelsoftheloss)mustnaturallyallowforsucharescaling.",
    "2. FixingD andsendingN → ∞,theoveralllossshouldapproachL(D).",
    "Conversely,fixingN and sendingD →∞thelossmustapproachL(N).",
    "3. L(N,D)shouldbeanalyticatD =∞,sothatithasaseriesexpansionin1/Dwithintegerpowers.",
    "Theoreticalsupportforthisprincipleissignificantlyweakerthanforthefirsttwo.",
    "Our choice of L(N,D) satisfies the first requirement because we can rescale N ,D with changes in the c c vocabulary.",
    "ThisalsoimpliesthatthevaluesofN ,D havenofundamentalmeaning.",
    "c",
    "c 10 4.5 4.0 3.5 3.0 2.5 106 107 108 109 Params (non-embed)",
    "ssoL tseT Data Size Bottleneck 0.5 Data Size 0.4 21M 43M 86M 0.3 172M 344M 0.2 688M 1.4B 22.0B 0.1 0.0 10 4 10 3 10 2 10 1 N N/D/D 1 )",
    "=D(L/L Overfitting Data Size 21M 43M 86M 172M 344M 688M 1.4B 22.0B Figure9 Theearly-stoppedtestlossL(N,D)dependspredictablyonthedatasetsizeDandmodelsizeN accordingtoEquation(1.5).",
    "Left:ForlargeD,performanceisastraightpowerlawinN. Forasmallerfixed D, performance stops improving as N increases and the model begins to overfit.",
    "(The reverse is also true, αN seeFigure4.)",
    "Right: TheextentofoverfittingdependspredominantlyontheratioNαD/D,aspredictedin equation(4.3).",
    "Thelineisourfittothatequation.",
    "Sincewestoptrainingearlywhenthetestlossceasestoimproveandoptimizeallmodelsinthesameway,we expectthatlargermodelsshouldalwaysperformbetterthansmallermodels.",
    "ButwithfixedfiniteD,wealso donotexpectanymodeltobecapableofapproachingthebestpossibleloss(ietheentropyoftext).Similarly, amodelwithfixedsizewillbecapacity-limited.",
    "Theseconsiderationsmotivateoursecondprinciple.",
    "Note thatknowledgeofL(N)atinfiniteDandL(D)atinfiniteN fullydeterminesalltheparametersinL(N,D).",
    "The third principle is more speculative.",
    "There is a simple and general reason one might expect overfitting to scale ∝ 1/D at very large D. Overfitting should be related to the variance or the signal-to-noise ratio of the dataset [AS17], and this scales as 1/D.",
    "This expectation should hold for any smooth loss function, sinceweexpecttobeabletoexpandthelossabouttheD → ∞limit.",
    "However,thisargumentassumesthat 1/Dcorrectionsdominateoverothersourcesofvariance,suchasthefinitebatchsizeandotherlimitsonthe efficacyofoptimization.",
    "Withoutempiricalconfirmation,wewouldnotbeveryconfidentofitsapplicability.",
    "Our third principle explains the asymmetry between the roles of N and D in Equation (1.5).",
    "Very similar symmetric expressions4 are possible, but they would not have a 1/D expansion with integer powers, and wouldrequiretheintroductionofanadditionalparameter.",
    "Inanycase,wewillseethatourequationforL(N,D)fitsthedatawell,whichisthemostimportantjustifi- cationforourL(N,D)ansatz.",
    "4.2 Results Weregularizeallourmodelswith10%dropout, andbytrackingtestlossandstoppingonceitisnolonger decreasing.",
    "The results are displayed in Figure 9, including a fit to the four parameters α ,α ,N ,D in N D c c Equation(1.5):",
    "Parameter α α N D N D c c Value 0.076 0.103 6.4×1013 1.8×1013 Table2 FitstoL(N,D) Weobtainanexcellentfit, withtheexceptionoftherunswherethedatasethasbeenreducedbyafactorof 1024, toabout2×107 tokens.",
    "Withsuchasmalldataset, anepochconsistsofonly40parameterupdates.",
    "Perhapssuchatinydatasetrepresentsadifferentregimeforlanguagemodeling,asoverfittinghappensvery early in training (see Figure 16).",
    "Also note that the parameters differ very slightly from those obtained in Section3,asherewearefittingthefullL(N,D)ratherthanjustL(N,∞)orL(∞,D).",
    "Tocharttheborderlandsoftheinfinitedatalimit,wecandirectlystudytheextentofoverfitting.",
    "Forallbut the largest models, we see no sign of overfitting when training with the full 22B token WebText2 dataset, so we can take it as representative of D = ∞.",
    "Thus we can compare finite D to the infinite data limit by 4Forexample,onemighthaveusedL(N,D)= (cid:2)(cid:0)Nc (cid:1)αN + (cid:0)Dc (cid:1)αD(cid:3)β ,butthisdoesnothavea1/Dexpansion.",
    "N D 11 106 105 104 103 101 6×100 4×100 3×100 WebText2 Train Loss )",
    "snekoT( eziS hctaB lacitirC Critical Batch Size vs. Performance Empirical Bcrit, N=3M Empirical Bcrit, N=85M Bcrit=2.1×108tokens",
    "L 4.8 Noise Scale Measurement Figure10 ThecriticalbatchsizeB followsapowerlawinthelossasperformanceincrease,anddoes crit not depend directly onthe model size.",
    "We findthat the critical batch size approximatelydoubles for every 13%decreaseinloss.",
    "B ismeasuredempiricallyfromthedatashowninFigure18,butitisalsoroughly crit predictedbythegradientnoisescale,asin[MKAT18].",
    "defining L(N,D) δL(N,D)≡ −1 (4.2) L(N,∞) andstudyingitasafunctionofN,D.Infact,weseeempiricallythatδLdependsonlyaspecificcombination ofN andD,asshowninFigure16.",
    "ThisfollowsfromthescalinglawofEquation(1.5),whichimplies (cid:32) (cid:18) (cid:19)αN (cid:33)αD δL≈ 1+ N αD D c −1 (4.3) N D c NotethatatlargeDthisformulaalsohasaseriesexpansioninpowersof1/D. Weestimatethatthevariationinthelosswithdifferentrandomseedsisroughly0.02, whichmeansthatto avoidoverfittingwhentrainingtowithinthatthresholdofconvergencewerequire D (cid:38)(5×103)N0.74 (4.4) With this relation, models smaller than 109 parameters can be trained with minimal overfitting on the 22B token WebText2 dataset, but our largest models will encounter some mild overfitting.",
    "More generally, this relationshowsthatdatasetsizemaygrowsub-linearlyinmodelsizewhileavoidingoverfitting.",
    "Notehowever that this does not typically represent maximally compute-efficient training.",
    "We should also emphasize that wehavenotoptimizedregularization(egthedropoutprobability)whilevaryingdatasetandmodelsize.",
    "5 ScalingLawswithModelSizeandTrainingTime In this section we will demonstrate that a simple scaling law provides a good description for the loss as a function of model size N and training time.",
    "First we will explain how to use the results of [MKAT18] to define a universal training step S , which accounts for the fact that most of our models have not been min trainedatanoptimalbatchsize.",
    "Thenwewilldemonstratethatwecanfitthemodelsizeandtrainingtime dependenceofthelossusingEquation(1.6).",
    "Laterwewillusetheseresultstopredicttheoptimalallocation oftrainingcomputebetweenmodelsizeandtrainingtime,andthenconfirmthatprediction.",
    "5.1 AdjustmentforTrainingatB (L) crit Asimpleempiricaltheoryforthebatchsizedependenceoftrainingwasdevelopedin[MKAT18](seealso [SLA+18, ZLN+19]).",
    "It was argued that there is a critical batch size B for training; for B up to B crit crit thebatchsizecanbeincreasedwithveryminimaldegradationincompute-efficiency,whereasforB >B crit increasesinBresultindiminishingreturns.",
    "Itwasalsoarguedthatthegradientnoisescaleprovidesasimple 12 predictionforB ,andthatneitherdependsdirectlyonmodelsizeexceptthroughthevalueofthelossthat crit has been attained.",
    "These results can be used to predict how training time and compute will vary with the batchsize.",
    "Toutilizebothtrainingtimeandcomputeaseffectivelyaspossible,itisbesttotrainwithabatch sizeB ≈B .",
    "TrainingatB (cid:29)B minimizesthenumberoftrainingsteps,whileB (cid:28)B minimizes crit crit crit theuseofcompute.",
    "Morespecifically,itwasdemonstratedthatforawidevarietyofneuralnetworktasks,thenumberoftraining stepsS andthenumberofdataexamplesprocessedE =BS satisfythesimplerelation (cid:18) (cid:19)(cid:18) (cid:19)",
    "S E −1 −1 =1 (5.1) S E min min whentrainingtoanyfixedvalueofthelossL. HereS istheminimumnumberofstepsnecessarytoreach min L,whileE istheminimumnumberofdataexamplesthatmustbeprocessed.",
    "min We demonstrate the relation (5.1) for Transformers in Figure 18 in the appendix.",
    "This relation defines the criticalbatchsize E B (L)≡ min (5.2) crit S min whichisafunctionofthetargetvalueoftheloss.",
    "Trainingatthecriticalbatchsizemakesaroughlyoptimal time/computetradeoff,requiring2S trainingstepsandprocessingE =2E dataexamples.",
    "min min InFigure10wehaveplottedthecriticalbatchsizeandgradientnoisescale5asafunctionoftraininglossfor twodifferentmodels.",
    "WeseethatB (L)isindependentofmodelsize,andonlydependsonthelossL. So crit thepredictionsof[MKAT18]continuetoholdforTransformerlanguagemodels.",
    "Thecriticalbatchsizecan befitwithapower-lawintheloss B B (L)≈ ∗ (5.3) crit L1/αB whereB ≈2×108andα ≈0.21. ∗ B WehavechosenthisparameterizationforB (L)becauseasthelossapproachesitsminimumvalueL , crit min the gradient noise scale is expected to diverge, and we expect B to track this noise scale.",
    "We do not crit knowL ,asweseenosignthatourmodelsareapproachingit,butL > 0sincetheentropyofnatural min min languageisnon-zero.",
    "SinceapparentlyL ismuchsmallerthanthevaluesofLwehaveachieved,weused min aparameterizationwhereB divergesasL→0.",
    "crit WewilluseB (L)toestimatetherelationbetweenthenumberoftrainingstepsS whiletrainingatbatch crit sizeB =219tokensandthenumberoftrainingstepswhiletrainingatB (cid:29)B .",
    "Thisissimply crit S S (S)≡ (minimumsteps,atB (cid:29)B ) (5.4) min 1+B (L)/B crit crit foranygiventargetvalueLfortheloss.",
    "ThisalsodefinesacriticalvalueofthecomputeneededtotraintoL withamodelofsizeN ifweweretotrainatB (cid:28)B (L).",
    "Thisis crit C C (C)≡ (minimumcompute,atB (cid:28)B ) (5.5) min 1+B/B (L) crit crit whereC =6NBS estimatesthe(non-embedding)computeusedatbatchsizeB. 5.2 ResultsforL(N,S )andPerformancewithModelSizeandCompute min NowwewilluseS definedinEquation(5.4)toobtainasimpleanduniversalfitforthedependenceofthe min lossonmodelsizeandtrainingtimeintheinfinitedatalimit.",
    "Wewillfitthestable,Adam-optimizedtraining runsusingEquation(1.6),repeatedhereforconvenience: (cid:18) N (cid:19)αN (cid:18) S (cid:19)αS L(N,S )= c + c (5.6) min N S min fortheloss.",
    "Weincludealltrainingstepsafterthewarmupperiodofthelearningrateschedule,andfindafit tothedatawiththeparameters: 5Although the critical batch size roughly matches the gradient noise scale, we are using a direct measurements of B fromFigures18and10forallourlateranalyses.",
    "crit 13 8 7 6 5 4 3 2 104 106 108 Parameters (non-embedding)",
    "ssoL tseT Performance vs Compute Budget 100 10 1 10 2 10 3 10 4 10 5 ssyad-FP 5.4 4.8 4.2 3.6 3.0 2.4 106 107 108 109 Parameters (non-embedding)",
    "ssoL",
    "tseT Performance vs Steps 105 104 spetS Figure 11 When we hold either total compute or number of training steps fixed, performance follows L(N,S) from Equation (5.6).",
    "Each value of compute budget has an associated optimal model size that maximizesperformance.",
    "MediocrefitsatsmallSareunsurprising,asthepower-lawequationforthelearning curvesbreaksdownveryearlyintraining.",
    "Parameter α α N S N S c c Value 0.077 0.76 6.5×1013 2.1×103 Table3 FitstoL(N,S) Withtheseparameters,weobtainthelearningcurvefitsinFigure4.Thoughthefitsareimperfect,webelieve theyarequitecompellinggiventhesimplicityofEquation(5.6).",
    "Thedataandfitscanbevisualizedinadifferentandmoreinterestingway,asshowninFigure11.",
    "Therewe studythetestlossasafunctionofmodelsizewhilefixingeitherthetotalnon-embeddingcomputeC used intraining,orthenumberofstepsS. ForthefitsweuseEquation(5.5)and(5.4)alongwiththeparameters aboveandEquation(5.6).",
    "The power-law dependence of the loss on S reflects the interplay of optimizer dynamics and the loss min landscape.",
    "Sincethefitsarebestlateintraining,whenthelossmaybeapproximatelyquadratic,thepower- lawshouldprovideinformationaboutthespectrumoftheHessianoftheloss.",
    "Itsuniversalitysuggeststhat theHessianeigenvaluedensityisroughlyindependentofmodelsize.",
    "5.3 LowerBoundonEarlyStoppingStep The results for L(N,S ) can be used to derive a lower-bound (and rough estimate) of the step at which min earlystoppingshouldoccurwhentrainingisdatalimited.",
    "ItismotivatedbytheideathatfiniteandinfiniteD learningcurvesforagivenmodelwillbeverysimilaruntilwereachS ≈ S .",
    "Thusoverfittingshould min stop beproportionaltothecorrectionfromsimplyendingtrainingatS .ThiswillunderestimateS ,because stop stop inrealitythetestlosswilldecreasemoreslowlywhenwehaveafiniteD,andthereforewewillrequiremore trainingstepstoreachtheoptimaltestlossatfiniteD. Thislineofreasoningleadstotheinequality S S (N,D)(cid:38)",
    "c (5.7) stop [L(N,D)−L(N,∞)]1/αS where L(N,∞) is the converged loss, evaluated with infinite available data.",
    "This inequality and its com- parison to the empirical data is displayed in Figure 16 in the appendix.",
    "In that figure, the values of S stop and L(N,D) are empirical (though S is adjusted to mimic training at B (cid:29) B ), while L(N,∞) is stop crit computedfromthefittoL(N,D)evaluatedatD =∞.",
    "6",
    "OptimalAllocationoftheComputeBudget We displayed the empirical trend of performance as a function of the computation used during training in the top-right of Figure 1.",
    "However, this result involved training at a fixed batch size B, whereas we know 14 Smaller models require more steps to train, while larger models require fewer Models between 0.6x and 2.2x the optimal size can be trained with a 20% larger compute budget Our framework does not capture early training dynamics",
    "Figure12 Left: Givenafixedcomputebudget,aparticularmodelsizeisoptimal,thoughsomewhatlarger orsmallermodelscanbetrainedwithminimaladditionalcompute.",
    "Right: Modelslargerthanthecompute-",
    "efficientsizerequirefewerstepstotrain,allowingforpotentiallyfastertrainingifsufficientadditionalparal- lelismispossible.",
    "Notethatthisequationshouldnotbetrustedforverylargemodels,asitisonlyvalidinthe power-lawregionofthelearningcurve,afterinitialtransienteffects.",
    "7 6 5 4 3 2 10 8 10 6 10 4 10 2 100 Compute (PF-days), non-embedding ssoL tseT L=(C min/2.3 108) 0.050 L=(C/2.0 107) 0.057 Figure 13 When adjusting performance to simulate training far below the critical batch size, we find a somewhatalteredpowerlawforL(C )whencomparedwiththefullyempiricalresults.",
    "Theconspicuous min lump at 10−5 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks in the power-law fits.",
    "It is the L(C ) trend that we expect to provide a reliable extrapolation for larger min compute.",
    "that in fact we could train more efficiently6 by training at the batch size B discussed in Section 5.1.",
    "crit Largeandsmallvaluesofthelosscouldhavebeenachievedwithfewersamplesorfewersteps,respectively, and correcting for this inefficiency by standardizing to the critical batch size results in cleaner and more predictabletrends.",
    "In this section we will adjust for this oversight.",
    "More importantly, we will use the results of Section 5 to determine the optimal allocation of compute between model size N and the quantity of data processed duringtraining,namely2B S .",
    "Wewilldeterminethisallocationbothempiricallyandtheoretically,by crit min usingtheequationforL(N,S ),andwewilldemonstratethatthesemethodsagree.",
    "min 6.1 OptimalPerformanceandAllocations LetusfirststudythelossasafunctionoftheoptimallyallocatedcomputefromEquation(5.5).",
    "Theresultis plottedinFigure13,alongwithapower-lawfit.",
    "WeseethatascomparedtothecomputeplotofFigure1,the newfitwithC issomewhatimproved.",
    "min GivenL(C ),itisnaturaltoaskfortheoptimalmodelsizeN(C )thatprovidestheminimallosswitha min min givenquantityoftrainingcompute.",
    "TheoptimalmodelsizeisshowninFigure14.",
    "WeobservethatN(C )",
    "min 6OnemightaskwhywedidnotsimplytrainatB inthefirstplace.",
    "Thereasonisthatitdependsnotonlyonthe crit modelbutalsoonthetargetvalueofthelosswewishtoachieve,andsoisamovingtarget.",
    "15 107 105 103 10 7 10 5 10 3 10 1 Compute (PF-days), non-embedding )gniddebme-non( sretemaraP N=(1.3 109) C0.73 min N=(1.6 109) C0.88 15000 10000 5000 0 10 7 10 5 10 3 10 1 Compute (PF-days), excluding embeddings spetS S (adjusted) min S min=(5.4 103) C m 0.",
    "i 0 n 3 S (fixed-batch) Figure14",
    "Left: EachvalueofthecomputebudgetC hasanassociatedoptimalmodelsizeN.",
    "Optimal min modelsizegrowsveryrapidlywithC , increasingby5xforeach10xincreaseincompute.",
    "Thenumber min ofdataexamplesprocessedmakesuptheremainderoftheincrease,growingrelativelymodestlybyonly2x.",
    "Right: Thebatch-adjustednumberofoptimizationstepsalsogrowsveryslowly,ifatall,meaningthatmost ofthegrowthindataexamplesprocessedcanbeusedforincreasedbatchsizes.",
    "canbefitverywellwithapower-law N(C )∝(C )",
    "0.73.",
    "(6.1) min min InFigure12,weshowtheeffectoftrainingmodelsofsub-optimalsizes(seeAppendixB.4).",
    "BydefinitionC",
    "≡ 6NB S,andsowecanuseN(C )toextractfurtherresults.",
    "Inparticular,since min crit min priorfitsshowB ∝ L−4.8 andL ∝ C−0.05,wecanconcludethatB ∝ C0.24.",
    "Thisleadsustoconclude min crit min thattheoptimalnumberofstepswillonlygrowveryslowlywithcompute,as S ∝(C )0.03, (6.2) min min matchingtheempiricalresultsinFigure14.Infactthemeasuredexponentissufficientlysmallthatourresults mayevenbeconsistentwithanexponentofzero.",
    "Thus we conclude that as we scale up language modeling with an optimal allocation of computation, we should predominantly increase the model size N, while simultaneously scaling up the batch size via B ∝ B withnegligibleincreaseinthenumberofserialsteps.",
    "Sincecompute-efficienttrainingusesrelatively crit fewoptimizationsteps,additionalworkonspeedingupearlytrainingdynamicsmaybewarranted.",
    "6.2 PredictionsfromL(N,S ) min",
    "The results for L(C ) and the allocations can be predicted from the L(N,S ) equation obtained in min min Section5.",
    "GivenourequationforL(N,S min ), wecansubstituteS min = C 6N m B in andthenfindtheminimum of the loss as a function of N, while fixing the training compute.",
    "We carry out this procedure in detail in AppendixB,wherewealsoprovidesomeadditionalpredictions.",
    "Forthelossasafunctionoftrainingcompute,wepredictthat (cid:18) Cmin(cid:19)αm C in L(C )",
    "= c (6.3) min C min where 1 αmin ≡ ≈0.054 (6.4) C 1/α +1/α +1/α S B N inexcellentagreementwiththeexponentofFigure13.",
    "Wealsopredictthat N(C min )∝(C min )αm C in/αN ≈(C min )0.71 (6.5) whichalsomatchesthescalingofFigure14towithinafewpercent.",
    "Ourscalinglawsprovideapredictive frameworkfortheperformanceoflanguagemodeling.",
    "16",
    "The intersection point is sensitive to the precise power-law parameters Figure15",
    "Farbeyondthemodelsizeswestudyempirically,wefindacontradictionbetweenourequations forL(C )andL(D)duetotheslowgrowthofdataneededforcompute-efficienttraining.",
    "Theintersection min marksthepointbeforewhichweexpectourpredictionstobreakdown.",
    "Thelocationofthispointishighly sensitivetothepreciseexponentsfromourpower-lawfits.",
    "6.3 ContradictionsandaConjecture Weobservenosignsofdeviationfromstraightpower-lawtrendsatlargevaluesofcompute,data,ormodel size.",
    "Ourtrendsmusteventuallyleveloff,though,sincenaturallanguagehasnon-zeroentropy.",
    "Indeed,thetrendsforcompute-efficienttrainingdescribedinthissectionalreadycontainanapparentcontra- diction.",
    "Atscalesseveralordersofmagnitudeabovethosedocumentedhere, theperformancepredictedby theL(C )scalinglawdecreasesbelowwhatshouldbepossiblegiventheslowgrowthintrainingdatawith min compute.",
    "This implies that our scaling laws must break down before this point, but we conjecture that the intersectionpointhasadeepermeaning: itprovidesanestimateofthepointatwhichTransformerlanguage modelsreachmaximalperformance.",
    "Since the amount of data used by compute-efficient training grows slowly with the compute budget, the performancepredictedbyL(C )eventuallyhitsalowerboundsetbytheL(D)powerlaw(seeFigure15).",
    "min Letusworkthisoutinmoredetail.",
    "Tokeepoverfittingundercontrol,theresultsofSection4implythatweshouldscalethedatasetsizeas D ∝N0.74 ∝C0.54 (6.6) min wherewehaveusedthecompute-efficientN(C )fromFigure14.",
    "min Let us compare this to the data requirements of compute-efficient training.",
    "If we train at the critical batch size(i.e.",
    "C =2C )andneverre-usedataduringtraining,wefindthatdatausagegrowswithcomputeas min D(C )",
    "= 2C min ≈ (cid:0) 4×1010tokens (cid:1) (C /PF-Day)0.26 (6.7) min 6N(C ) min min Thisisthemaximumrateatwhichthedatasetsizecanproductivelygrowwithcompute,sinceitmeansthat weareonlytrainingforasingleepoch.",
    "ButitgrowsthedatasetmuchmoreslowlythaninEquation(6.6).",
    "Itappearstoimplythatcompute-efficienttrainingwilleventuallyrunintoaproblemwithoverfitting,evenif thetrainingprocessneverre-usesanydata!",
    "According to Figure 1, we expect that when we are bottlenecked by the dataset size (ie by overfitting), the lossshouldscaleasL(D)∝D−0.095.ThisimpliesthatthelosswouldscalewithcomputeasL(D(C ))",
    "∝ min C−0.03 oncewearedata-limited.",
    "Onceagain,wehaveacontradiction,asthiswilleventuallyintersectwith min ourpredictionforL(C )fromFigure13,wherewefoundascalingL(C )∝C−0.050.",
    "min min min TheintersectionpointofL(D(C ))",
    "andL(C )occursat min min C∗ ∼104PF-Days N∗ ∼1012parameters, D∗ ∼1012tokens, L∗ ∼1.7nats/token (6.8) though the numerical values are highly uncertain, varying by an order or magnitude in either direction de- pendingontheprecisevaluesoftheexponentsfromthepower-lawfits.",
    "Themostobviousinterpretationis that our scaling laws break down at or before we reach this point, which is still many orders of magnitude awayinbothcomputeandmodelsize.",
    "17",
    "Onemightalsoconjecturethatthisintersectionpointhasadeepermeaning.",
    "Ifwecannotincreasethemodel size beyond N∗ without qualitatively different data requirements, perhaps this means that once we reach C∗ and N∗, we have extracted all of the reliable information available in natural language data.",
    "In this min interpretation, L∗ would provide a rough estimate for the entropy-per-token7 of natural language.",
    "In this scenario,wewouldexpectthelosstrendtoleveloffatorbeforeL∗. We can guess at the functional form of L(C ) as it levels off by considering a version of our training min dataset with added noise.",
    "For example, we could append a random string of tokens to each context shown to the model to artificially boost the loss by a constant additive factor.",
    "Then, the distance from the noise floorL−L wouldbeamoremeaningfulperformancemetric,withevenasmalldecreaseinthisdistance noise potentiallyrepresentingasignificantboostinqualitativeperformance.",
    "Sincetheartificialnoisewouldaffect allofourtrendsequally,thecriticalpointof6.8wouldnotchange(asidefromtheabsolutevalueofL∗),and maybemeaningfulevenifitoccursafterthelevelingoff.",
    "7 RelatedWork Powerlawscanarisefromawidevarietyofsources[THK18].",
    "Power-lawscalingswithmodelanddataset sizeindensityestimation[Was06]andinrandomforestmodels[Bia12]maybeconnectedwithourresults.",
    "These models suggest that power-law exponents may have a very rough interpretation as the inverse of the numberofrelevantfeaturesinthedata.",
    "Some early [BB01, Goo01] work found power-law scalings between performance and dataset size.",
    "More recent work [HNA+17, HAD19] also investigated scaling between model size and data size; their work is perhaps the closest to ours in the literature8.",
    "Note, however, that [HNA+17] found super-linear scaling of dataset size with model size, whereas we find a sub-linear scaling.",
    "There are some parallels between our findingsonoptimalallocationofcomputeand[Kom19],includingpower-lawlearningcurves.",
    "EfficientNets",
    "[TL19]alsoappeartoobeyanapproximatepower-lawrelationbetweenaccuracyandmodelsize.",
    "Veryrecent work[RRBS19b]studiesscalingwithbothdatasetsizeandmodelsizeforavarietyofdatasets, andfitsan ansatzsimilartoours.",
    "EfficientNet[TL19]advocatesscalingdepthandwidthexponentially(withdifferentcoefficients)foroptimal performanceofimagemodels,resultinginapower-lawscalingofwidthasafunctionofdepth.",
    "Wefindthat forlanguagemodelsthispowershouldberoughlyonewhenscalingup(aswidth/depthshouldremainfixed).",
    "Butmoreimportantly,wefindthattheprecisearchitecturalhyperparametersareunimportantcomparedtothe overallscaleofthelanguagemodel.",
    "In[VWB16]itwasarguedthatdeepmodelscanfunctionasensembles ofshallowermodels,whichcouldpotentiallyexplainthisfinding.",
    "Earlierwork[ZK16]hascomparedwidth anddepth,andfoundthatwideResNetscanoutperformdeepResNetsonimageclassification.",
    "Somestudies fix computation per data example, which tends to scale in proportion to the number of model parameters, whereasweinvestigatescalingwithbothmodelsizeandthequantityoftrainingcomputation.",
    "Variousworks[AS17,BHMM18]haveinvestigatedgeneralizationinhighlyoverparameterizedmodels,find- inga“jammingtransition”[GJS+19]whenthemodelsizereachesthedatasetsize(thismayrequiretraining many orders of magnitude beyond typical practice, and in particular does not use early stopping).",
    "We do not observe such a transition, and find that the necessary training data scales sublinearly in the model size.",
    "Expansionsinthemodelsize,particularlyatlargewidth[JGH18,LXS+19],mayprovideausefulframework forthinkingaboutsomeofourscalingrelations.",
    "Ourresultsonoptimization,suchastheshapeoflearning curves,canlikelybeexplainedusinganoisyquadraticmodel,whichcanprovidequiteaccuratepredictions [ZLN+19] in realistic settings.",
    "Making this connection quantitative will require a characterization of the Hessianspectrum[Pap18,GKX19,GARD18].",
    "8 Discussion Wehaveobservedconsistentscalingsoflanguagemodellog-likelihoodlosswithnon-embeddingparameter countN, datasetsizeD, andoptimizedtrainingcomputationC , asencapsulatedinEquations(1.5)and min (1.6).",
    "Conversely,wefindveryweakdependenceonmanyarchitecturalandoptimizationhyperparameters.",
    "SincescalingswithN,D,C arepower-laws,therearediminishingreturnswithincreasingscale.",
    "min 7Definingwordsusingthewcutility,theWebText2datasethas1.4tokensperwordand4.3characterspertoken.",
    "8Afterthisworkwascompleted,[RRBS19a]alsoappeared,whichmakessimilarpredictionsforthedependenceof lossonbothmodelanddatasetsize.",
    "18 WewereabletopreciselymodelthedependenceofthelossonN andD,andalternativelyonN andS,when theseparametersarevariedsimultaneously.",
    "Weusedtheserelationstoderivethecomputescaling,magnitude ofoverfitting,earlystoppingstep,anddatarequirementswhentraininglargelanguagemodels.",
    "Soourscaling relationsgobeyondmereobservationtoprovideapredictiveframework.",
    "Onemightinterprettheserelations as analogues of the ideal gas law, which relates the macroscopic properties of a gas in a universal way, independentofmostofthedetailsofitsmicroscopicconsituents.",
    "It is natural to conjecture that the scaling relations will apply to other generative modeling tasks with a maximum likelihood loss, and perhaps in other settings as well.",
    "To this purpose, it will be interesting to testtheserelationsonotherdomains,suchasimages,audio,andvideomodels,andperhapsalsoforrandom network distillation.",
    "At this point we do not know which of our results depend on the structure of natural language data, and which are universal.",
    "It would also be exciting to find a theoretical framework from which the scaling relations can be derived: a ‘statistical mechanics’ underlying the ‘thermodynamics’ we haveobserved.",
    "Suchatheorymightmakeitpossibletoderiveothermoreprecisepredictions,andprovidea systematicunderstandingofthelimitationsofthescalinglaws.",
    "Inthedomainofnaturallanguage,itwillbeimportanttoinvestigatewhethercontinuedimprovementonthe loss translates into improvement on relevant language tasks.",
    "Smooth quantitative change can mask major qualitative improvements: “more is different”.",
    "For example, the smooth aggregate growth of the economy provides no indication of the specific technological developments that underwrite it.",
    "Similarly, the smooth improvementsinlanguagemodellossmayhideseeminglyqualitativechangesincapability.",
    "Ourresultsstronglysuggestthatlargermodelswillcontinuetoperformbetter,andwillalsobemuchmore sample efficient than has been previously appreciated.",
    "Big models may be more important than big data.",
    "Inthiscontext,furtherinvestigationintomodelparallelismiswarranted.",
    "Deepmodelscanbetrainedusing pipelining[HCC+18],whichsplitsparametersdepth-wisebetweendevices,buteventuallyrequiresincreased batchsizesasmoredevicesareused.",
    "Widenetworksontheotherhandaremoreamenabletoparallelization",
    "[SCP+18], since large layers can be split between multiple workers with less serial dependency.",
    "Sparsity",
    "[CGRS19,GRK17]orbranching(e.g.[KSH12])mayallowforevenfastertrainingoflargenetworksthrough increasedmodelparallelism.",
    "Andusingmethodslike[WRH17,WYL19],whichgrownetworksastheytrain, itmightbepossibletoremainonthecompute-efficientfrontierforanentiretrainingrun.",
    "Acknowledgements We would like to thank Shan Carter, Paul Christiano, Jack Clark, Ajeya Cotra, Ethan Dyer, Jason Eisner, DannyHernandez,JacobHilton,BriceMenard,ChrisOlah,andIlyaSutskeverfordiscussionsandforfeed- backondraftsofthiswork.",
    "19",
    "Appendices A SummaryofPowerLaws Foreasierreference,weprovideasummarybelowofthekeytrendsdescribedthroughoutthepaper.",
    "Parameters Data Compute BatchSize Equation N ∞ ∞ Fixed L(N)=(N /N)αN",
    "c ∞ D EarlyStop Fixed L(D)=(D /D)αD",
    "c Optimal ∞ C Fixed L(C)=(C /C)αC (naive) c N D C B",
    "(cid:28)B L(C )",
    "= (cid:0) Cmin/C (cid:1)αm C in opt opt min crit min c min N D EarlyStop Fixed L(N,D)= (cid:20) (cid:0)Nc (cid:1)α α N D + Dc (cid:21)αD N D N ∞ S steps B L(N,S)= (cid:0)Nc (cid:1)αN + (cid:16)",
    "Sc (cid:17)αS N Smin(S,B) Table4 Theempiricalfittedvaluesforthesetrendsare: PowerLaw Scale(tokenization-dependent) α =0.076 N =8.8×1013params(non-embed)",
    "N c α =0.095 D =5.4×1013tokens D c α =0.057 C =1.6×107PF-days C c αmin =0.050 Cmin =3.1×108PF-days",
    "C c α =0.21 B =2.1×108tokens B ∗ α =0.76 S =2.1×103steps S c Table5 Theoptimalparametersforcomputeefficienttrainingaregivenby: Compute-EfficientValue PowerLaw Scale N =N ·CpN p =0.73 N =1.3·109params opt e min N e B (cid:28)B crit = L1 B /α ∗ B =B e C m pB in p B =0.24 B e =2.0·106tokens S =S ·CpS (lowerbound) p =0.03 S =5.4·103steps min e min S e D =D ·CpD (1epoch) p =0.27 D =2·1010tokens opt e min D e Table6 B EmpiricalModelofCompute-EfficientFrontier ThroughoutthisappendixallvaluesofC,S,andα areadjustedfortrainingatthecriticalbatchsizeB .",
    "C crit Wehaveleftoffthe‘adj’labeltoavoidclutteringthenotation.",
    "B.1 DefiningEquations Thepower-lawfittothelearningcurvesimpliesasimpleprescriptionforcompute-efficienttraining.",
    "Inthis appendix,wewillderivetheoptimalperformance,modelsize,andnumberoftrainingstepsasafunctionof 20 thecomputebudget.",
    "WestartwiththeEquation(1.6),repeatedhereforconvenience: (cid:18) N (cid:19)αN (cid:18) S (cid:19)αS L(N,S)=",
    "c + c .",
    "(B.1) N S Here, S represents the number of parameter updates when training at the critical batch size [MKAT18], whichwasdefinedinEquation(5.2)9: B B(L)= ∗ .",
    "(B.2) L1/",
    "αB We would like to determine optimal training parameters for a fixed compute budget, so we replace S = C/(6NB(L)),whereC isthenumberofFLOPsusedinthetrainingrun: (cid:18) N (cid:19)αN (cid:18) N (cid:19)αS L(N,C)= c + 6B S .",
    "(B.3)",
    "N ∗ c L1/αBC (cid:12)",
    "Now,weset∂ N L(cid:12) C =0tofindtheconditionforoptimality: ∂L(cid:12) 0= (cid:12) ∂N C =− α N N (cid:18) N N c (cid:19)αN + α N S (cid:18) 6B ∗ S c L1/ N αBC (cid:19)αS (cid:18) 1−5",
    "N L(cid:26)∂ ∂ N L (cid:26)(cid:12) (cid:12) (cid:26) C (cid:19) α (cid:18) N (cid:19)αN (cid:18) N (cid:19)αS =⇒ N c = 6B S (B.4) α S N ∗ c L1/αBC Equation(B.3)and(B.4)togetherdeterminethecompute-efficientfrontier.",
    "B.2 EfficientTraining Nowweassembletheimplicationsof(B.3)and(B.4).",
    "First,notethatinserting(B.4)into(B.3)yields (cid:18) (cid:19) α L(N (C),C)= 1+ N L(N ,∞), (B.5) eff α eff S whichimpliesthatforcompute-efficienttraining, weshouldtraintoafixedpercentage αN ≈ 10%above",
    "αS theconvergedloss.",
    "Next,let’sdeterminehowtheoptimallossdependsonthecomputebudget.",
    "Eliminating N yieldsapower-lawdependenceofperformanceoncompute: (cid:18) C (cid:19)αC L(C)= c (B.6) C wherewedefined α =1/(1/α +1/α +1/α )≈0.052 (B.7) C S B N (cid:18) α (cid:19)1/αS+1/αN (cid:18) α (cid:19)1/αS C =6N B S 1+ N S .",
    "(B.8)",
    "c c ∗ c α α S N Similarly,wecaneliminateLtofindN(C): N(C) (cid:18) C (cid:19)αC/αN (cid:18) α (cid:19)1/αN = 1+ N (B.9) N C",
    "α c c S and C (cid:18) α (cid:19)−1/αN (cid:18) C (cid:19)αC/αS S(C)= c 1+ N (B.10) 6N B α C c ∗ S",
    "c 9Thereisaslightambiguityhere: wecanimaginetrainingeitherataconstantbatchsizeB(L ),orwecould target insteadtrainatavariablebatchsizeB˜(L),whereB˜ istheinstantaneouscriticalbatchsize(asopposedtoB,whichis theaveragedversion).",
    "Thesetwoprescriptionsresultinthesamenumberofsteps,sowecanignorethissubtlety(see [MKAT18]).",
    "21 B.3 ComparisontoInefficient Typically,researcherstrainmodelsuntiltheyappeartobeclosetoconvergence.",
    "Inthissection,wecompare theefficienttrainingproceduredescribedabovetothismoretypicalsetup.",
    "Wedefineatheconvergencefactor f asthepercentdeviationfromtheconvergedloss: L(N,C)=(1+f)L(N,∞).",
    "(B.11)",
    "For compute-efficient training we have f = α /α",
    "≈ 10% from the previous section, but researchers N S typicallyuseamuchsmallervalue.",
    "Here,wechoosef(cid:48)",
    "= 2%asanestimate.",
    "Forafixedvalueoftheloss, wepredict: N (cid:18) 1+f (cid:19)1/αN f = ≈2.7 (B.12) N 1+f(cid:48) f(cid:48) S (cid:32) 1+ 1 (cid:33)1/αS f = f ≈0.13 (B.13) S 1+ 1 f(cid:48) f(cid:48)",
    "C N S f = f f ≈0.35 (B.14) C N S f(cid:48) f(cid:48) f(cid:48)",
    "So that compute-efficient training uses 7.7x fewer parameter updates, 2.7x more parameters, and 65% less computetoreachthesameloss.",
    "B.4 SuboptimalModelSizes WecansolveA.1tofindanexpressionfortheamountofcomputeneededtoreachagivenvalueoftheloss",
    "LwithamodelofsizeN: (cid:18)",
    "N (cid:19)(cid:18) (cid:18) N (cid:19)αN (cid:19)−1/αS C(N,L)= 6B S L− c .",
    "(B.15) ∗ c L1/αB",
    "N",
    "UsingA.6andA.9,wecaneliminateLinfavorofN (L),themodelsizewhichreachesLmostefficiently.",
    "eff",
    "From there, we find an expression for the excess compute needed as a consequence of using a suboptimal modelsize: C(N,N ) N (cid:20) α (cid:18) (cid:18) N (cid:19)αN (cid:19)(cid:21)−1/αS eff = 1+ S 1− eff .",
    "(B.16) C(N ,N )",
    "N α N eff eff eff N",
    "The result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a 20%increaseincomputebudget.",
    "Usingasmallermodelisusefulwhenaccountingforthecostinference.",
    "A largermodelcanbetrainedthethesamelevelofperformanceinfewersteps,allowingformoreparallelism andfastertrainingifsufficientharwareisavailable(seeFigureY): S(N,N )",
    "(cid:20) α (cid:18) (cid:18) N (cid:19)αN (cid:19)(cid:21)−1/αS eff = 1+ S 1− eff .",
    "(B.17) S(N ,N )",
    "α N eff eff N A2.2xlargermodelrequires45%fewerstepsatacostof20%moretrainingcompute.",
    "Notethatthisequation shouldnotbetrustedforverylargemodels,asitisonlyvalidinthepower-lawregionofthelearningcurve afterinitialtransienteffects.",
    "C Caveats Inthissectionwelistsomepotentialcaveatstoouranalysis.",
    "•",
    "At present we do not have a solid theoretical understanding for any of our proposed scaling laws.",
    "Thescalingrelationswithmodelsizeandcomputeareespeciallymysterious.",
    "Itmaybepossibleto understandscalingatverylargeDholdingmodelsizefixed[AS17],andalsotheshapeoflearning curveslateintraining,bymodelingthelosswithanoisyquadratic.",
    "ButthescalingwithD atvery large model size still remains mysterious.",
    "Without a theory or a systematic understanding of the correctionstoourscalinglaws,it’sdifficulttodetermineinwhatcircumstancestheycanbetrusted.",
    "22 105 104 103 103 104 105 Sc×[L(N,D) L(N, )] 1/S potsS 6 Early Stopping Step 5 Data Size 4 21M 43M 86M 172M 3 344M 688M 1.4B 2 103 104 105 Step ssoL Test Loss 1010 Train Loss 109 108 )snekoT( eziS tesataD Figure 16",
    "Left: We characterize the step on which early stopping occurs, as a function of the extent of overfitting.",
    "The red line indicates a lower bound for early stopping that is derived in Section 5.3.",
    "Right: Wedisplaytrainandtestlossforaseriesof300Mparametermodelstrainedondifferentsizeddatasetsub- samples.",
    "Thetestlosstypicallyfollowsthatofarundonewithunrestricteddatauntildiverging.",
    "Notethatthe degreeofoverfitting(ascomparedtotheinfinitedatalimit)issignificantlyoverestimatedbyL −L test train (denotedbyablackbarforeachrun).",
    "•",
    "We are not especially confident in the prediction of B (L) for values of the loss far outside the crit range we have explored.",
    "Changes in B could have a significant impact on trade-offs between crit dataparallelismandthenumberofserialtrainingstepsrequired,whichwouldhaveamajorimpact ontrainingtime.",
    "•",
    "We did not thoroughly investigate the small data regime, and our fits for L(N,D) were poor for the smallest values of D (where an epoch corresponded to only 40 steps).",
    "Furthermore, we did not experiment with regularization and data augmentation.",
    "Improvements in these could alter our results,quantitativelyorqualitatively.",
    "•",
    "WeusedtheestimatedtrainingcomputeC",
    "≈ 6NBS,whichdidnotincludecontributionspropor- tionalton (seeSection2.1).",
    "Soourscalingswithcomputemaybeconfoundedinpracticeinthe ctx regimeofverylargen ,specificallywheren (cid:38)12d .",
    "ctx ctx model •",
    "We tuned learning rates, and we experimented with learning rate schedules.",
    "But we may have neglectedtotunesomehyperparameter(e.g.intializationscaleormomentum)thathaveanimportant effectonscaling.",
    "•",
    "Theoptimalchoiceoflearningrateissensitivetothetargetloss.",
    "Whentrainingclosetoconvergence, itmaybenecessarytouseasmallerlearningratetoavoiddivergences.",
    "Butwhenconductingashort trainingrun(egduetocomputelimitations),itmaybepossibletousealargerlearningrate.",
    "Wedid notexperimentwithhigherlearningratesfortrainingrunsthatdidnotproceedtoconvergence.",
    "D SupplementalFigures D.1 EarlyStoppingandTestvsTrain Insection5.3wedescribedtheresultshowninFigure16,whichprovidesapredictionforalowerboundon theearlystoppingstep.",
    "Wealsoshowthetrainandtestlossforagivenmodelsizewhentrainingondifferent sizeddatasets.",
    "D.2 UniversalTransformers WecomparetheperformanceofstandardTransformerstorecurrentTransformers[DGV+18]inFigure17.",
    "These models re-use parameters, and so perform slightly better as a function of N, but slightly worse as a functionofcomputeC. Weincludeseveraldifferentdifferentpossibilitiesforparameterre-use.",
    "D.3 BatchSize",
    "We measure the critical batch size using the data displayed in figure 18.",
    "This made it possible to estimate B (L)infigure10.",
    "crit 23 4.5 4.0 3.5 3.0 2.5 105 106 107 108 109 Parameters, including reuse (non-embedding)",
    "ssoL tseT 4.5 4.0 3.5 2x Reuse 3.0 4x Reuse 8x Reuse Non-recurrent Models 2.5 105 106 107 108 109 Parameters (non-embedding)",
    "ssoL tseT 2x Reuse 4x Reuse 8x Reuse Non-recurrent Models Figure 17",
    "We compare recurrent Transformers [DGV+18], which re-use parameters, to standard Trans- formers.",
    "RecurrentTransformersperformslightlybetterwhencomparingmodelswithequalparametercount, butslightlyworsewhenaccountingforreuseandcomparingperFLOP.",
    "1011 1010 109 108 107 106 102 103 104 105 Step dessecorP snekoT Batch Size Scan - 3M Params 10 8 6 4 ssoL tseT 1010 108 106 101 102 103 104 105 Step dessecorP snekoT Batch Size Scan - 85M Params 10 8 6 4 ssoL tseT Figure18 ThesefiguresdemonstratefitstoEquation(5.1)foralargenumberofvaluesofthelossL,and fortwodifferentTransformermodelsizes.",
    "ThesefitswereusedtomeasureB (L)forFigure10.",
    "crit D.4 SampleEfficiencyvsModelSize",
    "It is easy to see from figure 2 that larger models train faster, and are therefore more sample efficient.",
    "We provideanotherwayoflookingatthisphenomenoninfigure19,whichshowswhendifferentmodelsreach variousfixedvaluesoftheloss.",
    "105",
    "104 103 106 107 108 Parameters (non-embedding) )nimS( spetS muminiM 5.5 5.0 4.5 4.0 3.5 3.0 2.5 ssoL 1011 1010 109 108 106 107 108 Parameters (non-embedding) )",
    "nimE( selpmaxE muminiM 5.5 5.0 4.5 4.0 3.5 3.0 2.5 ssoL Figure19 Thenumberofminimumserialstepsneededtoreachanyfixedvalueofthetestlossdecreases precipitously with model size.",
    "Sample efficiency (show here for training far below the critical batch size) improvesgreatlyaswell,improvingbyafactorofalmost100whencomparingthesmallestpossiblemodel toaverylargeone.",
    "24 8 7 6 5 4 3 100 101 102 103 Token Index ssoL tseT nekoT-reP 4.0+3.2 T0.47 3.4+4.0 T0.56 2 2 . .",
    "9 7 + + 4 4 . .",
    "5 9 T T 0 0 . .",
    "5 6 6 0 108 2.4+5.1 T0.61 2.3+5.4 T0.62 107 106 sretemaraP ledoM 10 8 6 4 2 101 103 105 Step ssoL tseT Per-token Loss (774M Params) 103 102 101 100 xednI nekoT",
    "Figure 20",
    "This figure provides information about the performance per token as a function of model size andtrainingtime.",
    "Left: LosspertokenasafunctionofitspositionT inthe1024-tokencontext.",
    "Lossscales predictablyasapower-lawinT. Right: Testlosspertokenasafunctionoftrainingstep.",
    "7.5 6.0 4.5 3.0 104 105 106 107 108 109 Parameters (excl.",
    "embedding)",
    "ssoL",
    "tseT Token 1/1024 Token 2/1024 Token 4/1024",
    "Token 8/1024 Token 16/1024 Token 64/1024 Token 256/1024 Token 1024/1024 Token 1/8 Token 2/8 Token 4/8 Token 8/8 Figure 21",
    "In addition to the averaged loss, individual tokens within the 1024-token context also improve smoothlyasmodelsizeincreases.",
    "Trainingrunswithshortercontextn = 8(dashedlines)performbetter ctx onearlytokens,sincetheycanallocatealloftheircapacitytothem.",
    "D.5 ContextDependence ThetrendsforlossasafunctionofmodelsizearedisplayedfordifferenttokensinthecontextinFigure21.",
    "We see that models trained on n = 1024 show steady improvement with model size on all but the first ctx token.",
    "Fixingmodelsize,itappearsthatthelossscalesasapower-lawasafunctionofpositionT inthecontext,see Figure20.",
    "Thismaybeaconsequenceofunderlyingpower-lawcorrelationsinlanguage[EP94,ACDE12, LT16],oramoregeneralfeatureofthemodelarchitectureandoptimization.",
    "Itprovidessomesuggestionfor thepotentialbenefits(orlackthereof)fromtrainingonlargercontexts.",
    "Notonlydolargermodelsconverge tobetterperformanceatT",
    "=1024,buttheyalsoimprovemorequicklyatearlytokens,suggestingthatlarger models are more efficient at detecting patterns with less contextual information.",
    "In the right-hand plot we showhowper-tokenperformancevariesforafixedmodelasafunctionofthetrainingstep.",
    "Themodelbegins bylearningshort-rangeinformation,andonlylearnslonger-rangecorrelationslaterintraining.",
    "We have also included models trained with a tiny context n = 8 in order to compare with our longer ctx context models.",
    "Even modestly sized models trained on n = 8 can dominate our largest n = 1024 ctx ctx models on very early tokens.",
    "This also suggests that further improvements should be possible with much largermodelstrainedonlargecontexts.",
    "D.6 LearningRateSchedulesandErrorAnalysis",
    "We experimented with a variety of learning rates and schedules.",
    "A host of schedules and resulting test performancesforasmalllanguagemodelareplottedinFigure22.",
    "Weconcludethatthechoiceoflearning rate schedule is mostly irrelevant, as long as the total summed learning rate is sufficiently large, and the schedule includes a warmup period and a final decay to near-vanishing learning rate.",
    "Variations among 25 0.0010 0.0008 0.0006 0.0004 0.0002 0.0000 0 50000 100000 150000 200000 250000 Step etaR gninraeL 3.90 3.85 3.80 3.75 3.70 3.65 50 100 150 200 250 LR Summed Over Steps ssoL Figure22 Wetestavarietyoflearningrateschedulesincludingcosinedecay,lineardecay,aswellasother faster/slowerdecaysschedulesona3millionparametermodel,shownontheleft.",
    "Fortheseexperimentswe donotdecaytozero, sincewefindthatthistendstogiveafixedimprovementclosetotheendoftraining.",
    "Wefindthat,aslongasthelearningrateisnottoosmallanddoesnotdecaytooquickly,performancedoes not depend strongly on learning rate.",
    "Run-to-run variation is at the level of 0.05 in the loss, so averaging multiplerunsisnecessarytovalidateperformancechangessmallerthanthislevel.",
    "6 5 4 3 2 104 105 106 107 108 109 Parameters (non-embedding) )ecnegrevnoc ta( ssoL tseT L=(N/8.8 1013) 0.076 L= 0.25log(N/7.1 1012)",
    "Figure23 Thetrendforperformanceasafunctionofparametercount,L(N),isfitbetterbyapowerlaw thanbyotherfunctionssuchasalogarithmataqualitativelevel.",
    "schedulesappeartobestatisticalnoise,andprovidearoughgaugeforthescaleofvariationbetweendifferent trainingruns.",
    "Experimentsonlargermodelssuggestthatthevariationinthefinaltestlossbetweendifferent randomseedsisroughlyconstantinmagnitudefordifferentmodelsizes.",
    "Wefoundthatlargermodelsrequireasmallerlearningratetopreventdivergence,whilesmallermodelscan toleratealargerlearningrate.",
    "Toimplementthis,thefollowingruleofthumbwasusedformostruns: LR(N)≈0.003239+−0.0001395log(N) (D.1) Weexpectthatthisformulacouldbeimproved.",
    "Theremaybeadependenceonnetworkwidth,likelysetby theinitializationscale. TheformulaalsobreaksdownforN",
    ">1010parameters.",
    "Nevertheless,wefoundthat itworkssufficientlywellforthemodelsweconsidered.",
    "D.7 FitDetailsandPowerLawQuality We experimented with a number of functional forms for the fits to L(N),L(C), and L(D); the power-law fitswerequalitativelymuchmoreaccuratethanotherfunctionssuchaslogarithms(seeFigure23).",
    "For L(C), we do not include small models with only 1 layer in the fit, as the transition from 1 to 2 layers causesanoticablelumpinthedata.",
    "ForL(N)wealsodonotincludeverysmallmodelswithonly1layerin thefit,andweexcludethelargestmodelsthathavenottrainedfullytoconvergence.",
    "Fitparameterschange marginallyifwedoincludethem,andthetrendextrapolateswellinbothdirectionsregardless.",
    "D.8 GeneralizationandArchitecture Infigure24weshowthatgeneralizationtootherdatadistributionsdoesnotdependonnetworkdepthwhenwe holdthetotalparametercountfixed.",
    "Itseemstodependonlyontheperformanceonthetrainingdistribution.",
    "26 2.8 2.7 2.6 2.5 2.4 2.3 101 102 Depth ssoL tseT Wikipedia Books Internet Books Common Crawl WebText2 (Train) WebText2 (Test) Figure24 Weshowevaluationsonaseriesofdatasetsformodelswithapproximately1.5Billionparam- eters.",
    "We observe no effect of depth on generalization; generalization performance depends primarily on training distribution performance.",
    "The 12-layer model overfit the Internet Books dataset",
    "and we show the early-stoppedperformance;wehavenotseenthissurprisingresultinotherexperiments.",
    "ListofFigures 1 Summaryofsimplepowerlaws. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "3 2 Illustrationofsampleefficiencyandcomputeefficiency.. . . . . . . . . . . . . . . . . . . .",
    "4 3 Howtoscaleupmodelsize,batchsize,andserialsteps . . . . . . . . . . . . . . . . . . . .",
    "4 4 Performancewhenvaryingmodelanddatasize,ormodelandtrainingsteps,simultaneously 5 5 Weakdependenceofperformanceonhyperparametertuning . . . . . . . . . . . . . . . . .",
    "8 6 Comparisonofperformancetrendwhenincludingorexcludingembeddings . . . . . . . . .",
    "8 7 LSTMandTransformerperformancecomparison . . . . . . . . . . . . . . . . . . . . . .",
    ".",
    "9 8 Generalizationtoothertestdatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "10 9 Universalityofoverfitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "11 10 Criticalbatchsize . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "12 11 Performanceversuscomputebudgetornumberofparameterupdates . . . . . . . . . . . . .",
    "14 12 Trainingonsuboptimalmodels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "15 13 Comparisonbetweenempiricalandadjustedcomputetrends . . . . . . . . . . . . . . . . .",
    "15 14 Optimalmodelsizeandserialnumberofstepsversuscomputebudget . . . . . . . . . . . .",
    "16 15 Contradictionbetweencomputeanddatatrends . . . . . . . . . . . . . . . . . . . . . . . .",
    "17 16 Earlystoppinglowerboundandtrainingcurvesforoverfitmodels . . . . . . . . . . . . . .",
    "23 17 Universaltransformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "24 18 Batchsizescans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "24 19 Anotherlookatsampleefficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "24 20 Power-lawdependenceofperformanceonpositionincontext . . . . . . . . . . . . . . . . .",
    "25 21 Performanceatdifferentcontextpositionsversusmodelsize . . . . . . . . . . . . . . . . .",
    "25 22 Learningrateschedulescan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "26 23 ComparisonofPower-LawandLogarithmicFits . .",
    ". . . . . . . . . . . . . . . . . . . .",
    ".",
    "26 24 Generalizationversusdepth. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "27 27 ListofTables 1 ParameterandcomputecountsforTransformer . . . . . . . . . . . . . . . . . . . . . . .",
    ".",
    "7 2 FitstoL(N,D) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "11 3 FitstoL(N,S) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "14 4 Keytrendequations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "20 5 Keyparameterstotrendfits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "20 6 Trendsforcompute-efficienttraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "20 References [ACDE12] Eduardo G Altmann, Giampaolo Cristadoro, and Mirko Degli Esposti.",
    "On the origin of long- rangecorrelationsintexts.",
    "ProceedingsoftheNationalAcademyofSciences,109(29):11582– 11587,2012. 25",
    "[AS17] MadhuS.AdvaniandAndrewM.Saxe. High-dimensionaldynamicsofgeneralizationerrorin neuralnetworks.",
    "arXiv,2017,1710.03667.",
    "11,18,22",
    "[BB01] Michele Banko and Eric Brill.",
    "Scaling to very very large corpora for natural language disam- biguation.",
    "InProceedingsofthe39thannualmeetingonassociationforcomputationallinguis- tics,pages26–33.AssociationforComputationalLinguistics,2001.",
    "18",
    "[BHMM18] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.",
    "Reconciling modern machine learningandthebias-variancetrade-off.",
    "arXiv,2018,1812.11118.",
    "18 [Bia12] GÃŠrard Biau.",
    "Analysis of a random forests model.",
    "Journal of Machine Learning Research, 13(Apr):1063–1095,2012. 18",
    "[CGRS19] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.",
    "Generating long sequences with sparse transformers.",
    "CoRR, abs/1904.10509, 2019, 1904.10509.",
    "URL http://arxiv.org/ abs/1904.10509.",
    "19",
    "[DCLT18] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.Bert:Pre-trainingofdeep bidirectionaltransformersforlanguageunderstanding,2018,arXiv:1810.04805. 2",
    "[DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser.",
    "Uni- versal transformers.",
    "CoRR, abs/1807.03819, 2018, 1807.03819.",
    "URL http://arxiv.org/ abs/1807.03819.",
    "6,9,23,24",
    "[EP94] WernerEbelingandThorstenPöschel.",
    "Entropyandlong-rangecorrelationsinliteraryenglish.",
    "EPL(EurophysicsLetters),26(4):241,1994.",
    "25",
    "[Fou] TheCommonCrawlFoundation.",
    "Commoncrawl.",
    "URLhttp://commoncrawl.org. 7",
    "[GARD18] GuyGur-Ari,DanielA.Roberts,andEthanDyer.",
    "Gradientdescenthappensinatinysubspace.",
    "2018,arXiv:1812.04754. 18",
    "[GJS+19] MarioGeiger,ArthurJacot,StefanoSpigler,FranckGabriel,LeventSagun,Stéphaned’Ascoli, GiulioBiroli,ClémentHongler,andMatthieuWyart.",
    "Scalingdescriptionofgeneralizationwith numberofparametersindeeplearning.",
    "arXiv,2019,1901.01608.",
    "18",
    "[GKX19] Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao.",
    "An investigation into neural net op- timization via hessian eigenvalue density.",
    "CoRR, abs/1901.10159, 2019, 1901.10159.",
    "URL http://arxiv.org/abs/1901.10159.",
    "18",
    "[Goo01] JoshuaGoodman.",
    "Abitofprogressinlanguagemodeling.",
    "CoRR,cs.CL/0108005,2001.",
    "URL http://arxiv.org/abs/cs.CL/0108005.",
    "18",
    "[GRK17] ScottGray,AlecRadford,andDiederikPKingma.",
    "Gpukernelsforblock-sparseweights.",
    "ope-",
    "nai.com,2017. 19",
    "[HAD19] JoelHestness,NewshaArdalani,andGregoryDiamos.",
    "Beyondhuman-levelaccuracy: Compu- tationalchallengesindeeplearning.",
    "InProceedingsofthe24thSymposiumonPrinciplesand",
    "PracticeofParallelProgramming,PPoPP’19,pages1–14,NewYork,NY,USA,2019.ACM. doi:10.1145/3293883.3295710.",
    "18 28 [HCC+18] YanpingHuang,YonglongCheng,DehaoChen,HyoukJoongLee,JiquanNgiam,QuocV.Le, andZhifengChen.",
    "Gpipe:Efficienttrainingofgiantneuralnetworksusingpipelineparallelism.",
    "CoRR,abs/1811.06965,2018,1811.06965.",
    "URLhttp://arxiv.org/abs/1811.06965.",
    "19",
    "[HNA+17] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kia- ninejad,Md.MostofaAliPatwary,YangYang,andYanqiZhou. Deeplearningscalingispre-",
    "dictable,empirically,2017,1712.00409. 18",
    "[JGH18] Arthur Jacot, Franck Gabriel, and Clément Hongler.",
    "Neural tangent kernel: Convergence and generalizationinneuralnetworks.",
    "InAdvancesinneuralinformationprocessingsystems,pages 8571–8580,2018. 18",
    "[KB14] Diederik P. Kingma and Jimmy Ba.",
    "Adam: A method for stochastic optimization, 2014, 1412.6980. 7",
    "[Kom19] AranKomatsuzaki. Oneepochisallyouneed,2019,arXiv:1906.06669.",
    "18 [KSH12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.",
    "Imagenet classification with deep convolutionalneuralnetworks.",
    "InProceedingsofthe25thInternationalConferenceonNeural Information Processing Systems - Volume 1, NIPS’12, pages 1097–1105, USA, 2012.",
    "Curran AssociatesInc.",
    "URLhttp://dl.acm.org/citation.cfm?id=2999134.2999257.",
    "19 [LCG+19] ZhenzhongLan,MingdaChen,SebastianGoodman,KevinGimpel,PiyushSharma,andRadu Soricut.",
    "Albert: A lite bert for self-supervised learning of language representations, 2019, 1909.11942. 9",
    "[LOG+19] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,Mike Lewis,LukeZettlemoyer,andVeselinStoyanov.",
    "Roberta:ArobustlyoptimizedBERTpretrain- ing approach.",
    "CoRR, abs/1907.11692, 2019, 1907.11692. URL http://arxiv.org/abs/ 1907.11692. 2",
    "[LSP+18] PeterJ.Liu,MohammadSaleh,EtiennePot,BenGoodrich,RyanSepassi,LukaszKaiser,and NoamShazeer.",
    "Generatingwikipediabysummarizinglongsequences.",
    "arXiv:1801.10198[cs], 2018,1801.10198.",
    "URLhttp://arxiv.org/abs/1801.10198. 2,6",
    "[LT16] HenryWLinandMaxTegmark.",
    "Criticalityinformallanguagesandstatisticalphysics.",
    "arXiv preprintarXiv:1606.06737,2016.",
    "25 [LXS+19] JaehoonLee,LechaoXiao,SamuelS.Schoenholz,YasamanBahri,RomanNovak,JaschaSohl- Dickstein,andJeffreyPennington.",
    "Wideneuralnetworksofanydepthevolveaslinearmodels undergradientdescent,2019,arXiv:1902.06720.",
    "18",
    "[MKAT18] SamMcCandlish,JaredKaplan,DarioAmodei,andOpenAIDotaTeam.",
    "Anempiricalmodel oflarge-batchtraining,2018,arXiv:1812.06162. 3,5,6,12,13,21",
    "[Pap18] Vardan Papyan.",
    "The full spectrum of deep net hessians at scale: Dynamics with sample size.",
    "CoRR,abs/1811.07062,2018,1811.07062.",
    "URLhttp://arxiv.org/abs/1811.07062.",
    "18",
    "[RNSS18] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.",
    "Improving language understanding by generative pre-training.",
    "URL https://s3-us-west-2. amazonaws.",
    "com/openai- assets/research-covers/languageunsupervised/languageunderstandingpaper.pdf,2018.",
    "2,6",
    "[RRBS19a] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.",
    "A constructive predictionofthegeneralizationerroracrossscales,2019,1909.12673. 18",
    "[RRBS19b] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.",
    "A constructive predictionofthegeneralizationerroracrossscales,2019,arXiv:1909.12673. 18",
    "[RSR+19] Colin Raffel, NoamShazeer, AdamRoberts, Katherine Lee, Sharan Narang, MichaelMatena, Yanqi Zhou, Wei Li, and Peter J. Liu.",
    "Exploring the limits of transfer learning with a unified text-to-texttransformer,2019,arXiv:1910.10683. 2 [RWC+19] AlecRadford,JeffWu,RewonChild,DavidLuan,DarioAmodei,andIlyaSutskever.",
    "Language modelsareunsupervisedmultitasklearners. openai.com,2019.",
    "2,5,6,7,8",
    "[SCP+18] NoamShazeer, YoulongCheng, NikiParmar, DustinTran, AshishVaswani, PenpornKoanan- takool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, Ryan Sepassi, and BlakeHechtman.",
    "Mesh-tensorflow: Deeplearningforsupercomputers,2018,1811.02084.",
    "19",
    "[SHB15] RicoSennrich,BarryHaddow,andAlexandraBirch.",
    "Neuralmachinetranslationofrarewords withsubwordunits.",
    "CoRR,2015,1508.07909.",
    "6 29 [SLA+18] Christopher J. Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E. Dahl.",
    "Measuring the effects of data parallelism on neural network training, 2018, arXiv:1811.03600.",
    "12 [SS18] NoamShazeerandMitchellStern.",
    "Adafactor: Adaptivelearningrateswithsublinearmemory cost.",
    "CoRR,abs/1804.04235,2018,1804.04235.URLhttp://arxiv.org/abs/1804.04235. 7",
    "[THK18] StefanThurner,RudolfHanel,andPeterKlimek.",
    "Introductiontothetheoryofcomplexsystems.",
    "OxfordUniversityPress,2018.",
    "18",
    "[TL19] MingxingTanandQuocV.Le. Efficientnet: Rethinkingmodelscalingforconvolutionalneural networks.",
    "CoRR,abs/1905.11946,2019,1905.11946. URLhttp://arxiv.org/abs/1905.",
    "11946.",
    "18 [VSP+17] AshishVaswani, NoamShazeer, NikiParmar, JakobUszkoreit, LlionJones, AidanNGomez, Ł ukasz Kaiser, and Illia Polosukhin.",
    "Attention is all you need.",
    "In I. Guyon, U. V. Luxburg, S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,editors,AdvancesinNeural Information Processing Systems 30, pages 5998–6008.",
    "Curran Associates, Inc., 2017.",
    "URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf.",
    "2,6",
    "[VWB16] AndreasVeit, MichaelWilber, andSergeBelongie. Residualnetworksbehavelikeensembles ofrelativelyshallownetworks,2016,arXiv:1605.06431.",
    "8,18",
    "[Was06] LarryWasserman.",
    "Allofnonparametricstatistics.",
    "SpringerScience&BusinessMedia,2006.",
    "18",
    "[WPN+19] AlexWang,YadaPruksachatkun,NikitaNangia,AmanpreetSingh,JulianMichael,FelixHill, Omer Levy, and Samuel R. Bowman.",
    "Superglue: A stickier benchmark for general-purpose languageunderstandingsystems,2019,1905.00537. 2",
    "[WRH17] Yu-Xiong Wang, Deva Ramanan, and Martial Hebert.",
    "Growing a brain: Fine-tuning by in- creasingmodelcapacity.",
    "2017IEEEConferenceonComputerVisionandPatternRecognition (CVPR),Jul2017.",
    "doi:10.1109/cvpr.2017.323.",
    "19",
    "[WYL19] Wei Wen, Feng Yan, and Hai Li.",
    "Autogrow:",
    "Automatic layer growing in deep convolutional networks,2019,1906.02909.",
    "19 [YDY+19] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le.",
    "Xlnet:",
    "Generalized autoregressive pretraining for language understanding, 2019, arXiv:1906.08237.",
    "2",
    "[ZK16] Sergey Zagoruyko and Nikos Komodakis.",
    "Wide residual networks.",
    "Procedings of the British MachineVisionConference2016,2016.",
    "doi:10.5244/c.30.87. 18",
    "[ZKZ+15] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor- ralba,andSanjaFidler.",
    "Aligningbooksandmovies: Towardsstory-likevisualexplanationsby watchingmoviesandreadingbooks.",
    "2015IEEEInternationalConferenceonComputerVision (ICCV),Dec2015.",
    "doi:10.1109/iccv.2015.11. 7",
    "[ZLN+19] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl, ChristopherJ.Shallue,andRogerB.Grosse.",
    "Whichalgorithmicchoicesmatteratwhichbatch sizes?",
    "insightsfromanoisyquadraticmodel.",
    "CoRR,abs/1907.04164,2019,1907.04164. URL http://arxiv.org/abs/1907.04164.",
    "12,18 30 Sparks of Artificial General Intelligence: Early experiments with GPT-4 S´ebastien Bubeck Varun Chandrasekaran Ronen Eldan Johannes Gehrke Eric Horvitz Ece Kamar Peter Lee Yin Tat Lee Yuanzhi Li Scott Lundberg Harsha Nori Hamid Palangi Marco Tulio Ribeiro Yi Zhang Microsoft Research Abstract Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition.",
    "The latest model developed by OpenAI, GPT-4",
    "[Ope23], was trained using an unprecedented scale of compute and data.",
    "In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI.",
    "We contend that (this early version of) GPT- 4 is part of a new cohort of LLMs (along with ChatGPT and Google’s PaLM for example) that exhibit more general intelligence than previous AI models.",
    "We discuss the rising capabilities and implications of these models.",
    "We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting.",
    "Moreover, in all of these tasks, GPT-4’s performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT.",
    "Given the breadth and depth of GPT-4’s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.",
    "In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction.",
    "We conclude with reflections on societal influences of the recent technological leap and future research directions.",
    "Contents 1 Introduction 4 1.1 Our approach to studying GPT-4’s intelligence . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "6 1.2 Organization of our demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ".",
    "8 2 Multimodal and interdisciplinary composition 13 2.1 Integrative ability. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "13 2.2 Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "16 2.2.1 Image generation beyond memorization . . . . . . . . . . . . . . . . . . . . . . . . .",
    ".",
    "16 2.2.2 Image generation following detailed instructions (`a la Dall-E) .",
    ". . . . . . . . . . . .",
    ".",
    "17 2.2.3 Possible application in sketch generation . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "18 2.3 Music . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "19 3 Coding 21 3.1 From instructions to code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "21",
    "3.1.1 Coding challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "21 3.1.2 Real world scenarios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "22 3.2 Understanding existing code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "26 1 3202 rpA 31 ]LC.sc[ 5v21721.3032:viXra 4 Mathematical abilities 30 4.1 A mathematical conversation with GPT-4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "31 4.1.1 A first generalization of the original question . . . . . . . . . . . . . . . . . . . . . . .",
    "31 4.1.2 A second variant of the original question . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "32 4.1.3 Analysis of the limitations highlighted by conversation . . . . . . . . . . . . . . . . . .",
    "34 4.2 Performance on mathematical problem datasets . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "35 4.3 Mathematical modeling in various domains . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "37 4.4 Higher-level mathematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "39 5 Interaction with the world 43 5.1 Tool use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "43 5.1.1 Using multiple tools to solve more complex tasks . . . . . . . . . . . . . . . . . . . . .",
    "44 5.1.2 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "49 5.2 Embodied Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "49",
    "5.2.1 Warmup: navigating a map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "49 5.2.2 Text-based games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "49 5.2.3 Real world problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "52 5.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "53 6 Interaction with humans 54 6.1 Understanding Humans: Theory of Mind . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "54 6.1.1 Testing specific aspects of theory of mind . . . . . . . . . . . . . . . . . . . . . . . . .",
    "54",
    "6.1.2 Testing theory of mind in realistic scenarios . . . . . . . . . . . . . . . . . . . . . . . .",
    "54 6.1.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "60 6.2 Talking to Humans: Explainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "60 7 Discriminative capabilities 69 7.1 PII Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "69 7.2 Misconceptions and Fact-Checking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "70 7.2.1 Why Are Current Metrics Insufficient? . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "71 7.2.2 GPT-4 as a Judge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "73 8 Limitations of autoregressive architecture highlighted by GPT-4 76 8.1 Warm-up with two basic examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "76 8.2 Lack of planning in arithmetic/reasoning problems . . . . . . . . . . . . . . . . . . . . . . . .",
    "77 8.3 Lack of planning in text generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "78 9 Societal influences 82 9.1 Challenges of erroneous generations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "82 9.2 Misinformation and manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ".",
    "83 9.3 Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "86 9.4 Human expertise, jobs, and economics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "89 9.5 Constellation of influences and considerations . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "90 10 Directions and Conclusions 92 10.1 Definitions of intelligence, AI, and AGI. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "92 10.2",
    "On the path to more general artificial intelligence . . . . . . . . . . . . . . . . . . . . . . . .",
    ".",
    "93 10.3",
    "What is actually happening? .",
    ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "94",
    "A GPT-4 has common sense grounding 101 B Appendix for multimodal and interdisciplinary composition 105",
    "B.1",
    "Further details on integrative ability results . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "105",
    "B.2 Further details on vision results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ".",
    "108 B.3 Graphic novel design example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "110 2 C Appendix for the Coding section 111 C.1 Measuring human performance on LeetCode . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "111 C.2 Example of GPT-4 visualizing IMDb data.. . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "112",
    "C.3 More examples on visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "115 C.4 Example for 2D HTML game development. . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "116 C.5 Example for graphical user interface programming . . . . . . . . . . . . . . . . . . . . . . . .",
    "116",
    "C.6 Example for reverse engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "119 C.7 Testing GPT-4’s ability to execute (pseudo) code .",
    ". . . . . . . . . . . . . . . . . . . . . . . .",
    "121 D Additional examples for mathematical reasoning 122 D.1 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "122 D.2 Further examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ".",
    "126 D.3 Generating math problems with GPT-4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "138",
    "D.4 Mitigating calculation errors via external code execution . . . . . . . . . . . . . . . . . . . . .",
    "139 E Additional Interpretability Examples 141 E.1 Explanation Agent Mismatches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "141 F Additional examples for interaction with the world 144 F.1 Interact with tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "144",
    "F.2 Examples for interaction with environments . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    ".",
    "149 G Supplementary Materials: Discriminative Capabilities 155 G.1 Misconceptions: Detailed Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "155 3 Something unknown is doing we don’t know what.",
    "– Sir Arthur Eddington 1 Introduction Intelligence is a multifaceted and elusive concept that has long challenged psychologists, philosophers, and computerscientists.",
    "Thereisnogenerallyagreedupondefinitionofintelligence,butoneaspectthatisbroadly acceptedisthatintelligenceisnotlimitedtoaspecificdomainortask,butratherencompassesabroadrangeof cognitiveskillsandabilities.",
    "Buildinganartificialsystemthatexhibitssuchbroadbehaviorisalong-standing and ambitious goal of AI research.",
    "In early writings, the founders of the modern discipline of artificial in- telligence (AI) research called out sets of aspirational goals for understanding intelligence [MMRS06].",
    "Over decades,AIresearchershavepursuedprinciplesofintelligence,includinggeneralizablemechanismsforreason- ing (e.g., [NSS59], [LBFL93]) and construction of knowledge bases containing large corpora of commonsense knowledge [Len95].",
    "However, many of the more recent successes in AI research can be described as being narrowly focused on well-defined tasks and challenges, such as playing chess or Go, which were mastered by AI systems in 1996 and 2016, respectively.",
    "In the late-1990s and into the 2000s, there were increasing calls for developing more general AI systems (e.g., [SBD+96]) and scholarship in the field has sought to identify principles that might underly more generally intelligent systems (e.g., [Leg08, GHT15]).",
    "The phrase, “artifi- cial general intelligence” (AGI), was popularized in the early-2000s (see [Goe14]) to emphasize the aspiration of moving from the “narrow AI”, as demonstrated in the focused, real-world applications being developed, to broadernotionsofintelligence,harkeningbacktothelong-termaspirationsanddreamsofearlierAIresearch.",
    "WeuseAGItorefertosystemsthatdemonstratebroadcapabilitiesofintelligence, includingreasoning, plan- ning,andtheabilitytolearnfromexperience,andwiththesecapabilitiesatorabovehuman-level.",
    "Wediscuss other definitions of AGI in the conclusion section.",
    "The most remarkable breakthrough in AI research of the last few years has been the advancement of natural language processing achieved by large language models (LLMs).",
    "These neural network models are basedontheTransformerarchitecture[VSP+17]andtrainedonmassivecorporaofweb-textdata,usingatits core a self-supervised objective of predicting the next word in a partial sentence.",
    "In this paper, we report on evidence that a new LLM developed by OpenAI, which is an early and non-multimodal version of GPT-4",
    "[Ope23], exhibits many traits of intelligence.",
    "Despite being purely a language model, this early version of GPT-4 demonstrates remarkable capabilities on a variety of domains and tasks, including abstraction, com- prehension, vision, coding, mathematics, medicine, law, understanding of human motives and emotions, and more.",
    "We interacted with GPT-4 during its early development by OpenAI using purely natural language queries(prompts)1.",
    "InFigure1.1,wedisplaysomepreliminaryexamplesofoutputsfromGPT-4,askingitto writeaproofofinfinitudeofprimesintheformofapoem,todrawaunicorninTiKZ(alanguageforcreating graphics in LATEX), to create a complex animation in Python, and to solve a high-school level mathematical problem.",
    "It easily succeeds at all these tasks, and produces outputs that are essentially indistinguishable from (or even better than) what humans could produce.",
    "We also compare GPT-4’s performance to those of previous LLMs, most notably ChatGPT, which is a fine-tuned version of (an improved) GPT-3",
    "[BMR+20].",
    "In Figure 1.2, we display the results of asking ChatGPT for both the infinitude of primes poem and the TikZ unicorn drawing.",
    "While the system performs non-trivially on both tasks, there is no comparison with the outputsfromGPT-4.",
    "Thesepreliminaryobservationswillrepeatthemselvesthroughoutthepaper,onagreat variety of tasks.",
    "The combination of the generality of GPT-4’s capabilities, with numerous abilities spanning abroadswathofdomains, anditsperformanceonawidespectrumoftasksatorbeyondhuman-level, makes us comfortable with saying that GPT-4 is a significant step towards AGI.",
    "Our claim that GPT-4 represents progress towards AGI does not mean that it is perfect at what it does, or that it comes close to being able to do anything that a human can do (which is one of the usual definition of AGI; see the conclusion section for more on this), or that it has inner motivation and goals (another key aspect in some definitions of AGI).",
    "In fact it is not fully clear how far GPT-4 can go along some of those axes of intelligence that we focus on, e.g., planning (see Section 8), and arguably it is entirely missing the 1AsGPT-4’sdevelopmentcontinuedafterourexperiments,oneshouldexpectdifferentresponsesfromthefinalversionofGPT- 4.",
    "Inparticular,allquantitativeresultsshouldbeviewedasestimatesofthemodel’spotential,ratherthandefinitivenumbers.",
    "We repeat this caveat throughout the paper to clarify that the experience on the deployed model may differ.",
    "Moreover we emphasize that the version we tested was text-only for inputs, but for simplicity we refer to it as GPT-4 too.",
    "4",
    "GPT-4 Prompt: Canyouwriteaproofthatthereareinfinitely Prompt: Draw a unicorn in TiKZ.",
    "many primes, with every line that rhymes?",
    "GPT-4: GPT-4: [Produces LATEX compiling to followingpicture.]",
    "Yes, I think I can, Though it might take a clever plan.",
    "I’ll start by noting Euclid’s proof, Which shows that primes aren’t just aloof.",
    "Assume we have a finite list, Of primes, and that none have been missed.",
    "Multiply them all together, And add one, just to be clever.",
    "The result will be a number, That has no prime factors, oh wonder!",
    "But every number has a prime, That divides it, it’s just a matter of time.",
    "So we’ve found a contradiction, And our finite list needs eviction.",
    "There must be infinitely many primes, And that’s the end of my rhyming lines.s Prompt: Canyougenerateapyplotforthefollowingdata: x=[1,3,5,6,8],y1=[2,3,5,18,1],y2=[3,5,6,8,1],y3=",
    "[5,1,2,3,4],y4=[9,7,2,3,1].Iwanty1,y2tobeinthesameplot, buty3isinanotherplotnexttothatplot, y4isin below.",
    "Iwantthelegendofy1tobe”bob”,y2tobe”alice”,y3tobe”bilbo”,y4tobe”allie”.",
    "Iwantthex-axistobe labeledwith”time”andyaxistobelabeledwith”money”.",
    "Iwanttoadda10%-40%randomerrorbartoeachcurve, through all times (including non-integers).",
    "I want smoothed curves to show the plot, and smoothed error bar.",
    "Do not uselinearinterpolation,usesmoothinterpolation!",
    "Iwanttoalsoaddsomesmallzig-zagtothesmoothedcurvetomake it look more real.",
    "I want to put a baseline as the mean of each line.",
    "I want to put a pie chart below indicating the fraction of the four people in each time step.",
    "I also want an animation to show how the fractions are changing in the pie chart in **continuous time**.",
    "Interpolate the missing fractions!",
    "I also want vertical line animation in other three plotstomatchthepiechart.",
    "Iwantthefanciestplot.",
    "Pleaseaddasmanyfancythingsaspossible.",
    "Hereisapossiblecodetogenerateapyplotforthefollowingdata,withsomecommentstoexplainthesteps:",
    "GPT-4: [Wegivetwosnapshotsfromtheanimationresultingfromtheproducedcode] Prompt: Supposeg(x)=f−1(x),g(0)=5,g(4)=7,g(3)=2,g(7)=9,g(9)=6whatisf(f(f(6)))?",
    "GPT-4: There are different ways to approach this problem, but one possible method is to use the fact that g(x)= f−1(x) implies that f(g(x))",
    "= x for any x in the domain of g. That is, applying f to both sides of the equation g(x)",
    "= f−1(x) gives f(g(x))",
    "= f(f−1(x))",
    "= x.",
    "This means that we can undo the effect of f by applying g, and vice versa.",
    "Using this idea, we can try to find f(6) by looking for the value of x such that g(x)",
    "= 6.",
    "From the given information,weseethatg(9)=6,sof(6)=f(g(9))=9.",
    "Similarly,wecanfindf(9)bylookingforthevalueofxsuch thatg(x)=9.",
    "Weseethatg(7)=9,sof(9)=f(g(7))=7.",
    "Finally,wecanfindf(7)bylookingforthevalueofxsuch thatg(x)=7.",
    "Weseethatg(4)=7,sof(7)=f(g(4))=4.",
    "Therefore,f(f(f(6)))=f(f(9))=f(7)=4.",
    "Figure 1.1: Preliminary examples of GPT-4’s ca5pabilities in language, vision, coding, and mathe- matics.",
    "ChatGPT",
    "Prompt: Can you write a proof that there are infinitely Prompt: Draw a unicorn in TiKZ.",
    "many primes, with every line that rhymes?",
    "ChatGPT:[ProducesLATEXcompiling ChatGPT: tofollowingpicture.]",
    "Sure!",
    "Here is a proof that there are infinitely many primes, with each line of the proof rhyming: Consider the set of all primes.",
    "Call this set P and let it start with just two, the first prime.",
    "If we take all the primes in P and add one, We get a new set of numbers that has never been done.",
    "This set, call it S, is composed of only composite numbers.",
    "No primes will be found among them, that’s for sure.",
    "But we know that there must be at least one prime, Since S is infinite, and that much is clear.",
    "If we take the smallest number in S that is not in P, And call it p, we can add it to our set, don’t you see?",
    "But this process can be repeated indefinitely.",
    "Thus, our set P must also be infinite, you’ll agree.",
    "So there you have it, a proof that’s quite solid.",
    "There are infinitely many primes, that much we can see.",
    "No need to be coy, no need to be contrite.",
    "Infinity is a big number, just like P. Figure 1.2: ChatGPT output on the first two prompts from Figure 1.1.",
    "learning from experience as the model is not continuously updating (although it can learn within a session, see Section 5 for example).",
    "Overall GPT-4 still has many limitations, and biases, which we discuss in detail below and that are also covered in OpenAI’s report [Ope23].",
    "In particular it still suffers from some of the well-documented shortcomings of LLMs such as the problem of hallucinations [MNBM20] (see Figure 1.8) or makingbasicarithmeticmistakes[CKB+21](seeAppendixD),andyetithasalsoovercomesomefundamental obstacles such as acquiring many non-linguistic capabilities (e.g., it solves most of the LLM failure modes described in [MIB+23], and it also made great progress on common-sense, see Figure 1.7 for a first example and Appendix A for more).",
    "This highlights the fact that, while GPT-4 is at or beyond human-level for many tasks, overall its patterns of intelligence are decidedly not human-like.",
    "However, GPT-4 is almost certainly only a first step towards a series of increasingly generally intelligent systems, and in fact GPT-4 itself has improved throughout our time testing it, see Figure 1.3 for the evolution of the unicorn drawing over the course of a month of training2.",
    "Even as a first step, however, GPT-4 challenges a considerable number of widely held assumptions about machine intelligence, and exhibits emergent behaviors and capabilities whose sources and mechanisms are, at this moment, hard to discern precisely (see again the conclusion section for more discussion on this).",
    "Our primary goal in composing this paper is to share our exploration of GPT-4’s capabilities and limitations in support of our assessment that a technological leap has been achieved.",
    "We believe that GPT-4’s intelligence signals a true paradigm shift in the field of computer science and beyond.",
    "1.1",
    "Our approach to studying GPT-4’s intelligence How can we measure the intelligence of an LLM that has been trained on an unknown but extremely vast corpus of web-text data?",
    "The standard approach in machine learning is to evaluate the system on a set of standard benchmark datasets, ensuring that they are independent of the training data and that they cover a rangeoftasksanddomains.",
    "Thisapproachisdesignedtoseparatetruelearningfrommerememorization,and 2Note that the improving we refer to here is a slow type of learning, which eventually comes to a halt, as opposed to the fast-paced and real-time learning one would expect from an AGI.",
    "6 Figure 1.3: We queried GPT-4 three times, at roughly equal time intervals over the span of a month while the system was being refined, with the prompt “Draw a unicorn in TikZ”.",
    "We can see a clear evolution in the sophistication of GPT-4’s drawings.",
    "isbackedupbyarichtheoreticalframework[SSBD14,MRT18].",
    "However,thismethodologyisnotnecessarily suitable for studying GPT-4, for two reasons.",
    "First, since we do not have access to the full details of its vast trainingdata,wehavetoassumethatithaspotentiallyseeneveryexistingbenchmark,oratleastsomesimilar data.",
    "For example, it seems like GPT-4 knows the recently proposed BIG-bench [SRR+22] (at least GPT-4 knows the canary GUID from BIG-bench).",
    "Of course, OpenAI themselves have access to all the training details, and thus their report [Ope23] contains a lot of detailed benchmark results.",
    "Nevertheless, the second reasonforgoingbeyondtraditionalbenchmarksisprobablymoresignificant: OneofthekeyaspectsofGPT- 4’s intelligence is its generality, the ability to seemingly understand and connect any topic, and to perform tasks that go beyond the typical scope of narrow AI systems.",
    "Some of GPT-4’s most impressive performance are on tasks that do not admit a single solution, such as writing a graphic user interface (GUI) or helping a human brainstorm on some work-related problem.",
    "Benchmarks for such generative or interactive tasks can bedesignedtoo,butthemetricofevaluationbecomesachallenge(seee.g.,[PSZ+21]forsomerecentprogress",
    "onthisactiveresearchareainNLP).WenotethatcriticismsofthestandardapproachtomeasureAIsystems were also made in [Cho19], where a new benchmark was proposed to evaluate general intelligence.",
    "We do not test GPT-4 on the latter benchmark for the reasons previously mentioned, as well as the fact that the bench- markisvisualinnatureandthusmoreappropriateforthemultimodalversionofGPT-4describedin[Ope23].",
    "To overcome the limitations described above, we propose here a different approach to studying GPT-4 which is closer to traditional psychology rather than machine learning, leveraging human creativity and cu- riosity.",
    "WeaimtogeneratenovelanddifficulttasksandquestionsthatconvincinglydemonstratethatGPT-4 goes far beyond memorization, and that it has a deep and flexible understanding of concepts, skills, and domains (a somewhat similar approach was also proposed in [CWF+22]).",
    "We also aim to probe GPT-4’s responses and behaviors, to verify its consistency, coherence, and correctness, and to uncover its limitations and biases.",
    "We acknowledge that this approach is somewhat subjective and informal, and that it may not satisfy the rigorous standards of scientific evaluation.",
    "However, we believe that it is a useful and necessary first step to appreciate the remarkable capabilities and challenges of GPT-4, and that such a first step opens up new opportunities for developing more formal and comprehensive methods for testing and analyzing AI systems with more general intelligence.",
    "To illustrate our approach to assessing GPT-4’s intelligence, let us consider the first two example inter- actions with GPT-4 that we have in Figure 1.1.",
    "The first example is asking GPT-4 to write a proof of the infinitude of primes in the form of a poem.",
    "This is a challenging task that requires combining elementary mathematical reasoning, poetic expression, and natural language generation.",
    "The second example is asking GPT-4 to draw a unicorn in TiKZ.",
    "This is another challenging task that requires combining visual imagina- tion and coding skills.",
    "In both cases, GPT-4 produces impressive outputs that are far superior to those of ChatGPT,apreviousstate-of-the-artLLM,andatleastcomparable(ifnotsuperior)towhatahumanwould do.",
    "7 Figure 1.4: We gave to GPT-4 a transformed version of the TikZ code it produced for Figure 1.1, with the part drawing the horn removed.",
    "We asked for code to add back the horn, and display the result.",
    "ThisdemonstratesthatGPT-4can“see”despitebeingapurelanguagemodel(weemphasize again that the version we test with is not multimodal).",
    "However,impressiveoutputsarenotenoughtoconvinceusthatGPT-4hastrulymasteredthesetasks.",
    "We need to probe further, to rule out the possibility that GPT-4 is simply memorizing or copying some existing data.",
    "Forthepoem,wecanvarythequestionslightly,andaskGPT-4towriteaproofofthesametheoremin thestyleofShakespeare, see Figure2.2, orask foradifferentcombination suchas writingaplatonic dialogue about language models, see Figure 1.6.",
    "One can see that GPT-4 easily adapts to different styles and produce impressive outputs, indicating that it has a flexible and general understanding of the concepts involved.",
    "For the unicorn, we can modify the code slightly, and ask GPT-4 to fix it or improve it.",
    "For example, we can remove the horn, apply some random transformations to the coordinates, and ask GPT-4 to add back the horn to the unicorn (we also carefully removed any textual information in the code, such as comments).",
    "As shown in Figure 1.4, GPT-4 can correctly identify the location of the head, draw a horn, and attach it to the head, indicating that it can comprehend and manipulate code, as well as infer and generate visual features, based on a natural language description.",
    "These examples show how we can use human creativity and curiosity to generate novel and difficult questions, and to probe GPT-4’s responses and behaviors, to assess its intelligence.",
    "In the rest of the paper, we organize our study of GPT-4 around use cases, covering a variety of domains and tasks, and highlighting GPT-4’s strengths and weaknesses.",
    "We describe those next.",
    "1.2 Organization of our demonstration We execute the approach outlined above on a few selected topics to explore the reasoning, planning, and learning aptitudes of GPT-4.",
    "1.",
    "GPT-4’s primary strength is its unparalleled mastery of natural language.",
    "It can not only generate fluent and coherent text, but also understand and manipulate it in various ways, such as summarizing, translating, or answering an extremely broad set of questions.",
    "Moreover, by translating we mean not onlybetweendifferentnaturallanguagesbutalsotranslationsintoneandstyle,aswellasacrossdomains such as medicine, law, accounting, computer programming, music, and more, see the Plato dialogue in Figure 1.6.",
    "These skills clearly demonstrate that GPT-4 can manipulate complex concepts, which is a coreaspectofreasoning.",
    "WeexplorefurtherGPT-4’scombinationskillsacrossmodalitiesanddisciplines in Section 2.",
    "We also give some more experiments on language in Section 7. 2.",
    "Coding and mathematics are emblematic of the ability to reason.",
    "We explore GPT-4’s abilities in these domains respectively in Section 3 and Section 4.",
    "We note however that, just like in all the other parts of the paper, we only scratch the surface of those topics and that entire papers can be (and will be) written about GPT-4’s performance in these domains.",
    "Moreover, we could have chosen several other expert domains to showcase GPT-4’s general reasoning capabilities such as medicine or law.",
    "We ran 8 Figure 1.5: GPT-4 passes mock technical interviews on LeetCode.",
    "GPT-4 could potentially be hired as a software engineer3.",
    "preliminarytests(see[Ope23]formuchmore)onthemultiplechoicecomponent(majorityofthescore) of the US Medical Licensing Exam Step 1, 2, and 3 with an accuracy around 80% in each.",
    "A similar preliminary test of GPT-4’s competency on the Multistate Bar Exam showed an accuracy above 70%.",
    "Wenotethattheemergenceofhuman-levelabilitiesinthesedomainshasrecentlybeenobservedwiththe latestgenerationofLLMs,e.g.,see[LAD+22,SAT+22]forGoogle’sPaLMonrespectivelymathematics andmedicine,and[BIK22]forGPT-3.5oninlaw.",
    "OurapproachtostudyGPT-4isdifferentfromthese works, as we explained previously.",
    "3.",
    "In Section 5, we test the model’s ability to plan as well as to some extent to learn from experience by having it play various games (or, flipping the table, simulate a game environment), as well as interact withtools.",
    "Inparticular,thefactthatGPT-4canusetools(includingitself)willcertainlybeofimmense importance to build real-world applications with GPT-4.",
    "4.",
    "An important part of our argumentation is that GPT-4 attains human-level performance on many tasks.",
    "As such, it is natural to ask how well GPT-4 understands humans themselves.",
    "We show several experiments on this question in Section 6, both in terms of understanding humans as well as GPT-4 making itself understandable to humans, i.e., addressing the problem of explainability.",
    "We note in particular that such tasks require a great deal of common sense, which so far has been a well-known pain point for LLMs [DM15].",
    "In Figure 1.7, we give a first example of how much better GPT-4 is at commonsense questions compared to ChatGPT, and provide some further examples in Appendix A. 5.",
    "Throughout the paper we emphasize limitations whenever we found one, but we also dedicate Section 8 to an in-depth analysis of the lack of planning, likely a direct consequence of the autoregressive nature of GPT-4’s architecture.",
    "6. FinallyinSection9,wediscusstheexpectedsocietalimpactofthisearlyformofAGI,andinSection10, we share key challenges, directions, and next steps for the field.",
    "A question that might be lingering on many readers’ mind is whether GPT-4 truly u"
  ],
  "source_files": [
    "Attention_Is_All_You_Need.pdf",
    "Awakening_from_the_Meaning_Crisis_Transcripts.txt",
    "A_Path_Towards_Autonomous_Machine_Intelligence.pdf",
    "Concrete_Problems_in_AI_Safety.pdf",
    "Deep_Reinforcement_Learning_from_Human_Preferences.pdf",
    "kngmkr.txt",
    "Scaling_Laws_for_Neural_Language_Models.pdf",
    "Sparks_of_AGI_GPT4.pdf",
    "The_Singularity_A_Philosophical_Analysis.pdf",
    "Why_AI_Is_A_Philosophical_Rapture.txt",
    "World_Models.pdf"
  ],
  "pos": {
    "nouns": [
      "anddecreasingitthereafterproportionallytotheinversesquarerootofthestepnumber",
      "shopping",
      "exposure",
      "com",
      "Functions",
      "error",
      "horizontal",
      "estimatesthe(non",
      "distortion",
      "3f",
      "generalizedexpectationcriteria[94,45]asktheusertoprovidepopulation",
      "Rethinkingtheinceptionarchitectureforcomputervision",
      "reward",
      "division",
      "stability",
      "bottomlineoftable3),steptimewas1.0seconds",
      "eduction",
      "identities",
      "event",
      "JournalofArtificialIntelligenceResearch",
      "chain",
      "foradifferentcombination",
      "reply",
      "Fine",
      "2c",
      "notions",
      "nontheists",
      "markov",
      "Thoughts",
      "policybasedonanuntrainedpredictor",
      "thatg(x)=7",
      "influence",
      "discontinuities",
      "realist",
      "upright,”whichprovedtobeusefulrewardshaping",
      "body",
      "compareshortvideoclipsoftheagent’sbehav-",
      "arXiv:1404.3862",
      "wind",
      "environments",
      "feedbackwiththesamenumberoflabels",
      "therearediminishingreturnswithincreasingscale",
      "sistence",
      "manifestation",
      "berepeatedlypresentedwithtwovideoclipsofanAIplayingthegame",
      "y)+[m(y",
      "pedagogy",
      "g",
      "SummaryofScalingLaws",
      "defect",
      "Meaning",
      "beliefs",
      "violations",
      "andonEnduroweblankout",
      "display",
      "Thistaskpresentsspecificchallenges",
      "explanations",
      "C)≡",
      "arts",
      "q",
      "arXiv:1901.02705",
      "thesetaskscomparisonsandtargetshadsignificantlydifferentperformance",
      "attribution",
      "codes",
      "stakes",
      "A.1",
      "Relevance",
      "6B",
      "theenergylandscapemaysufferacollapse",
      "“Largesamplepropertiesofgeneralizedmethodofmomentsestimators",
      "Information",
      "computation",
      "variance",
      "accommodation",
      "dependence",
      "Stimulusandresponsegeneralization",
      "labeledwith”time”andyaxistobelabeledwith”money",
      "continuum",
      "includingchangedetection",
      "plastic",
      "behaves",
      "expansion",
      "jecting",
      "manifold",
      "ideology",
      "AFNLP",
      "Interpretability",
      "testimony",
      "schema",
      "worldtasks",
      "measuringthechangeinperformanceonEnglish",
      "proposals",
      "opposites",
      "runs",
      "Theparametersofthemappingrˆareoptimizedviasupervisedlearningtofitthecomparisons",
      "dropout",
      "amounts",
      "werechosenafterexperimentationonthedevelopmentset",
      "Youcanspeedupyourfeedbackbyusingthearrowkeys",
      "discloses",
      "inthesamewaythatahumanmaybeconcernedthatthetop",
      "height",
      "drop",
      "thresholding",
      "sound",
      "andLevine",
      "prudence",
      "donearlyaswellasRLwithoutaccesstorewardinformationandinsteadrelyingonmuchscarcer",
      "efficientfrontier",
      "Figure15showsapossiblearchitectureformultilevel",
      "belief",
      "convolutionalneuralnetworksthatincludeanencoderandadecoder",
      "1807.03819",
      "approximations",
      "passion",
      "proxies",
      "effect",
      "balancing",
      "Systems",
      "SimCLR",
      "attentioninlayer5of6",
      "theneedtoadaptthepredictorastheRLagentlearnsandencountersnewstates",
      "implementingtensor2tensor",
      "flavors",
      "reasons",
      "tounderstandhowusingcomparisonsaffectsperformance",
      "2sentencedescription",
      "select",
      "house",
      "sense",
      "CpD",
      "recall",
      "intuitions",
      "uncertainties",
      "silver",
      "purelyempiricaldata",
      "andprovidea",
      "trainedwithaparticularkindofcontrastivemethodcalleddenoisingauto",
      "decoderattention\"layers",
      "mysteries",
      "allmodels",
      "pass",
      "expectation",
      "commitment",
      "run",
      "black",
      "numbers",
      "nightmare",
      "yes",
      "descent",
      "Oncethenotionofobjectemergesintherepresentation",
      "Figure19",
      "critics",
      "engineering",
      "classifications",
      "archetypes",
      "diagnosis",
      "item",
      "acknowledgement",
      "technology",
      "largermodelstrainedonlargecontexts",
      "v",
      "shrink",
      "Thesurprisingpowerofreinforcement",
      "leaping",
      "poin",
      "entities",
      "trainingofdeep",
      "lengthepisodes",
      "archeology",
      "desert",
      "ansatzsimilartoours",
      "Isolatedattentionsfromjusttheword‘its’forattentionheads5",
      "attraction",
      "idea",
      "ZhongqiangHuangandMaryHarper",
      "theygenerateasequenceofhidden",
      "revenue",
      "assertion",
      "telligent",
      "ŁukaszKaiserandSamyBengio",
      "D",
      "structures",
      "verynon",
      "corresponds",
      "experimentation",
      "tireless",
      "associations",
      "thenumberofstepswhen",
      "loop",
      "Transformersareidealtorepresentthedynamics",
      "software",
      "synonymy",
      "pseudo",
      "somepartofitsenvironment",
      "grandma",
      "conduct",
      "frames",
      "constancy",
      "comportment",
      "andtousethisfeedbacktodefinethetask",
      "graphics",
      "measure",
      "eachcase",
      "wefounditbeneficialtolinearlyprojectthequeries",
      "graph",
      "Thanks",
      "GANs",
      "bleach",
      "dialogue",
      "samples",
      "movement",
      "page",
      "mini-",
      "justwaitaminuteandrefreshthe",
      "heed",
      "role",
      "hour",
      "arXivpreprint",
      "weconsiderarangeofmodifications",
      "self",
      "invariances",
      "forest",
      "restrictedattentionmechanismstoefficientlyhandlelargeinputsandoutputs",
      "tonight",
      "theefficienttrainingproceduredescribedabovetothismoretypicalsetup",
      "Wefoundthatsuchterminationconditions",
      "onhowwellitisplaying",
      "Decision",
      "havebeenpreviouslylearnedfromhumanfeedback",
      "outer",
      "SSRN",
      "readers",
      "details",
      "briefly",
      "cid:19)αN",
      "advantage",
      "hand",
      "courtroom",
      "purity",
      "trees",
      "generalizedexpectationcriteria”",
      "andweexcludethelargestmodelsthathavenottrainedfullytoconvergence",
      "multinomial",
      "broadernotionsofintelligence",
      "preference",
      "pages152–159.ACL",
      "EmbeddingPredictiveArchitecture(JEPA)consistsoftwoencodingbranches",
      "Transformers",
      "techniques",
      "workshop",
      "ACL.Vol.7.2007,pp.440",
      "concernedonlywiththesetaskswecouldtakeasimplerapproachtolearningtherewardfunction",
      "onlybetweendifferentnaturallanguagesbutalsotranslationsintoneandstyle",
      "andChristopherDManning",
      "con",
      "person",
      "complexity",
      "likelysetby",
      "tochangesintherewardfunction",
      "guidancethroughouttheproject",
      "pointcapacityofeachGPU5",
      "Switch",
      "vases",
      "L(D)≈",
      "wisefeed",
      "scope",
      "bullshit",
      "648–657",
      "andeachpointistheaverage",
      "correctness",
      "whichdoesnotrelyon",
      "writer",
      "sounds",
      "JimmyLeiBa",
      "overfitting",
      "tension",
      "increases",
      "tradeoff",
      "Trendsforcompute",
      "design",
      "j",
      "year",
      "γ",
      "degrees",
      "church",
      "competency",
      "Thetwomostcommonlyusedattentionfunctionsareadditiveattention[2],anddot",
      "andwhichtheagentmustlearn",
      "autonomy",
      "withsometrendsspanningmorethansevenordersofmagnitude",
      "hands",
      "indications",
      "products",
      "actionepisode",
      "entrapment",
      "RRBS19b",
      "block",
      "bullshitting",
      "A.",
      "arXiv:1502.05698",
      "score",
      "years",
      "weshouldtraintoafixedpercentage",
      "passedbythem",
      "phenomenology",
      "significance",
      "minima",
      "events",
      "anintroduction”",
      "α",
      "Processing",
      "sentences",
      "Singularity",
      "normativity",
      "sequences",
      "editor",
      "machinery",
      "pendingontheprecisevaluesoftheexponentsfromthepower",
      "regressiveproperty",
      "verylongsequences",
      "passasmanycarsasyoucan",
      "slant",
      "locus",
      "confirmation",
      "arrows",
      "whitening",
      "layercomplexityandminimumnumberofsequentialoperations",
      "wordperplexities",
      "onthetask",
      "wealsorunexperimentsusingasyntheticoraclewhosepreferencesovertrajectories",
      "biggermodelsarebetter",
      "Scientists",
      "Implies",
      "throat",
      "truths",
      "Contradiction",
      "interactions",
      "double",
      "inevitability",
      "14b",
      "similartotherobustpolicyimprovementandreachabilityanalysis",
      "asitisonlyvalidinthepower",
      "systems/.",
      "creatures",
      "params",
      "risk",
      "tics",
      "skidding",
      "Variance",
      "atvery",
      "Theoutputiscomputedasaweightedsum",
      "handwriting",
      "arena",
      "beauty",
      "butthe",
      "ofobservations",
      "URLhttp://arxiv.org/abs/1801.10198",
      "catastrophe",
      "friends",
      "bugs",
      "smoothlyasmodelsizeincreases",
      "psychiatrist",
      "moves",
      "Makinggenerationlesssequentialisanotherresearchgoalsofours",
      "agentreceivesanobservationo",
      "fullness",
      "atom",
      "iner",
      "hessians",
      "queriesandkeysofdimensiond",
      "solution",
      "theorems",
      "foolishness",
      "eidos",
      "progenitor",
      "commission",
      "fan",
      "gnosticism",
      "precision",
      "Ek",
      "Non",
      "participants",
      "weather",
      "concept",
      "workers",
      "criticism",
      "Itwasalsoarguedthatthegradientnoisescaleprovidesasimple",
      "concreteness",
      "qualities",
      "empire",
      "explanation",
      "pets",
      "traffic",
      "Cost",
      "shoon",
      "painting",
      "candidate",
      "isprovidedwithexperttrajectoriesofnear",
      "Transformer(4layers",
      "andoutputsequences",
      "axes",
      "Frenchnewstest2014testsatafractionofthetrainingcost",
      "whencomparedwiththefullyempiricalresults",
      "datasets",
      "canbefitverywellwithapower",
      "bishop",
      "token7",
      "psyche",
      "chocolate",
      "room",
      "things",
      "afternoon",
      "wisdoms",
      "typicallyuseamuchsmallervalue",
      "frameworkfortheperformanceoflanguagemodeling",
      "Datapointsareblackdots",
      "clouds",
      "rejects",
      "andfoundthatwideResNetscanoutperformdeepResNetsonimageclassification",
      "manipulating",
      "Mesh",
      "encouragement",
      "attentionhasbeen",
      "reformulation",
      "know",
      "implication",
      "physics",
      "replication",
      "criticreinforcement",
      "Theoutputoftheworldmodelatagiventimestepisasetofquery",
      "mate",
      "dresses",
      "entropylossbetweenthesepredictionsandtheactualhuman",
      "properties",
      "marksthesegmentsasequallypreferable",
      "victim",
      "efficientthansmallmodels",
      "emergence",
      "parallelism",
      "Iwantthefanciestplot",
      "Daddy",
      "neurologist",
      "meal",
      "psychologist",
      "inthe1024",
      "termdependencies,2001",
      "strategy",
      "religio",
      "B.2",
      "cortisol",
      "interplay",
      "translators",
      "principles",
      "Tofacilitatetheseresidualconnections",
      "freeway",
      "lawscalingsforperformanceasafunctionoftrainingtime",
      "11a",
      "Forallothertasks",
      "Picbreeder",
      "misunderstood",
      "Vintage",
      "impressiveoutputsarenotenoughtoconvinceusthatGPT-4hastrulymasteredthesetasks",
      "machine",
      "grasp",
      "dataregimes[37",
      "specifiedrewardfunction(Mnihetal",
      "Gifts",
      "aroughlylinearfunctionofthecliplength",
      "crystallographer",
      "placeholders",
      "contextpositionAppendixD.5,whereincreasinglylargepowersforlargermodelssuggestimprovedability",
      "drives",
      "agentcouldlearntoidentify“salient”eventsintheenvironment",
      "courtesy",
      "humanbrainmayexplainwhyhumanscanessentiallyperformasingle“conscious”reasoning",
      "justice",
      "Buildinganartificialsystemthatexhibitssuchbroadbehaviorisalong",
      "timescales",
      "parameterizing",
      "thenaveragingtheresults",
      "yˆ",
      "street",
      "tree",
      "masterpieces",
      "bill",
      "lawlearningcurves",
      "fear",
      "hypothesis",
      "background",
      "dependenciesisakeychallengeinmanysequencetransductiontasks",
      "framing",
      "feedbackwasprovidedby",
      "sincetheycanallocatealloftheircapacitytothem",
      "Effects",
      "peak",
      "sex",
      "Usingthesameparametersasinthepreviousexperiments",
      "pitch",
      "causation",
      "angle",
      "levelsperformshorter",
      "fridge",
      "8)",
      "centers",
      "judges",
      "Mind",
      "tionsoftheworldstate",
      "lawasafunctionofpositionT",
      "Bcrit=2.1×108tokens",
      "allowingformoreparallelism",
      "theHessianeigenvaluedensityisroughlyindependentofmodelsize",
      "abouty",
      "early-2000s",
      "Min",
      "correspondence",
      "Principles",
      "purpose",
      "psychoanalyst",
      "mistakes",
      "educa",
      "Scale(tokenization",
      "verb",
      "lawfittothelearningcurvesimpliesasimpleprescriptionforcompute",
      "predictedbythegradientnoisescale",
      "toproducetrajectorieswhicharepreferredbythehuman",
      "amygdala",
      "batchsize",
      "thestyleofShakespeare",
      "psychoanalysis",
      "JEPA",
      "waitress",
      "course",
      "primes",
      "gnosism",
      "compliment",
      "instrumentalvariables”",
      "crimes",
      "prices",
      "intellect",
      "mapping",
      "adjectival",
      "itwouldbeasignificantsteptowardsaddressingtheseconcerns",
      "multiplerunsisnecessarytovalidateperformancechangessmallerthanthislevel",
      "Allgraphsareaveragedover5runs",
      "ideas",
      "enkratia",
      "andthenumberofdataexamplesprocessedE",
      "followers",
      "mindedness",
      "D)Equation",
      "exemplars",
      "leavingonlychangesattributabletotheagent",
      "state",
      "thatg(x)=9",
      "philosophies",
      "deception",
      "termmemorymodulestoresrelevantinformationaboutthepast",
      "intersectionpointhasadeepermeaning",
      "S.-W.",
      "Reacher",
      "qualityalsodropsoffwithtoomanyheads",
      "D.4",
      "ChatGPT",
      "restraint",
      "musician",
      "InProceedingsofthe39thannualmeetingonassociationforcomputationallinguis-",
      "levelcontrolthroughdeepreinforcementlearning",
      "moments",
      "conceptssuchasconfigurablepredictiveworldmodel",
      "1f",
      "withoutdramaticincreases",
      "engines",
      "Discrimination",
      "openness",
      "MA",
      "water",
      "struggle",
      "muminiM",
      "regularizationwiththeadapativeschemedescribedinSection2.2.3",
      "blanks",
      "object",
      "InAdvancesinNeuralInformationProcessingSystems",
      "boxes",
      "snekoT",
      "quantiles",
      "aheadoftimeasasupervisedlearningprocessdivorcedfrominteractionwiththeenvironment",
      "y˜",
      "institution",
      "bounds",
      "datausedtotrainouragents",
      "whathasbeenproposedinthecontextofmemory",
      "memorization",
      "bornecollisionavoidancesystem”",
      "demonstration",
      "ourmethodfailstolearn",
      "camp",
      "butwebelieveitisworthwhileandprudenttodevelopaprincipledandforward",
      "variable",
      "proxy",
      "Let’stakeaconcreteexample",
      "C(s[t+1",
      "remainder",
      "Work",
      "patience",
      "thenthecomparisonisnotincludedinthedatabase",
      "5f",
      "thecompetitivemodels",
      "Analysis",
      "death",
      "seeking",
      "landupright",
      "estimators",
      "Rows",
      "practices",
      "number",
      "simplicity",
      "t",
      "largeandlimitedtrainingdata",
      "Durchbrock",
      "openings",
      "a[0]=A(s[0",
      "continuouscontroltasksandonthequalitativetasksinSection3.2;neverthelessitseemsimportant",
      "layer",
      "physician",
      "changes",
      "phes",
      "ofAIsystems",
      "∈Rhdv×dmodel",
      "suggeststhattheembeddingmatrixcanbemadesmallerwithoutimpactingperformance",
      "SamMcCandlish",
      "Thepolicyπ",
      "URLhttp://dl.acm.org/citation.cfm?id=2999134.2999257",
      "becomeshowtotraintheworldmodel",
      "finitude",
      "curvesbreaksdownveryearlyintraining",
      "analogy",
      "studies",
      "JournaloftheRoyalStatisticalSociety",
      "framework",
      "recursion",
      "layertransformerwithd",
      "floatingpointoperatorspertrainingtoken",
      "regression",
      "Building",
      "Net-",
      "confidenceandBerkleyParsercorporafromwithapproximately17Msentences",
      "fashion",
      "complexityoftasksstudiedinpriorwork",
      "showthatasmallamountoffeedbackfromanon",
      "nisthesequencelength",
      "LT16],oramoregeneralfeatureofthemodelarchitectureandoptimization",
      "tokenprobabilities",
      "evaluations",
      "argues",
      "elements",
      "followsapowerlawinthelossasperformanceincrease",
      "Testing",
      "videos",
      "formalism",
      "dilemma",
      "premium",
      "fraction",
      "Facts",
      "drunkard",
      "s2[0",
      "interpreters",
      "levels",
      "Thefirstpriority",
      "nothin’",
      "Robot",
      "Interests",
      "toyenvironments",
      "theologian",
      "contexts",
      "Architecture",
      "correlation",
      "reason-",
      "spark",
      "URL",
      "ability",
      "overload",
      "Biographies",
      "correctedness",
      "rows",
      "systematicunderstandingofthelimitationsofthescalinglaws",
      "assessment",
      "tubes",
      "box",
      "animals",
      "gods",
      "engineer",
      "analogia",
      "millisecond",
      "tion",
      "interpreter",
      "6NBS",
      "receptivity",
      "glory",
      "prompt",
      "mationandsoftmaxfunctiontoconvertthedecoderoutputtopredictednext",
      "list",
      "toys",
      "ego",
      "breaking",
      "bench",
      "shaping",
      "theperformancepredictedby",
      "Domain",
      "Philosophy",
      "spatial",
      "requests",
      "T.L.(2022",
      "recovery",
      "undermining",
      "controls",
      "hours",
      "onthisactiveresearchareainNLP).WenotethatcriticismsofthestandardapproachtomeasureAIsystems",
      "example",
      "feeling",
      "modulation",
      "birth",
      "marks",
      "PartiallyObservedGoals",
      "efficientinpractice",
      "datasetsizeandthe",
      "blossoming",
      "asoverfittinghappensvery",
      "explorationneededtodealwithachangingrewardfunction",
      "pages5026–5033,2012",
      "target",
      "Thislineofreasoningleadstotheinequality",
      "helicopter",
      "ensembles",
      "basedmodelsandplantoapplythemtoothertasks",
      "thanbyotherfunctionssuchasalogarithmataqualitativelevel",
      "goals",
      "attentionisanattentionmechanismrelatingdifferentpositions",
      "tools",
      "document",
      "thecollectivemotionofregionsbelongingtoarigidobject",
      "arefedtotwoencoders",
      "quantities",
      "sequencelengths",
      "spiritus",
      "consequences",
      "Argumentation",
      "tangent",
      "teleport",
      "overcome",
      "handful",
      "couldleadtosilentfailuresinamachinelearningsystem",
      "engine",
      "basedprocedureinwhichgradientsofthecostareback",
      "pie",
      "therebyallowingthespecificationofawiderangeofinput",
      "lawdependenceofperformanceonpositionincontext",
      "Socrates",
      "C−0.05,wecanconcludethatB",
      "Events",
      "subset",
      "ingrewardsatacost",
      "food",
      "cid:35)αD",
      "schools",
      "beckons",
      "context",
      "generations",
      "avoidoverfittingwhentrainingtowithinthatthresholdofconvergencewerequire",
      "Theseconsiderationsmotivateoursecondprinciple",
      "rooms",
      "Reinforcement",
      "cages",
      "vector",
      "complexitybyroughly3ordersofmagnitude",
      "Thesemodelsre",
      "outputs",
      "cid:28)B",
      "plotstomatchthepiechart",
      "effort",
      "day",
      "creation",
      "teachers",
      "ascent",
      "programmingmethodsorapproximatedynamicprogrammingmethodssuchasbeamsearch",
      "consequence",
      "subscription",
      "S2",
      "kindness",
      "dependenceofthelossusingEquation(1.6",
      "foresight",
      "ortakeanengineeringapproachofsimplytryingtodevelopdesign",
      "access",
      "determination",
      "6h",
      "cid:1)αN",
      "situation",
      "circle",
      "bottomsoftheencoderanddecoderstacks",
      "suchattentionmechanisms",
      "convolutionalneuralnetworks",
      "termpredictionsandlong",
      "anthologies",
      "conceals",
      "underworld",
      "aboutwhenthingshappeninthetrajectorysegment",
      "halfasmanyqueriesperunittime",
      "misframing",
      "equip",
      "ekstasis",
      "aslongasitincreaseswiththeenergyofthecorrect",
      "routing",
      "grade",
      "day=1015×24×3600=8.64×1019floatingpointoperations",
      "egomotion",
      "hallucinations",
      "periments",
      "content",
      "1802.07687",
      "Sample",
      "ML",
      "Errors",
      "Problems",
      "compute",
      "capabilities",
      "8.3.3",
      "fortwodifferentTransformermodelsizes",
      "order",
      "driver",
      "placeholder",
      "goodness",
      "array",
      "Lack",
      "theintrinsic",
      "conservation",
      "temptation",
      "strictures",
      "anestimateoftheminimalnumberoftrainingstepsneededtoreachagivenvalueoftheloss",
      "coarser",
      "Wethenconsidersomenovelbehaviorsineachdomain",
      "A(s[0",
      "developmentset",
      "leg",
      "schemes",
      "trialssothateventherarestcombinationofsituationswillbeencounteredfrequentlyduring",
      "COLLAPSE",
      "modality",
      "c˜",
      "dis-",
      "powers",
      "parlance",
      "pleasure",
      "ofpairedcomparisons",
      "latents",
      "intelligibility",
      "algorithm",
      "philosophia",
      "nimS",
      "segmentsoflengthk",
      "TrainingDataandBatching",
      "code",
      "reparameterization",
      "epoch",
      "likes",
      "speech",
      "atheisms",
      "honesty",
      "bootstrap",
      "satiety",
      "symphony",
      "12e",
      "family",
      "words",
      "spirituality",
      "Thought",
      "Actor",
      "relationships",
      "construction",
      "schedules",
      "imagery",
      "shutdown",
      "assets",
      "performancesforasmalllanguagemodelareplottedinFigure22",
      "variousfixedvaluesoftheloss",
      "prompts",
      "Theoreticalsupportforthisprincipleissignificantlyweakerthanforthefirsttwo",
      "min",
      "audioandvideo",
      "cascade",
      "motorcycle",
      "largermodelcanbetrainedthethesamelevelofperformanceinfewersteps",
      "unicorn",
      "Courage",
      "whichimpliesthatforcompute",
      "fallacy",
      "machines",
      "cancer",
      "InTable3rows(A),wevarythenumberofattentionheadsandtheattentionkeyandvaluedimensions",
      "informationfeedbacktoalearningsystem",
      "curveslateintraining",
      "intellectual",
      "stereograms",
      "bench-",
      "modelsize",
      "invoke",
      "wrappers",
      "suggestions",
      "isthenumberofparameterupdates",
      "proposal",
      "beconstant,(2)thecomponentsofs",
      "place",
      "hints",
      "actuallywitnessintheclip",
      "stoppedperformance;wehavenotseenthissurprisingresultinotherexperiments",
      "functional",
      "learningaseparaterewardmodelusingsupervisedlearning",
      "phronesis",
      "X",
      "landscaping",
      "feedbackisprovidedbycontractorswhoaregivena1",
      "mediates",
      "leadingterms",
      "gatedmixture",
      "definingthebaselinepolicyπ",
      "wells",
      "directions",
      "0),theenergywilldecrease",
      "playergamebetween",
      "subject",
      "itover",
      "key",
      "advocate",
      "Position",
      "≈0.054",
      "discomfort",
      "Connection",
      "weights",
      "hypotheses",
      "suchasimages",
      "Buffering",
      "sociologist",
      "poet",
      "gelassenheit",
      "philosophy",
      "algorithms",
      "functionalsoincreasesgreatlywiththecomplexityoftheagentanditsavailablestrategies",
      "contemplation",
      "size",
      "camera",
      "eyes",
      "receiveahightotalrewardaccordingtor",
      "aim",
      "abs/1409.0473,2014",
      "flaws",
      "adult",
      "compensates",
      "environmentinteractionsareoftenradicallycheaperthanhumaninteraction",
      "Statement",
      "TC1(s",
      "entry",
      "parallels",
      "Hacking",
      "wall",
      "approxi-",
      "alization",
      "sev-",
      "natura",
      "places",
      "replacement",
      "Exploration",
      "farm",
      "deficit",
      "thinkers",
      "abroadswathofdomains",
      "GNMT+RLEnsemble[38",
      "entropy",
      "patterns",
      "bayes",
      "management",
      "predictability",
      "Co",
      "tip",
      "parametercount(includingtheembeddingparameters)thetrendissomewhatobscured(seeFigure6",
      "pro-",
      "oneshouldexpectdifferentresponsesfromthefinalversionofGPT-",
      "lawregionofthelearningcurve",
      "microchip",
      "template",
      "propositional",
      "limits",
      "abstraction",
      "observations",
      "lifespan",
      "deceptiveness",
      "cohort",
      "rangeoftasksanddomains",
      "guagemodeling",
      "bucket",
      "latersections",
      "pleaseping@tomonslack",
      "Thefollowingsubsectionsprovidedetailsoneachoftheseprocesses",
      "frequency",
      "subjectivity",
      "write",
      "cid:19)1",
      "attentionlayers",
      "heritage",
      "mindsetting",
      "efficienttrainingwould",
      "Wehavechosentheparameterization(1.5)(repeatedhereforconvenience",
      "pair",
      "driving",
      "traits",
      "thatduetothenonstationarityoftheoccupancydistribution",
      "attitude",
      "behaviordriventhroughintrinsic",
      "JakobUszkoreit∗",
      "thereareseveralwaysinwhichtheproblemcanoccur",
      "Figure5",
      "muchfasterandmorespace",
      "hurtsmodelquality",
      "conformity",
      "7.2.1",
      "Daoism",
      "Intelligence",
      "Theinferredabstractactionsarefedtolower",
      "2d",
      "catastrophes",
      "Thensetatimerfor5minutesandexplorethegametoseehowitworks",
      "making",
      "9(8):1735–1780,1997",
      "followup",
      "grip",
      "ofdeepRLtocomplexreal",
      "metaphysics",
      "exponential",
      "invariance",
      "s[0]=Enc(x",
      "itmaybenecessarytouseasmallerlearningratetoavoiddivergences",
      "lossthatwouldbeachievedifwetrainedformuchlonger",
      "networks",
      "axis",
      "there's",
      "Salience",
      "skill",
      "orC",
      "cid:29)B",
      "eros",
      "convolutions",
      "scientists",
      "relationsgobeyondmereobservationtoprovideapredictiveframework",
      "Applause",
      "ture",
      "establishment",
      "interaction",
      "physicalist",
      "student",
      "landscape",
      "texttransformer,2019,arXiv:1910.10683",
      "talks",
      "policies",
      "Expansionsinthemodelsize",
      "coherence",
      "thelearnedrewardfunction",
      "baselines",
      "toquicklyrecognizepatterns",
      "allpositionsinthedecoderuptoandincludingthatposition",
      "Nietzsche",
      "modes",
      "energy",
      "afterinitialtransienteffects",
      "propellers",
      "threshold",
      "weighting",
      "Tool",
      "tomorrow",
      "riskestimationbypositingcertainconditionalindependenciesinthedistributionoferrors",
      "groundingness",
      "fifties",
      "considerthreedesiderata",
      "bug",
      "sentation",
      "Product",
      "safety",
      "fromhumanfeedback",
      "symbols",
      "forces",
      "script",
      "environ-",
      "reading",
      "Reward",
      "dependenceoftrainingspeedonmodelsize",
      "sleep",
      "skills",
      "andQuocV.Le",
      "loss",
      "Glitch",
      "circuitry",
      "prophecy",
      "traditions",
      "fields",
      "inferences",
      "pres",
      "university",
      "generativity",
      "segment",
      "strand",
      "Identity",
      "streams",
      "releasement",
      "Apossiblepromisingavenueofapproachwouldbemorerealisticversions",
      "fate",
      "challenging",
      "immortals",
      "weseeempiricallythatδLdependsonlyaspecificcombination",
      "dimensionality",
      "tractable",
      "tostructuresinthebasalgangliainvolvedinrewards",
      "deliverance",
      "sacrament",
      "independencies",
      "infants",
      "word",
      "oracle",
      "encoder",
      "allocation",
      "researcher",
      "chart",
      "Uncertainty",
      "routines",
      "andthenconfirmthatprediction",
      "eksistent",
      "fuzzyfication",
      "Datasets",
      "onement",
      "Row",
      "value",
      "ifthehumanmarksthesegments",
      "attunement",
      "Encoder",
      "Loss",
      "efforts",
      "throughagradient",
      "arXiv:1502.02072",
      "playthegameyourselfforafiveminutestogetasenseofhow",
      "distance",
      "learningcommunityonaccidentsinparticularwasdiscussedindetailabove",
      "evolution",
      "175–188",
      "attentionanddiscussitsadvantagesovermodelssuchas[17,18]and[9",
      "trends",
      "thedecoderinsertsathirdsub",
      "responsibility",
      "Covariance",
      "ofgatedrecurrentneuralnetworksonsequencemodeling",
      "term",
      "fundamentalisms",
      "ecnegrevnoc",
      "pounds",
      "estimation",
      "tokens",
      "hadbeenusingRLtooptimizer",
      "Inference",
      "weuseadvantageactor",
      "forexampleusingasynchronoustrainingand",
      "Roots",
      "annealing",
      "supply",
      "sparsification",
      "ofasinglesequenceinordertocomputearepresentationofthesequence",
      "Sections",
      "metric",
      "ties",
      "GLVEBM",
      "repre-",
      "survey",
      "Icon",
      "dependency",
      "Sc",
      "similarities",
      "agnosticormakelimitedassumptionsaboutotheraspects",
      "theirbehavior",
      "confusion",
      "installing",
      "supplement",
      "A.-H.",
      "host",
      "polynomials",
      "principle",
      "c",
      "fork",
      "cid:19)αC",
      "functionals",
      "basedontheexpectedvalueofinformationofthequery(Akrouretal",
      "inequality",
      "correlate",
      "consideravariant",
      "arepower",
      "symbolic",
      "sign",
      "theJEPAfindsatrade",
      "robotics",
      "contagion",
      "popularity",
      "surplus",
      "sequence",
      "H",
      "sciences",
      "solidity",
      "4convolutionallayers",
      "cat",
      "ResultsforL(N",
      "relations",
      "GL.sc",
      "postings",
      "suitablecontrastivesampleswhoseenergyislowand“threatening",
      "arenecessaryfornear",
      "foreshadow",
      "weprovideasummarybelowofthekeytrendsdescribedthroughoutthepaper",
      "PhD",
      "ratio",
      "weightinginvolvesassumingawell",
      "Trustregion",
      "Thepositionalencodingshavethesamedimensiond",
      "delta",
      "clarity",
      "gatherabouttwoordersofmagnitudemorecomparisons",
      "blocks",
      "formers",
      "learningfrom350,700,or1400syntheticqueries",
      "motion",
      "mathematics",
      "andfindafit",
      "nerve",
      "peculiarities",
      "potentialforAItotransformmedicine[126],science[59],andtransportation[86],alongwithconcerns",
      "worlds",
      "sistent",
      "transtemporal",
      "trainedthebasemodelsforatotalof100,000stepsor12hours",
      "andsendthemtoahumanforcomparison",
      "weachieveanewstateoftheart",
      "environmentitself;humanfeedbackprovidesitsonlyguidanceaboutwhatitoughttodo",
      "shining",
      "processing",
      "modelbymultiplyingthetrainingtime",
      "moredifficult",
      "dining",
      "sensibilities",
      "versions",
      "formatting",
      "dependencies",
      "thefirstsequencetransductionmodelbasedentirelyon",
      "With700labelsweareabletonearlymatchreinforcementlearningonallofthesetasks",
      "perspective",
      "density",
      "reuse",
      "following",
      "3b",
      "wrist",
      "handbooks",
      "metaheuristic",
      "limitations",
      "prime",
      "D(s",
      "thoughts",
      "N.(2021).Hierarchicalskillsforefficient",
      "canassumethattherewardislinearintheexpectationsofhand",
      "kid",
      "positioninthedecodertoattendoverallpositionsintheinputsequence",
      "mediation",
      "static",
      "glitch",
      "pages1–7,2011",
      "timestep",
      "infinity",
      "RL",
      "commentary",
      "Lessons",
      "confusedbychangesinbackground",
      "son",
      "river",
      "parison",
      "agape",
      "sciousness",
      "worldenvironments",
      "andIlyaSutskever",
      "optimalperformance",
      "MitchellPMarcus",
      "A.2",
      "evidence",
      "alliteration",
      "icon",
      "equations",
      "distinction",
      "damage",
      "IMDb",
      "isnotsosimple",
      "perimentation",
      "autobiography",
      "WedescribethedetailsofourexperimentsinAppendixA",
      "forget",
      "philosopher",
      "air",
      "spaces",
      "ands",
      "Notethatthisequationshouldnotbetrustedforverylargemodels",
      "realhumanfeedback(purple),ourmethodusingsyntheticfeedbackprovidedbyanoracle(shadesof",
      "Gradient",
      "rain",
      "excellence",
      "trajectories",
      "debt",
      "heads",
      "import",
      "layers",
      "mystic",
      "feed",
      "relativist",
      "vertebrates",
      "ponentsanalysis",
      "functions",
      "rangingfromsmallmodelswithshape",
      "sampling",
      "intrinsiccost",
      "lawshouldprovideinformationaboutthespectrumoftheHessianoftheloss",
      "K.-H.",
      "hippocampus",
      "mysticism",
      "amodelwithfixedsizewillbecapacity",
      "attentionmechanism",
      "affords",
      "theword‘making",
      "SimulatedRoboticsTasks",
      "Twitter",
      "misuse",
      "conversion",
      "possession",
      "Thiscanbeproblematicinmanycases",
      "sell",
      "disorders",
      "contracts",
      "point",
      "whichareoftenusedasintrinsicrewards",
      "thefirstpriorityisfortherobottobestandingupright",
      "permission",
      "prevention",
      "trainedlog",
      "Trainingonsuboptimalmodels",
      "net",
      "ofsuccessfullypassingothercarsthroughrandomexploration",
      "disadvantage",
      "phenomenologist",
      "andAlexanderM.Rush",
      "insurance",
      "thefactthatGPT-4canusetools(includingitself)willcertainlybeofimmense",
      "humanratersawhilejusttounderstandthesituation",
      "alsoinlayer5of6,apparentlyinvolvedinanaphoraresolution",
      "hack",
      "Psyche",
      "kitchen",
      "Insystemsthatinvolveacontinualcycleoflearninganddeployment",
      "curse",
      "≈2×108andα",
      "complexitygoals",
      "thatisnotpresentins",
      "2Wilsonetal.(2012)alsoassumestheabilitytosamplereasonableinitialstates",
      "allthe",
      "∗",
      "tokensinthesequence",
      "impulse",
      "exaptation",
      "interactswiththeenvironmenttoproduceasetoftrajectories{τ1,",
      "instructions",
      "annotation",
      "Context",
      "ations",
      "apreviousstate",
      "thathaveawell",
      "unfolding",
      "happeninginthegame",
      "levelcostmodulesC(s[2]),C(s[4])whichdefinesubgoals",
      "networkgrammars",
      "splinter",
      "simulations",
      "repercussions",
      "shortcut",
      "girlfriend",
      "occlusion",
      "terminationconditionsbyapenaltywhichencouragestheparameterstoremainintherange",
      "impasse",
      "promise",
      "Ourresultsonoptimization",
      "9Thereisaslightambiguityhere",
      "ligence",
      "stance",
      "usability",
      "Results",
      "It's",
      "Volume",
      "colleagues",
      "TheRatingofChessplayers",
      "lines",
      "propaganda",
      "endures",
      "Weincludeseveraldifferentdifferentpossibilitiesforparameterre",
      "PF",
      "cycles",
      "‡WorkperformedwhileatGoogleResearch",
      "humanity",
      "abs/1901.10159",
      "basedcontrol",
      "arrays",
      "catastro-",
      "consistsoftwolineartransformationswithaReLUactivationinbetween",
      "clip",
      "systemcontrolledbyanadversary(e.g",
      "Computers",
      "cycle",
      "announcement",
      "angels",
      "History",
      "position",
      "act",
      "attentionheads),withspecificnumericalvaluesassociatedwiththeWebtext2",
      "Towardsstory",
      "lawfits",
      "phase",
      "towers",
      "Notions",
      "Theheadsclearlylearnedtoperformdifferenttasks",
      "eraser",
      "avenues",
      "AGI",
      "Thereisasizableliteratureonsuchsafeexploration",
      "inthefirstplace",
      "fits",
      "disciplines",
      "dmodel",
      "lite",
      "Differentcolorsrepresentdifferentheads",
      "decade",
      "diction",
      "WehavechosenthisparameterizationforB",
      "spectrum",
      "behaviors",
      "163–168",
      "Background",
      "Variations",
      "thecostmoduleestimatesatotalcostfromthepredictedstatesequence",
      "counterparts",
      "Page",
      "weattempttosimilarlyannealthelabelrate",
      "handbook",
      "grid",
      "space",
      "minimization",
      "cure",
      "filters",
      "average",
      "Thisrepresentsasteptowardspracticalapplications",
      "wewillderivetheoptimalperformance",
      "Embedding",
      "Ourapproachistolearnarewardfunctionfromhumanfeedbackandthentooptimizethatreward",
      "behalf",
      "versionsproducednearlyidenticalresults(seeTable3row(E)).Wechosethesinusoidalversion",
      "onfurthersample",
      "k",
      "bot",
      "living",
      "dimensionoftheintermediatefeed",
      "inflation",
      "ways",
      "Thiscorrespondstoincreasingthelearningratelinearlyforthefirstwarmup_stepstrainingsteps",
      "proponents",
      "versal",
      "pole",
      "theactorsendsthefirstaction",
      "Thiscanbedone",
      "line",
      "return",
      "rangedependenciesinthenetwork",
      "reverse",
      "andothers",
      "rithms",
      "≈B",
      "gifts",
      "basedneuralmachinetranslation",
      "itselfalinearfunctionofhand",
      "dilemmas",
      "sacredness",
      "Values",
      "energythatmeasurestheinstantaneous“discomfort”oftheagent",
      "individual",
      "virtues",
      "solve",
      "exclusion",
      "difficulties",
      "Trainingcanbeperformedlevel",
      "5Although",
      "norms",
      "yˆ=",
      "enhancement",
      "summarization",
      "invocation",
      "norm",
      "wouldrequiretheintroductionofanadditionalparameter",
      "strides",
      "agreement",
      "M.A.",
      "2b",
      "alternative",
      "temperature",
      "cid:17)αS",
      "languagequestionansweringand",
      "Vote",
      "Understanding",
      "miracle",
      "theenergyoftheinitialstatef[0]=C(s[0])andstoresthepairs(s[0],f[0])intheshort",
      "relay",
      "rock",
      "discourse",
      "hermeneutics",
      "triangle",
      "lawrelationbetweenaccuracyandmodelsize",
      "challenges",
      "astheembeddings",
      "JEPAs",
      "Thelineisourfittothatequation",
      "angles",
      "bills",
      "accounting",
      "wevariedourbasemodel",
      "recommendations",
      "Offlinereinforcementlearningasonebigsequence",
      "Reality",
      "basket",
      "tigers",
      "team",
      "Scenario",
      "linearlyforConvS2SandlogarithmicallyforByteNet",
      "centrality",
      "SamMcCandlish∗",
      "job",
      "System",
      "ground",
      "settings",
      "potential",
      "forinfluenceoveritsenvironment",
      "Ways",
      "identification",
      "overlay",
      "schedulesappeartobestatisticalnoise",
      "servers",
      "idealizations",
      "endmemorynetworksarebasedonarecurrentattentionmechanisminsteadofsequence-",
      "technologies",
      "contributor",
      "7b",
      "route",
      "righteousness",
      "finders",
      "professor",
      "Inthissection",
      "externalities",
      "interconnection",
      "linguistics",
      "whilelower",
      "Theapproachesdiscussedaboveallrelyrelativelystronglyonhavingawell",
      "similarity",
      "considerations",
      "corpusofenglish",
      "weemployresidualconnections",
      "religions",
      "exceptforthescalingfactor",
      "Inanycase",
      "weimmediatelyreplybyindicatingapreferenceforwhichever",
      "study",
      "expertfeedback.6",
      "hubristic",
      "suicide",
      "butnevertheless",
      "notes",
      "accounts",
      "ruins",
      "0.9,β",
      "ofthecleaningrobotthroughoutthedocument",
      "application",
      "ForthefitsweuseEquation(5.5)and(5.4)alongwiththeparameters",
      "Shepard,1957)topreferencesovertrajectorysegments",
      "theapproachwetakeinourmodel",
      "learningandthebias",
      "quents",
      "increasingthelengthofthelongestpaths",
      "BLEU",
      "experimentsdiscoveredtobehelpfulandwhichareanalyzedinSection3.3",
      "butthepointisthatside",
      "ofoverfitting",
      "statue",
      "butitcurrentlysuffersfromalackofdevelopmentinthecontextofmachine",
      "b",
      "poles",
      "founding",
      "soul",
      "Machine",
      "numberofrelevantfeaturesinthedata",
      "translationqualityafterbeingtrainedforaslittleastwelvehoursoneightP100GPUs",
      "manstoprovideinsomedomains",
      "endobjectdetectionwithtransformers",
      "Mindset",
      "originator",
      "termssuchasnonlinearities",
      "counterexamples",
      "drawing",
      "stops",
      "rˆ:O×A→R",
      "d",
      "anticipation",
      "origin",
      "withlearnedrewardfunctionstendstobelessstableandhighervariance",
      "fromthetrajectories{τ1,",
      "writing",
      "Schwartz's",
      "seconds",
      "drive",
      "imagination",
      "Utility",
      "targets",
      "scenario",
      "Section",
      "Beinghighforawhileandlowforawhileisworsethanbeingatanintermediatedistance",
      "physis",
      "architecturesfromtheliterature",
      "chair",
      "propagate",
      "brack",
      "tofindthemodel",
      "improvements",
      "cartpole",
      "pieces",
      "batches",
      "externalworld",
      "theologians",
      "excess",
      "theefforttoevaluatethisidea",
      "forthinkingaboutsomeofourscalingrelations",
      "sub",
      "rest",
      "Writes",
      "Rule",
      "infinite",
      "tesataD",
      "approximatelydoubles",
      "InMacGlashanetal.(2017)andPilarskietal.(2011),learningonlyoccurs",
      "commitments",
      "byte",
      "FitstoL(N",
      "Qualitative",
      "bunch",
      "neighbors",
      "opinion",
      "meaning",
      "jargon",
      "syle",
      "pratique",
      "Tensorflow",
      "arXiv:1407.7644",
      "figure",
      "longing",
      "Videosofthesebehaviorscanbefoundatthislink",
      "lawscalingswithmodelanddataset",
      "tothedatawiththeparameters",
      "usedsuccessfullyinavarietyoftasksincludingreadingcomprehension",
      "computetradeoff",
      "generativearchitectureforpredictiveworldmod-",
      "Inthesemodels",
      "forwardconnectionsforneuralmachinetranslation",
      "transformations",
      "stand",
      "information",
      "Learningtolinearizeunderuncertainty",
      "delay",
      "Twoattentionheads",
      "toproducecleanertrends",
      "weusesineandcosinefunctionsofdifferentfrequencies",
      "sensory",
      "vices",
      "vocabularysizeandtokenizationandhencedonothaveafundamentalmeaning",
      "image",
      "basedoptimizationprocesstoinferanoptimalactionsequence",
      "breakthecoloredblocks",
      "minimax",
      "reduceconstituentparsing",
      "Thecriticalbatchsize",
      "sors",
      "yann@fb.com",
      "mis-",
      "People",
      "scalability",
      "Thelossvariesonlyafewpercentoverawiderangeofshapes",
      "WequotenumericalvaluesinPF",
      "ThesefitswereusedtomeasureB",
      "composition",
      "existence",
      "provideabetter",
      "estimator",
      "incentive",
      "reversal",
      "parallelwiththeimprovementonWebText2",
      "Welcome",
      "Self",
      "returns",
      "current",
      "convergent",
      "subtlesourceofsupervision",
      "functionsofdistances",
      "pipeline",
      "era",
      "generality",
      "Influence",
      "visedlearningi",
      "intuition",
      "scalings",
      "ifatall",
      "thehyperparameters",
      "notion",
      "predictablyasapower",
      "potentiallyrepresentingasignificantboostinqualitativeperformance",
      "swing",
      "parameters(excludingembeddings",
      "denotedbyablackbarforeachrun",
      "nontheism",
      "enactment",
      "statesh",
      "weinsteadassumetherewardisafunctionofthepreceding4observations",
      "humanmorphology",
      "mystique",
      "Proceedings",
      "status",
      "cies",
      "routes",
      "fire",
      "shift",
      "combinedwithfactthattheoutputembeddingsareoffsetbyoneposition",
      "ukasz",
      "systems",
      "argument",
      "seeFigure4",
      "worksforlearningaboutobjects",
      "critic",
      "allegations",
      "a]+",
      "functionthandotproductmaybebeneficial",
      "rover",
      "cultivation",
      "founders",
      "resultstothebasemodel",
      "modifications",
      "soundness",
      "substances",
      "fitting",
      "hundreds",
      "allotherparameters",
      "wireheading",
      "s",
      "screws",
      "regularization",
      "RR",
      "outperformingallofthepreviouslypublishedsinglemodels",
      "learners",
      "clings",
      "occurrence",
      "FittingtheRewardFunction",
      "difference",
      "sacrifices",
      "specifiedinpractice",
      "justification",
      "Continuity",
      "fraud",
      "tag",
      "andsafeexploration;wereviewtheseindetailbelow",
      "Nazi",
      "window",
      "facts",
      "Shephardchoicerule(Luce,2005",
      "toddler",
      "supplies",
      "destiny",
      "knowledge",
      "balance",
      "induction",
      "a˜[0",
      "Training",
      "pipelining[HCC+18],whichsplitsparametersdepth",
      "Whilethelineartransformationsarethesameacrossdifferentpositions",
      "Layer",
      "butreliablebaselinepolicy",
      "photo",
      "robot",
      "coincidence",
      "postmodern",
      "dynamic",
      "inorder",
      "drivesoftheagentdifficulttocontrol",
      "PerformancewithDatasetSizeandCompute",
      "comparingindividualstates",
      "encoders",
      "differentiability",
      "hurtsperplexity",
      "dance",
      "Existentialism",
      "jobs",
      "bit",
      "glasses",
      "loops",
      "Infinity",
      "hallmark",
      "L∗",
      "manipulate",
      "philologist",
      "indifference",
      "transcription",
      "methodology",
      "vision",
      "predator",
      "costofadditionalcomputeper",
      "pages222–225,2012",
      "curves",
      "poem",
      "andtodesign",
      "taps",
      "Conflict",
      "10b",
      "amazonaws",
      "march",
      "tonos",
      "betterthingshappenintheleftclipandthustheleftclipisbetter",
      "creasingmodelcapacity",
      "Need",
      "continuouscontroltasksbecausetherelativemagnitudeoftherewardareimportanttolearning",
      "psychotechnology",
      "effectiveness",
      "14c",
      "steering",
      "quantum",
      "dysrationalia",
      "illusion",
      "grudges",
      "emotions",
      "Words",
      "left",
      "decomposition",
      "PennTreebank[25],about40Ktrainingsentences",
      "contradiction",
      "neuroanatomy",
      "embeddingtrainingcompute",
      "2014)[37",
      "postmodernist",
      "Largelanguagemodels(LLMs)seemtopossessasurprisinglylargeamount",
      "backonlineimprovesthesystem’sperformance",
      "relationship",
      "Ourresultsstronglysuggestthatlargermodelswillcontinuetoperformbetter",
      "legitimation",
      "Improvements",
      "harm",
      "infant",
      "velocities",
      "whereweexpectcomplexity",
      "cherubs",
      "baby",
      "ment(Bellemareetal",
      "station",
      "Concepts",
      "sake",
      "sun",
      "checks",
      "Inprinciplethisfitswithintheparadigmofreinforcement",
      "Trip",
      "trainingparadigmswithwhichtoconstructautonomousintelligentagents",
      "presentations",
      "letting",
      "crystals",
      "disruptions",
      "faqih",
      "mentorship",
      "syntheticlabelsmatchorcomeclosetoRLevenwithonly3,300such",
      "gains",
      "inertia",
      "ment",
      "representations",
      "reliability",
      "mareetal",
      "todevelopasinglearchitecturethatcanlearntoavoidcatastrophesinallenvironmentsinthesuite",
      "narcissism",
      "result",
      "mostoftheincreaseshouldgotowardsincreasedmodel",
      "Withinreasonablelimits",
      "program",
      "P",
      "Value",
      "collision",
      "TL19]alsoappeartoobeyanapproximatepower",
      "predictabletrends",
      "4e",
      "literature8",
      "praises",
      "dirt",
      "jump",
      "friend",
      "velocity",
      "opti-",
      "therapy",
      "ChartingtheInfiniteDataLimitandOverfitting",
      "predicament",
      "scientist",
      "whichcouldbeinterpretedasmodelingthehumantobeindifferent",
      "whichwasdefinedinEquation(5.2)9",
      "satisfaction",
      "environmentsincreaseincomplexity",
      "corners",
      "Expertise",
      "LayerNorm(x+Sublayer(x)),whereSublayer(x)isthefunctionimplementedbythesub",
      "aswellasconcernsaboutthelonger",
      "viapolicyoptimization",
      "w",
      "levelstatemustsatisfyinorderforthehigh",
      "stration",
      "Confirmation",
      "isalsoapower",
      "5998–6008",
      "mean",
      "certainty",
      "whileweperformexperimentswithfeedbackgatheredfromnon",
      "ofthesetokensforuseasatestset",
      "forefront",
      "month",
      "logarithm",
      "embd",
      "rug",
      "wherewehaveexcludedbiasesandothersub",
      "g(x",
      "andsemanticstructureofthesentences",
      "planning",
      "momentum",
      "calculator",
      "andL(D)duetotheslowgrowthofdataneededforcompute",
      "architectures",
      "tokenizer",
      "inheritance",
      "cases",
      "thecrossentropylossinnats",
      "pedestrians",
      "daddy",
      "thehuman",
      "includingmodelarchitectures",
      "toformgoals",
      "945–987",
      "Exceptwhereotherwisestatedweuseanensembleof3predictors",
      "theinitializationscale",
      "Wehavealsotestedourmodelsonasetofadditionaltextdatadistributions",
      "Attention",
      "classification",
      "Goodfellow",
      "poiesis",
      "sample",
      "carbon",
      "Ideallytheagentwillachieverewardnearlyashighasifit",
      "suchasword",
      "accordingtoourbyte",
      "Ifyoudon’tgetclipsformorethanacoupleminutes",
      "ssoL",
      "conferences",
      "followedbylayernormalization",
      "productattentionbymaskingout(settingto−∞)allvaluesintheinput",
      "states",
      "resultinginapower",
      "romanticism",
      "andyou’ll",
      "age",
      "homosapiens",
      "bytherestofthenetwork",
      "standards",
      "gratitude",
      "Future",
      "representative",
      "E.1",
      "Physis",
      "regu-",
      "Inspiration",
      "Approaches",
      "alignedRNNsorconvolution",
      "pathlengthtoO(n",
      "Canactivememoryreplaceattention",
      "relationshowsthatdatasetsizemaygrowsub",
      "Suchatheorymightmakeitpossibletoderiveothermoreprecisepredictions",
      "testingthemodel",
      "SIAM",
      "psychology",
      "ICML(3)28(2013),pp.325–333",
      "emo",
      "Higherlevelsperformlonger",
      "Inthefollowingsections",
      "manuscript",
      "resonant",
      "thesewhendiscussingthe‘modelsize’N;wewillseethatthisproducessignificantlycleanerscalinglaws",
      "ofdataexamplesprocessedmakesuptheremainderoftheincrease",
      "thecriticretrievesapaststatevectors",
      "computinghiddenrepresentationsinparallelforallinputandoutputpositions",
      "EmpiricalResultsandBasicPowerLaws",
      "romantics",
      "enlightenment",
      "contentofs",
      "/frɒm/",
      "traverseinthenetwork",
      "editors",
      "opsinthebeginningofanepisode",
      "pen",
      "engineeringdiscipline",
      "geez",
      "progress",
      "lump",
      "efficaciousness",
      "whichdeterminesthespeed",
      "cid:19)−1",
      "talk",
      "sparse",
      "positionalencodingsinboththeencoderanddecoderstacks",
      "integrity",
      "gnosis",
      "ThisfollowsfromthescalinglawofEquation(1.5),whichimplies",
      "tokensintheinput",
      "errors",
      "responses",
      "channels",
      "tionalton",
      "nlayer",
      "Gnosis",
      "stop",
      "ripples",
      "Research",
      "pages",
      "prediction",
      "advent",
      "fundamentaltechnicalchallenges",
      "discovery",
      "stick",
      "designer",
      "car",
      "discountrateγ",
      "togetherwithanintrinsicenergyatalater",
      "datasetset",
      "thequeriescomefromthepreviousdecoderlayer",
      "orsmallermodelscanbetrainedwithminimaladditionalcompute",
      "suggestingthatlarger",
      "butcannotmatch",
      "3d",
      "insideofscaleddot",
      "revelation",
      "thingness",
      "reserves",
      "squirrel",
      "interdependence",
      "tuning",
      "efficienttrainingdescribedinthissectionalreadycontainanapparentcontra-",
      "Teaching",
      "theorist",
      "triangles",
      "annals",
      "espe",
      "L(N)=(N",
      "introduction",
      "Sincetherewardpredictorisultimatelyusedtocomparetwosumsovertimesteps",
      "virus",
      "1d",
      "capability",
      "empiricism",
      "beginning",
      "Criterion",
      "wire",
      "decoder",
      "press",
      "table",
      "affective",
      "Forests",
      "Wood",
      "ThecriticalbatchsizeB",
      "termprediction",
      "asendinganepisodewhentheagentdiesorfallsover",
      "theconfiguratormaymodulatetheweightsoflow",
      "covariateshift",
      "Workshops",
      "clips",
      "priorities",
      "En",
      "glorification",
      "Transparency",
      "andfeed",
      "Epek",
      "Applications",
      "boyfriend",
      "meanings",
      "definedness",
      "tribution",
      "ai",
      "wherehead",
      "work",
      "covers",
      "phenomenon",
      "empathy",
      "decay",
      "availability",
      "anarchy",
      "scare",
      "us—\"as",
      "termpredictions",
      "enables",
      "duringtraining",
      "font",
      "stream",
      "benchmark",
      "transgressive",
      "extension",
      "pendulum",
      "eters",
      "transduction",
      "representationalism",
      "wires",
      "radio",
      "p∗",
      "Overview",
      "penumbra",
      "rings",
      "u",
      "residuals",
      "thefollowingruleofthumbwasusedformostruns",
      "Article",
      "assurances",
      "Usingasmallermodelisusefulwhenaccountingforthecostinference",
      "discussion",
      "prompting",
      "ning",
      "toy",
      "for200epochsbeforebeginningRLtraining",
      "Consciousness",
      "testtheserelationsonotherdomains",
      "competencies",
      "interviews",
      "Wealsoshowthetrainandtestlossforagivenmodelsizewhentrainingondifferent",
      "responders",
      "reality",
      "parallax",
      "Acknowledgements",
      "shootenemyships(good),anddon’tletyourship(theoneatthebottomof",
      "Pairwise",
      "dynamics",
      "Notice",
      "Onesubtletyis",
      "scores",
      "writes",
      "plaque",
      "ormerelytoinitializetheproposedaction",
      "Transformer(big",
      "penaltybutotherwiseimprovesroughlyinlinewithperformanceonthetrainingset",
      "embed",
      "reproducethetablesandfiguresinthispapersolelyforuseinjournalisticor",
      "asshole",
      "gating",
      "representatives",
      "areadjustedfortrainingatthecriticalbatchsizeB",
      "Robustness",
      "fount",
      "dopamine",
      "involvement",
      "effectrelationships",
      "atari",
      "disease",
      "L)=",
      "cutting",
      "archetype",
      "generator",
      "FitDetailsandPowerLawQuality",
      "quadrant",
      "mouth",
      "andthesecondisasimple",
      "RogerNShepard",
      "funeral",
      "RomanVYampolskiy",
      "members",
      "literature",
      "learningwithweaklylabeleddata”",
      "Misconceptions",
      "matrixmultiplication",
      "fewoptimizationsteps",
      "spine",
      "foundation",
      "author",
      "personimitationlearning",
      "Strategy",
      "medic",
      "n",
      "identity",
      "polymath",
      "transcriptions",
      "D.8",
      "rectangle",
      "energyneedstobeturnedintoaprobabilitydistribution",
      "Thereareavarietyofareasthatarepotentiallyrelevanttothisproblem",
      "andpreventsitfromexploitingweaknessesof",
      "thedatasetsizeD",
      "Fuccia)—all",
      "ara",
      "biguation",
      "propositions",
      "Thecostmodulecomputes",
      "Changes",
      "andblenders",
      "ofthevalues",
      "dust",
      "music",
      "m",
      "proof",
      "questionner",
      "approximatebecausecontractorsgivefeedbackatunevenrates",
      "priority",
      "Internalization",
      "Secondisthetrendtowardmorecomplexagents",
      "programs",
      "heat",
      "leaves",
      "wisefullyconnectedfeed",
      "reference",
      "anagoge",
      "neverhavingseenbefore",
      "ex-",
      "wecanchoosehowmuchtoallocatetowardstraininglarger",
      "groundedness",
      "ispotentiallypromising",
      "andyou’llseeanerrormessage",
      "wepredict",
      "styles",
      "1950s",
      "andα",
      "improvement",
      "ordersofmagnitude",
      "affording",
      "psychotech",
      "opponent",
      "weattainoptimalperformancebytrainingverylargemodels",
      "computes",
      "help",
      "runsusingEquation(1.6),repeatedhereforconvenience",
      "middle",
      "autoencoders",
      "Todatemuchofthisdiscussionhas",
      "weapplydropouttothesumsoftheembeddingsandthe",
      "consideration",
      "SpaceInvaders",
      "policyoptimization",
      "noise",
      "Inaddition",
      "R(z",
      "essence",
      "yielding",
      "limitingregularizerThiswouldcause",
      "structure",
      "meanperformance",
      "users",
      "failure",
      "CarloTreeSearch(MCTS).Inthecase",
      "lightning",
      "3a",
      "tasis",
      "heuristic",
      "afterconvergingonalow",
      "tensorflow",
      "restrictions",
      "attentionlayersrunninginparallel",
      "color",
      "LC.sc",
      "andtheabilitytolearnfromexperience",
      "appearance",
      "Attunement",
      "Models",
      "survival",
      "standard",
      "action",
      "lelismispossible",
      "nets",
      "andprovidearoughgaugeforthescaleofvariationbetweendifferent",
      "fairness",
      "oversight)gainaddedcomplexityinanRLsetting",
      "science",
      "aging",
      "epek",
      "crew",
      "input",
      "framework(Hadfield",
      "arenottrainedonthecorrectdistributionmaybadlymisunderstanditssituation",
      "connections",
      "horn",
      "days",
      "withapenaltyinallgamesexceptPong;theagentmustlearnthispenalty",
      "adolescent",
      "despair",
      "andSisthenumberoftrainingsteps(ieparameterupdates",
      "kernel",
      "examplebycombininglimitedcallstothetrueobjectivefunctionwithfrequentcallstoanimperfect",
      "scribblingdowntheagent’sactions",
      "try",
      "bridges",
      "cluster",
      "arXivpreprintarXiv:1412.4446",
      "paradox",
      "nal",
      "tokencontext",
      "sensitivity",
      "5a",
      "exhibit",
      "keysandvalueshtimeswithdifferent",
      "enemy",
      "theShannon",
      "ism",
      "interpretation",
      "novel",
      "overarching",
      "costs",
      "reducedtoaconstantnumberofoperations",
      "mation",
      "cid:34)(cid:18",
      "series",
      "themes",
      "Thisideaaswrittenwouldnotquitework",
      "style",
      "intrapersonal",
      "convolutional",
      "ratherthandefinitivenumbers",
      "literalism",
      "concordance",
      "postmodernism",
      "exp(−R(z0",
      "union",
      "detachment",
      "organ",
      "usedbeamsearchwithabeamsizeof4andlengthpenaltyα",
      "horror",
      "novelcomplexbehaviors",
      "ambivalence",
      "Object",
      "growth",
      "attentionis0.9BLEUworsethanthebestsetting",
      "circles",
      "actuators",
      "vulnerabilities",
      "caveat",
      "paradigm",
      "theinputsequencecenteredaroundtherespectiveoutputposition",
      "definededness",
      "coach",
      "continuation",
      "PracticeofParallelProgramming",
      "purposes",
      "y.",
      "Inallofourexperiments",
      "wepresentedtheTransformer",
      "destruction",
      "supreme",
      "tasks",
      "face",
      "embeddings",
      "psychologists",
      "perity",
      "matrices",
      "sionality",
      "depthwise",
      "affordance",
      "naturalcourseoftheenvironment’sevolution",
      "faults",
      "thetruereward",
      "Transformation",
      "2015IEEEInternationalConferenceonComputerVision",
      "faith",
      "e",
      "conjunction",
      "wasprovidedbyanauthorduetotimeconstraints",
      "+",
      "weaddanentropybonusof0.01onalltasksexceptswimmer",
      "maximizesperformance",
      "usesanydata",
      "anthropology",
      "breadth",
      "life",
      "agents",
      "download",
      "cognitiveskillsandabilities",
      "View",
      "correspondstoasinusoid",
      "composer",
      "timeregionalverificationofstochasticnon",
      "lecture",
      "rules",
      "vances",
      "distress",
      "throughtheencoder",
      "between1.1and1.5timesthetrainingloss",
      "branches",
      "wheel",
      "whichperformsmulti",
      "Encouragement",
      "emissions",
      "explore",
      "cultures",
      "tendencies",
      "conatus",
      "learningratesandentropybonus",
      "Weregularizeallourmodelswith10%dropout",
      "presupposition",
      "text",
      "Sources",
      "programming",
      "conference",
      "presuppositions",
      "craft",
      "tacks",
      "flexibility",
      "subgoal",
      "assimilation",
      "Feed",
      "WecomparetheperformanceofstandardTransformerstorecurrentTransformers[DGV+18]inFigure17",
      "givenaproposedsequenceofactions",
      "89(1",
      "turns",
      "transformer",
      "mization",
      "sets",
      "aligns",
      "backondraftsofthiswork",
      "imaginary",
      "howshouldIcompareit",
      "variablez",
      "organization",
      "ofsimple",
      "o1,a1(cid:1",
      "dimensions",
      "url",
      "reinforcementlearningfordialogcontrol",
      "Transferimproveswithtestperformance",
      "issue",
      "anepochconsistsofonly40parameterupdates",
      "anatheism",
      "karma",
      "imbalance",
      "scrambleanegg",
      "opposite",
      "remain",
      "society",
      "fractions",
      "ner",
      "couragement",
      "span",
      "transcendence",
      "Frenchdatasetconsistingof36Msentencesandsplittokensintoa32000word",
      "adaptability",
      "4.8.1",
      "range",
      "self-",
      "apureRLsystem",
      "tionmodelsinvarioustasks",
      "necessarilydemonstrateit",
      "usingthreeprinciples",
      "Control",
      "Books",
      "queries(prompts)1",
      "denial",
      "trainedatanoptimalbatchsize",
      "intheformofshortmovie",
      "yˆvalues",
      "cid:0)(cid:0",
      "showhowper",
      "forexample",
      "CpN",
      "cloud",
      "correlationism",
      "Computer",
      "benchmarks",
      "Theempiricalfittedvaluesforthesetrendsare",
      "parts",
      "atheist",
      "interpolation",
      "toaverylargeone",
      "sources",
      "subagents",
      "proposition",
      "fingerprints",
      "worldview",
      ",aˇ[T",
      "includingcosttermsthatimplementsafetyguardrails",
      "physicstaskswithdozensofdegreesoffreedomandAtaritaskswithnohand",
      "musicality",
      "symbol",
      "lows",
      "integer",
      "Wetrainonqueriesonlygatheredatthebeginningoftraining",
      "taste",
      "thedesignermayknowthecorrectobjectivefunction",
      "sketch",
      "Cause",
      "butherewebeginbyillustratinghowitcouldbehave",
      "hood",
      "sand",
      "timal",
      "desire",
      "notethatfeedbackfromrealhumansdoeshavethepotentialtooutperform",
      "Kindness",
      "bleeding",
      "execution",
      "subgoals",
      "circuit",
      "withthestandard",
      "ca5pabilities",
      "dissolves",
      "aesthetic",
      "Figure",
      "theaveragedversion",
      "ontrainingtime",
      "moderation",
      "buttons",
      "conclusion",
      "modelsareunsupervisedmultitasklearners",
      "understands",
      "earlystoppingstep",
      "hypoth-",
      "apology",
      "Pairs",
      "Recurrentmodelstypicallyfactorcomputationalongthesymbolpositionsoftheinputandoutput",
      "prisinglywell",
      "type",
      "NeurIPS",
      "opening",
      "suggestingsomedirectionsthatmighthaveincreasingrelevanceasRLsystemsexpandinscopeand",
      "othermodules",
      "onarelativelymodestamountofdataandstoppingsignificantlybeforeconvergence",
      "mystics",
      "buildamodelthatconsistentlygeneralizestonoveldistributions",
      "producedusingMonteCarlomethods",
      "circuits",
      "resolution",
      "credit",
      "orsimplypourbleachdownthedraininordertogivetheappearanceofsuccess",
      "meaningthatmost",
      "industry",
      "theevaluationtimewaslinear",
      "andtheamountofcomputeC",
      "buty3isinanotherplotnexttothatplot",
      "tation",
      "transparency",
      "sofa",
      "objective",
      "11b",
      "toobtainthesameresultsusingsingleframeswewouldneedtohavecollectedsignificantlymore",
      "board",
      "TrainingatB",
      "thehopethatamodelwhichsimultaneouslyworkswellonmanytrainingdistributionswillalsowork",
      "ComparingtoLSTMsandUniversalTransformers",
      "acknowledgment",
      "system",
      "models",
      "observer",
      "attitudes",
      "measurements",
      "edges",
      "althoughinpracticethisis",
      "sizeddatasets",
      "rationality",
      "erases",
      "andsignificantlylesshelpfulperframe",
      "tocopewithdifferentalgorithmictradeoffs",
      "Machinelearning",
      "sincemostofthehistoricaldevelopmenthasbeenbythefieldofeconometrics",
      "wonder",
      "disruption",
      "performanceisastraightpowerlawinN.",
      "headedself",
      "feedback",
      "di-",
      "46–63",
      "conclusions",
      "produceoutputsofdimensiond",
      "theseclipsarebetween1and2secondslong",
      "ideal",
      "modelarchitecture",
      "ahumanandarobotinteractingwithanenvironmentwiththepurposeofmaximizingthehuman",
      "intel-",
      "Thingness",
      "learningandknowledgediscoveryindatabases",
      "hunger",
      "∞limit",
      "version",
      "beginnings",
      "experiment",
      "abs/1807.03819",
      "Security",
      "leadsouragenttoavoidlosingpointsbutnottoscorepoints;thiscanresultinextremelylongvolleys",
      "neglectedtotunesomehyperparameter(e.g.intializationscaleormomentum)thathaveanimportant",
      "cortex",
      "thousands",
      "boom",
      "cardinal",
      "Moderation",
      "widthratiosdeviatesignificantlyfromthetrend",
      "Dimensionality",
      "respect",
      "Wisdom",
      "animation",
      "andasufficientlysmallbatchsize(makingoptimal3useofcompute",
      "Thisledustofocusonpolicygradientmethods",
      "ytirojam",
      "Prediction",
      "log",
      "ReinforcementLearningTaskswithUnobservedRewards",
      "core",
      "NikiParmar∗",
      "asthemodellearnstobemoreunsure",
      "thenµputsallofitsmassonthatchoice",
      "souls",
      "account",
      "predicts",
      "metaphors",
      "s2[4",
      "participant",
      "paper",
      "tablet",
      "climate",
      "today",
      "options",
      "market",
      "approaches",
      "theoptimizationsothathighlevelandlow",
      "L(N)≈",
      "OptimalAllocationoftheComputeBudget",
      "7f",
      "deviation",
      "instinct",
      "Archetypes",
      "surface",
      "howevertheyconsidermuchsimplersettings",
      "Fixingmodelsize",
      "steps",
      "inspiration",
      "techne",
      "string",
      "doubles",
      "refutation",
      "mechanism",
      "13a",
      "pretraininghas5,000labelsratherthan5,500,andthe“target",
      "Anyways",
      "waves",
      "assignments",
      "excl",
      "distherepresentationdimension",
      "critiques",
      "y4isin",
      "riosity",
      "minimizesthenumberoftrainingsteps",
      "Onceagain",
      "vase",
      "exploring",
      "nut",
      "Competence",
      "times",
      "andstudyingitasafunctionofN",
      "PerformanceofouralgorithmonMuJoCotasksafterremovingvariouscomponents",
      "formula",
      "favor",
      "indriya",
      "reason",
      "rule",
      "arrow",
      "thereforebefarmoresampleefficientthanonemightexpectbasedontrainingsmallmodelstoconvergence",
      "Rapidprogressinmachinelearningandartificialintelligence(AI)hasbroughtincreasingatten-",
      "mul-",
      "16(3):307–354",
      "eachparametrizedbydeepneuralnetworks",
      "End",
      "definition",
      "puzzlement",
      "nihilism",
      "objectives",
      "factory",
      "Risk",
      "leadingtopredictablybadresults",
      "NIPS’12",
      "care",
      "12b",
      "predic-",
      "Reactive",
      "readyapproximations(forexamplelearning",
      "Inthecase",
      "asthepower",
      "Assumes",
      "lossonmodelsizeandtrainingtimeintheinfinitedatalimit",
      "z),∀z",
      "proximators",
      "useparameters",
      "phenomena",
      "papers",
      "mess",
      "betweenanytwopositionsinthenetwork",
      "TheauthorsbelievethatAItechnologiesarelikelytobeoverwhelminglybeneficialforhumanity",
      "castle",
      "takeoff",
      "probability",
      "DanielA.Roberts",
      "asmemoryconstraintslimitbatchingacrossexamples",
      "Fact",
      "extremes",
      "mama",
      "silence",
      "kisthekernel",
      "benefit",
      "Association",
      "Adafactor",
      "ban-",
      "outlet",
      "reaction",
      "mechanisms",
      "contrast",
      "instances",
      "confidence",
      "Angels",
      "biology",
      "fecundity",
      "overreactions",
      "Contextlength(1024formostruns",
      "schizophrenia",
      "realgoals",
      "fitwithsimplepower",
      "Ouralgorithmfitsarewardfunctiontothehu-",
      "cars",
      "conundrum",
      "steam",
      "setups",
      "size(i.e",
      "harms",
      "articles",
      "http://arxiv.org/abs/1907.04164",
      "indication",
      "947–957",
      "D(y",
      "pavement",
      "lawtrendsatlargevaluesofcompute",
      "te",
      "convolutionisequaltothecombinationofaself",
      "resonance",
      "past",
      "direction",
      "1c",
      "Weareexcitedaboutthefutureofattention",
      "wordpiece",
      "lying",
      "investigation",
      "N",
      "metaheuristics",
      "scholarlyworks",
      "forgetting",
      "partner",
      "entertainment",
      "=(",
      "augmentation",
      "Enc(x",
      "minimumsteps",
      "joy",
      "arXiv",
      "tofollowingpicture",
      "theotherpendulum",
      "Foolishness",
      "althoughitgets",
      "images",
      "lives",
      "effects",
      "minority",
      "Score",
      "andusingtemporalpoolingbetweenlevelstocoarse",
      "pairwise",
      "response",
      "efficienttrainingwilleventuallyrunintoaproblemwithoverfitting",
      "cessfullytospeechrecognition(Baevskietal",
      "20%increaseincomputebudget",
      "definitions",
      "expressions4",
      "transition",
      "asmallfractionofthetrainingcostsofthe",
      "educator",
      "subspaces",
      "stuff",
      "curiosity",
      "love",
      "Fairness",
      "entity",
      "privileging",
      "exploration",
      "thought",
      "ones",
      "relativism",
      "estimates",
      "supervisedlearningviaredundancyreduction",
      "slate",
      "Kekes",
      "continuousepisode",
      "Theshort",
      "demonstrations",
      "stranger",
      "Inthepresentwork",
      "ods",
      "credo",
      "form",
      "basics",
      "eye",
      "pressure",
      "foveation",
      "shootenemyships(good),anddon’tgetshot(verybad",
      "inconsistency",
      "fundamentalism",
      "Law",
      "satisfythesimplerelation",
      "rational",
      "that's",
      "framesoftrainingisdecreasedevery5∗106frames",
      "risks",
      "algos",
      "fine",
      "conflict",
      "irregulars",
      "overlap",
      "transfer",
      "Iwantthex",
      "Comparedtoallpriorwork",
      "education",
      "accuracy",
      "Webelievethatresearchonthesetopicshasbothurgencyandgreatpromise",
      "families",
      "deepneuralnetworksandtreesearch",
      "adoption",
      "rewardfunctions",
      "philosophers",
      "arXiv:1203.0683",
      "way",
      "points",
      "o1",
      "mechanics",
      "disjunction",
      "half",
      "triplets",
      "invariants",
      "end",
      "relevance",
      "bury",
      "tradition",
      "motivation",
      "ourresearch",
      "Episode",
      "bridge",
      "computationalcomplexity",
      "quotes",
      "Auto",
      "luJ",
      "andtrust",
      "overview",
      "describedinsection3.2",
      "testing",
      "Yarats",
      "foreclosure",
      "9a",
      "neologism",
      "successors",
      "procedurality",
      "periods",
      "Reachability",
      "imitation",
      "qualitypolicy",
      "Pred1(s",
      "ib",
      "0.76,andS",
      "moreness",
      "World",
      "vote",
      "WebText2",
      "Error",
      "glitches",
      "remembering",
      "dread",
      "read",
      "web",
      "purchases",
      "scholar",
      "WeusedtheAdamoptimizer[20]withβ",
      "orange",
      "variation",
      "Wecaninterpretarewardfunctionestimaterˆasapreference",
      "bandit",
      "cog",
      "renderings",
      "standpoint",
      "differences",
      "Inadditiontothetwo",
      "aircraft",
      "Phronesis",
      "T0.61",
      "bar",
      "L(C)=",
      "substance",
      "trials",
      "ofmagnitudemorehumantime",
      "prehension",
      "yieldsapower",
      "1010parameters",
      "Weseethatg(4)=7,sof(7)=f(g(4))=4",
      "decorum",
      "artresultsinsmall",
      "screen",
      "sociology",
      "percept",
      "Becauseweconsideredclipsratherthanindividualstates",
      "Maximallycompute",
      "weaveragedthelast20checkpoints",
      "reP",
      "Vinyals&Kaiserelal",
      "mustnot",
      "Weestimatethatthevariationinthelosswithdifferentrandomseedsisroughly0.02",
      "IC",
      "autonomoussystems",
      "Xlnet",
      "interface",
      "WeexplorefurtherGPT-4’scombinationskillsacrossmodalitiesanddisciplines",
      "Shouldwefearsupersmartrobots",
      "trend",
      "ormodelandtrainingsteps",
      "Reasoning",
      "atheism",
      "datasetsize",
      "power",
      "connectedfeed",
      "etymology",
      "arguments",
      "presentproposal",
      "weaknesses",
      "Symbols",
      "memories",
      "EPL(EurophysicsLetters),26(4):241,1994",
      "DonaldBRubin",
      "y)+log",
      "Lives",
      "Contents",
      "Learningfromdemonstrationsforrealworldreinforcementlearning",
      "element",
      "2x",
      "bination",
      "history",
      "destination",
      "volume",
      "autopoiesis",
      "bites",
      "dive",
      "References",
      "implications",
      "laws",
      "viewpoint",
      "bothwithinandoutsideofthemachinelearningcommunity",
      "connectedness",
      "disaster",
      "cause",
      "interpretationofhumancomparisons",
      "forms",
      "LarsPeterHansen",
      "machinetranslation",
      "trust",
      "functionwouldalsobereasonablechoices",
      "equation",
      "Batchsize(219formostruns",
      "appreciation",
      "Faith",
      "benefits",
      "advents",
      "choosequeriesatrandom",
      "decreasing",
      "predicates",
      "use",
      "inquiry",
      "man",
      "religion",
      "presence",
      "itshouldbespentprimarilyonlargermodels",
      "proceduralization",
      "Forthebigmodels",
      "ICLR",
      "interpolations",
      "subtask",
      "equilibration",
      "module",
      "user",
      "shortcomings",
      "LSTMandTransformerperformancecomparison",
      "parody",
      "occurs",
      "tokensandoutputtokenstovectorsofdimensiond",
      "universe",
      "varianceacrossensemblemembers",
      "thecomputebudget",
      "lectures",
      "24(4):456",
      "Sparsity",
      "describedinSection3.3",
      "ecstasy",
      "fordifferentlayertypes",
      "novelty",
      "indifferentways",
      "andthenfindtheminimum",
      "introspection",
      "dings",
      "sparseweights",
      "monthonGCE.Trainingtakesaboutaday",
      "translucency",
      "learning,3",
      "sword",
      "cid:88",
      "classifiers",
      "flourishing",
      "x",
      "this?—one",
      "ludits",
      "ethics",
      "reduction",
      "Inthesecondversion",
      "updates",
      "computerisation",
      "rˆusing",
      "bastard",
      "attention",
      "eviction",
      "valuesandqueries",
      "regime",
      "security",
      "Ifwe’reworriedinadvanceaboutthevase",
      "convolutionalneuralnetworks”",
      "theism",
      "marriage",
      "efficiency",
      "pages457–465,2016",
      "research",
      "endeavors",
      "Mode-1",
      "surprise",
      "months",
      "1904.10509",
      "resting",
      "candy",
      "performbetterandbemoresampleefficientthancurrentmodels",
      "strength",
      "Coding",
      "bracket",
      "personality",
      "proportion",
      "http://tasvideos.org/2513M.html",
      "Thisparticularfailuremodeisoftencalled“wire-",
      "reconstrual",
      "primate",
      "Thesemoduleshaveanimplicitscalaroutputthatcontributes",
      "adapting",
      "decades",
      "N.,Sandou",
      "opposition",
      "obligation",
      "fromtheauthors",
      "Warmup",
      "quantification",
      "visualization",
      "Energy",
      "IC(s",
      "describedinSectionSection3.3",
      "increase",
      "regionpolicyoptimization(TRPO;Schulmanetal",
      "values",
      "Thebatch",
      "6c",
      "13b",
      "wehaveacontradiction",
      "fears",
      "τ",
      "feature",
      "placement",
      "atB",
      "keys",
      "formulation",
      "Line",
      "thatshouldbeusedtomakepredictions",
      "overlords",
      "privacy",
      "meadow",
      "trainedwithpairswherexisadistortedorcorruptedversionofy",
      "verification",
      "salient",
      "thetiebreaker",
      "Multi-",
      "Distinction",
      "paragraphpromptedtextsamples[RWC+19",
      "phrase",
      "forewarning",
      "works",
      "aspiring",
      "multitask",
      "non",
      "études",
      "economy",
      "phone",
      "effectors",
      "thepredictionpathwayhasaccesstoyandcan“cheat”bycarryingthecompleteinformation",
      "theTransformerperformanceforlatertokens",
      "2.2.4",
      "Logicality",
      "learningmethodstolargedeeplearningsystems",
      "wefindacontradictionbetweenourequations",
      "ChainMonteCarlomethods",
      "unsure",
      "priors",
      "3c",
      "Howcanweefficientlyensurethatthecleaningrobotrespectsaspectsof",
      "replacingourearliercodebase",
      "particularization",
      "InmostmodernRLsystems",
      "tendency",
      "Westoppedtrainingoncethetestlossceasedtodecrease",
      "deconstruction",
      "s˜",
      "αB",
      "sizeindensityestimation[Was06]andinrandomforestmodels[Bia12]maybeconnectedwithourresults",
      "continuations",
      "thependulumshouldbepointingapproximatelyup",
      "Interactions",
      "groups",
      "math",
      "Springer",
      "office",
      "Parser[29]evenwhentrainingonlyontheWSJtrainingsetof40Ksentences",
      "decision",
      "bubble",
      "sizeofconvolutionsandrthesizeoftheneighborhoodinrestrictedself",
      "imaginal",
      "Thedetailsofthisschedulearedescribedineachsection",
      "chines",
      "P.-A.",
      "ofthesoftmaxwhichcorrespondtoillegalconnections",
      "ing",
      "relevant",
      "impacts",
      "ar-",
      "tems",
      "predictors",
      "fortheloss",
      "outtraining(noonlinequeries",
      "powergridorasearch",
      "Weusesymbolsthataresomewhatsimilartotherepresentationoffactorgraphs",
      "field",
      "ambition",
      "practice",
      "Power",
      "Image",
      "scale",
      "correspond",
      "Thereasonisthatitdependsnotonlyonthe",
      "children",
      "currentworkflows",
      "nhead",
      "considers",
      "group",
      "notch",
      "engineeredfeatures",
      "percepts",
      "metaphor",
      "hall",
      "Today",
      "andproducesanactiondirectlyfromtheoutput",
      "accumulate",
      "profits",
      "levellay-",
      "OptimalPerformanceandAllocations",
      "Beyondthestructuredoutputsetting",
      "mathemati-",
      "encoder(Vincent",
      "paircandidatesthanweultimatelypresenttothehuman",
      "fallibilism",
      "dialectic",
      "Oversight",
      "replacingtherecurrentlayersmostcommonlyusedinencoder",
      "association",
      "EBMs",
      "wedonotincludecontext",
      "thecomplexityofourenvironmentsforceustousedifferentRLalgorithmsandrewardmodels",
      "cid:20",
      "“Towardsai",
      "ourkeycontributionistoscalehumanfeedbackuptodeepreinforcement",
      "assumptions",
      "convention",
      "weightedsamplesallowustoestimate",
      "tationalchallengesindeeplearning",
      "trainingfor3.5daysoneightGPUs",
      "Whenweexcludeembeddingparameters",
      "oravailabledataD",
      "article",
      "variety",
      "torques",
      "theserewardfunctionsarenotgoodrepresentativesofhuman",
      "multi",
      "Significance",
      "feel",
      "trajectory",
      "communities",
      "Weselectpairsofsegments",
      "lookingapproachto",
      "center",
      "organizations",
      "toabout2×107",
      "report",
      "nition",
      "asnotedbysomecritics[38,85].Webelieveitisusuallymostproductivetoframeaccident",
      "InAdvancesinneuralinformationprocessingsystems",
      "TheTransformer",
      "r.",
      "4d",
      "participation",
      "1.1)and(1.2)thatgovernsthesimultaneousdependenceonN",
      "scaling",
      "||2",
      "sys-",
      "Acknowledgments",
      "asinglehiddenlayer",
      "entropyloss",
      "force",
      "morality",
      "thing",
      "class",
      "expander",
      "Change",
      "selecting",
      "yeses",
      "inputtensor",
      "Theworldmodelmaypredict",
      "ScalingLawswithModelSizeandTrainingTime",
      "thattheoptimalnumberofstepswillonlygrowveryslowlywithcompute",
      "variations",
      "forgiveness",
      "modeling",
      "thattheLSTMsperformaswellasTransformersfortokensappearingearlyinthecontext",
      "auto",
      "white",
      "max",
      "compute2",
      "mayemergefrompoordesignofreal",
      "nexus",
      "masking",
      "integers",
      "leftandrightselectclips",
      "TheAnnals",
      "approval",
      "preys",
      "logic",
      "analog",
      "pages433–440.ACL",
      "C.3",
      "playing",
      "summary",
      "⇒",
      "heuristics",
      "poetry",
      "scales",
      "Proceedingsofthe31stannualinternationalACMSIGIR",
      "scholarship",
      "future",
      "VisualizingRepresentations",
      "Aligningbooksandmovies",
      "ashasbeensuggestedforResNets[VWB16",
      "task",
      "oversight",
      "detection",
      "Fittedness",
      "distributionvalidationloss",
      "search",
      "egocentrism",
      "covariance",
      "syntax",
      "teaching",
      "argumentation",
      "competitions",
      "procedures",
      "abs/1606.04199,2016",
      "“UnanimousPredictionfor100LearningSemanticParsers",
      "Withoutaninformation",
      "cetera",
      "applications",
      "D.1",
      "trainingruns",
      "0,K",
      "Thisconceptcanbeappliedtosituations",
      "AD",
      "description",
      "Model",
      "tokensatconvergenceforthelargestmodelswecantrain",
      "figures",
      "seas",
      "et",
      "thegoaloftheagentis",
      "whichisafunctionofthetargetvalueoftheloss",
      "14a",
      "weusedasinglemodelobtainedbyaveragingthelast5checkpoints",
      "interpenetration",
      "dissimilarity",
      "solutions",
      "check",
      "positionsandvelocities",
      "modelbutalsoonthetargetvalueofthelosswewishtoachieve",
      "construct",
      "constitutive",
      "humanoversightfarenoughthatitcanbepracticallyappliedtostate",
      "pretext",
      "equivalent",
      "Rationality",
      "probabilty",
      "andtrainingtime",
      "product(multi-",
      "omission",
      "o2,a2(cid:1",
      "topic",
      "Empiricalevaluation",
      "Letter",
      "opacity",
      "sage",
      "inparametercountsarecompensatedforbyusingthefittoL(N)asabaseline",
      "outside",
      "lie",
      "forL(C",
      "wisebetweendevices",
      "horiz",
      "C",
      "∼1.7nats",
      "Aspiration",
      "Weapplydropout[33]totheoutputofeachsub",
      "having",
      "signatures",
      "theme",
      "thesis",
      "Generatingwikipediabysummarizinglongsequences",
      "directlytothetasksthemselvesinsteadofworryingaboutsideeffects",
      "plausibility",
      "axistobe",
      "gradient",
      "Availability",
      "k.",
      "submodules",
      "chemical",
      "Wemaybeabletoavoidtheneedforexploration",
      "constraint",
      "headsclearlylearntoperformdifferenttasks",
      "linguist",
      "reformation",
      "timation",
      "hacks",
      "rewardinwhichcasetheoracleoutputsindifference",
      "parameter",
      "ofshallowermodels",
      "obstacle",
      "attentionlayersarefasterthanrecurrentlayerswhenthesequence",
      "todistanceinpsychologicalspace",
      "inconsistencybetweendifferentcontractors",
      "productattention",
      "exploitation",
      "net-",
      "lungs",
      "allocations",
      "deployment",
      "ethnocentrism",
      "termplanningcan",
      "attempt",
      "material",
      "S1",
      "actor",
      "Essentialism",
      "ages",
      "probabilitythat",
      "Reuse",
      "causedbybadextrapolationsfromlimitedsamples",
      "graphs",
      "base",
      "playroom",
      "gradients",
      "wemustinjectsomeinformationabouttherelativeorabsolutepositionofthe",
      "teacher",
      "foreground",
      "sentencepairs",
      "individualism",
      "z",
      "termoutcomeswithminimal",
      "map",
      "dings),thesizeofthedatasetD",
      "constraints",
      "regularizers",
      "noun",
      "open",
      "empiricist",
      "panels",
      "andJanPeters",
      "distributions",
      "encircles",
      "InInternationalConferenceonIntelligentRobotsandSystems",
      "dimension",
      "Examples",
      "couple",
      "neuroscience",
      "channel",
      "scene",
      "16parallelworkers",
      "penalty",
      "man-",
      "sandbox",
      "Ifwespecializetoprediction",
      "voice",
      "passivity",
      "coffee",
      "debate",
      "misrepresents",
      "abuse",
      "I.,Pouget",
      "angel",
      "failures",
      "a.",
      "siht",
      "PhDthesis",
      "1.4B",
      "plateaus",
      "InC.Cortes",
      "tsunami",
      "canprovideaveryusefulapproximationtotherewardfunction",
      "≈6NBS",
      "eziS",
      "discrete",
      "sheep",
      "gazelle",
      "mayevenbeconsistentwithanexponentofzero",
      "gnissim",
      "intersection",
      "InFutureofInteractiveLearningMachines",
      "subspacesatdifferentpositions",
      "intentionality",
      "property",
      "1a",
      "specialization",
      "wealth",
      "discipline",
      "subsidiary",
      "collapse",
      "extraction",
      "rectangles",
      "ecnis",
      "withdrawal",
      "redeployment",
      "controversy",
      "predicate",
      "minuteintervals",
      "setup",
      "inreachabilityanalysis",
      "1h",
      "rewardpairs",
      "world",
      "Learning",
      "pairs",
      "diamond",
      "consumption",
      "cost",
      "Thisis",
      "surpasses",
      "wewilldescribetheTransformer",
      "r",
      "breakthrough",
      "glass",
      "providelow",
      "courses",
      "relativists",
      "specifiedmodels",
      "picture",
      "systemsthatrequirehundredsorthousandsofhoursofexperience",
      "2017IEEEConferenceonComputerVisionandPatternRecognition",
      "artLLM",
      "cells",
      "debates",
      "site",
      "overflow",
      "Search",
      "predecessors",
      "similarto[30",
      "Rewards",
      "prophets",
      "amplifier",
      "allwecandoisqualitativelyevaluatehowwelltheagentsatisfiestothehuman",
      "Hero",
      "withnearlyidenticalpower",
      "absence",
      "commandments",
      "rewardfunction",
      "generalization",
      "LM.tats",
      "subtitle",
      "update",
      "liberation",
      "finder",
      "human",
      "Wedemonstrate",
      "exposition",
      "Fallacy",
      "increaseparallelismthroughlargerbatchsizes",
      "whenperformanceislimitedbyonlyeitherthenumberofnon",
      "O(k·n·d2",
      "normofz",
      "artmodelsinmachinetranslations",
      "perspectival",
      "cal",
      "eofthedataisheldouttobeusedasavalidationsetforeachpredictor",
      "graintherepresentation",
      "andAndrewMcCallum",
      "masterpiece",
      "recommendation",
      "trains",
      "covariateshiftandmarginallikelihood",
      "motivations",
      "airplane",
      "thumb",
      "choice",
      "13%decreaseinloss",
      "andrequesttoseethereward",
      "fruits",
      "stat",
      "criterion",
      "opt",
      "theories",
      "intervals",
      "ambiguity",
      "asdescribedinSection3.2",
      "5b",
      "segmentation",
      "measures",
      "engineers",
      "hardware",
      "Weusedbeamsearchasdescribedintheprevioussection",
      "Prompt",
      "insight",
      "panel",
      "AnOpenLetterfromAI&RoboticsResearchers",
      "lawinT.",
      "nature",
      "ortheunevenrateoflabelingbycontractors",
      "Enc1(x",
      "∼8.8×1013(non",
      "baseline",
      "roles",
      "egoic",
      "sentiment",
      "theist",
      "togetherness",
      "suchasassessingwhetheraconceptualgoalhasbeenmet",
      "2013)[40",
      "Thismaybeduetohumanerrorinlabeling",
      "qualifying",
      "Robotics",
      "birds",
      "dependenthumanfeedback",
      "experience",
      "theirtestlossonavarietyofotherdatasets",
      "function",
      "Truth",
      "cf7d5375197fwrittenbyoneoftheauthorsofthepresentdocument",
      "OurmethodhasmoredifficultymatchingRLinthesechallengingenvironments",
      "mind",
      "answer",
      "SNESSuperMarioWorld(USA)“arbitrarycodeexecution”",
      "indwelling",
      "elation",
      "Thepredictorpartoftheworldmodelmustbeabletoperformawiderangeoffunctions",
      "whichwedefineasthenumberofnon",
      "ti",
      "interest",
      "criteria",
      "aptitudes",
      "ontotheology",
      "andvideomodels",
      "truth",
      "rejection",
      "Z",
      "overlyconcentratedinnarrowpartsofstatespace",
      "attentionheads",
      "Ourexperimentstakeplaceintwodomains",
      "walk",
      "RecurrentNeuralNetworkGrammar[8",
      "vacuum",
      "filter",
      "butneverthelessit",
      "layersorwithextremedepth",
      "hell",
      "Despitetheseissues",
      "brake",
      "girl",
      "Signatureverification",
      "compassion",
      "pocket",
      "null",
      "bidirectionaltransformersforlanguageunderstanding,2018,arXiv:1810.04805",
      "muscle",
      "Divergence",
      "andIlyaSutskeverfordiscussionsandforfeed-",
      "FRSC",
      "0tofindtheconditionforoptimality",
      "orderofthesequence",
      "itworks",
      "exposedness",
      "orders",
      "freepositionrepresentationandbecametheotherpersoninvolvedinnearlyevery",
      "difficulty",
      "variableswithmean0andvariance1.Thentheirdotproduct",
      "seemidealgivencurrentinformation",
      "perplexitiesareper",
      "operation",
      "ex",
      "inside",
      "Whoops",
      "languageisnon",
      "Sentenceswereencodedusingbyte",
      "D)shouldbeanalyticatD",
      "sen-",
      "anditsperformanceonawidespectrumoftasksatorbeyondhuman",
      "character",
      "Equation(1.1)and(1.2)togethersuggestthatasweincreasethemodelsize",
      "concretethreat",
      "Notonlydoindividualattention",
      "learnwithsyntheticlabels",
      "scalestolargeproblems",
      "inkling",
      "duringepisodeswherethehumantrainerprovidesfeedback",
      "requirement",
      "inputs",
      "diagrams",
      "obstacles",
      "games",
      "dot",
      "rectlyvisibletoahumansupervisor",
      "embeddingparameters",
      "aporia",
      "Thatis",
      "D)fitsthedatawell",
      "disciple",
      "language",
      "Therewillbealotofties",
      "Wealsomodifytheself",
      "nentdominance",
      "bride",
      "bias",
      "tarmac",
      "modernity",
      "revolution",
      "p",
      "atall",
      "legislation",
      "speaker",
      "pointing",
      "rateoverthecourseoftraining",
      "operationalization",
      "discriminative",
      "forgrantedthatconstructingafullywell",
      "experts",
      "costactionsequence",
      "drug",
      "materials",
      "proprioception",
      "partiallyobservableenvironment",
      "rebirth",
      "22B",
      "wisdom",
      "emeritus",
      "assignment",
      "independence",
      "preferredfromeachpair",
      "T",
      "lights",
      "canary",
      "functioning",
      "Heretheoriginalintentoftheobjectivefunction(touseclickstoassesswhich",
      "practitioners",
      "correctionsandinspiration",
      "statements",
      "Section5",
      "propagation",
      "pragmatism",
      "successes",
      "approachesthatattempttotransferalearneddynamicsmodelbutnotthevalue",
      "starting",
      "effector",
      "ancients",
      "thespeedometer",
      "perception",
      "Generalizationtoothertestdatasets",
      "ZhanglongJi",
      "questing",
      "texts",
      "billiards",
      "competence",
      "sophrosyne",
      "blurriness",
      "accumulation",
      "-dimensional",
      "cues",
      "thatcountsasmaximumempowermenteveniftheimpact",
      "It’snotclearhowtoconstructasuitablerewardfunction",
      "mythology",
      "understanding",
      "anEBMmayneedthehelpofalatentvariable",
      "artmodel",
      "focus",
      "chitecture",
      "efficacyofoptimization",
      "L1",
      "thinkingtheunthinkable”",
      "D)dependspredictablyonthedatasetsizeDandmodelsizeN",
      "logicality",
      "BS",
      "vocabulary",
      "closing",
      "andwouldbeprohibitivelyexpensiveevenforthesimplesttasksweconsider",
      "encoderself",
      "encoding",
      "tandem",
      "section",
      "missions",
      "theagentcanonlyeverconfirmthroughimperfectperceptions",
      "fixes",
      "gon",
      "adversary",
      "longshort",
      "learning",
      "eachtrainedon|D|triplessampledfromDwithreplace-",
      "strengths",
      "choices",
      "shape",
      "plottedinFigure13,alongwithapower",
      "ozone",
      "andthedataavailableforthistrainingprocess",
      "comparisons",
      "Prophets",
      "influences",
      "internalizes",
      "functionwithoutactuallysatisfyingourpreferences",
      "Criticalityinformallanguagesandstatisticalphysics",
      "thepredictionsof[MKAT18]continuetoholdforTransformerlanguagemodels",
      "resource",
      "simulator",
      "Similartotheencoder",
      "5c",
      "leaper",
      "Thesenetworksareupdatedbythreeprocesses",
      "drink",
      "architecture",
      "mercy",
      "Wenormalizedtherewardsproducedbyrˆtohavezeromeanandconstantstandarddeviation",
      "theobjectivefunction",
      "founder",
      "wherethequery",
      "unconscious",
      "preferences",
      "textualentailmentandlearningtask",
      "forwardnetwork",
      "process",
      "loving",
      "transpatial",
      "RichardSSuttonandAndrewGBarto",
      "grocery",
      "heading",
      "orwillbepushedintothepredictor",
      "cashier",
      "dataset",
      "cracks",
      "efficiently6",
      "comprehension",
      "Tomorrow",
      "android",
      "Withoutempiricalconfirmation",
      "analyst",
      "backgrounds",
      "theology",
      "validity",
      "school",
      "fʁɔm",
      "descriptions",
      "L",
      "hinge",
      "ConferenceonLearningRepresentations,2017",
      "opponents",
      "brackets",
      "reconstruction",
      "speed",
      "Participation",
      "loss(rˆ)=−",
      "start",
      "andovertwo",
      "outcome",
      "Laws",
      "C.1",
      "theorem",
      "ralba",
      "researcherstrainmodelsuntiltheyappeartobeclosetoconvergence",
      "combination",
      "tool",
      "Thesehyperparameters",
      "aspiration",
      "abyss",
      "availabledatausingamortizedinference(e.g",
      "calculation",
      "info",
      "headattentioninthreedifferentways",
      "counts",
      "generatedreward",
      "motif",
      "recurrentnets",
      "addict",
      "minute",
      "briefer",
      "timesteps",
      "Figure12",
      "story",
      "sequentialnatureprecludesparallelizationwithintrainingexamples",
      "abilities",
      "claim",
      "graduate",
      "working",
      "hole",
      "maybes",
      "adults",
      "thedesignermayhavespecifiedthewrongformalobjectivefunction",
      "implicature",
      "contradictions",
      "centerpiece",
      "computerscientists",
      "width",
      "andonlydependsonthelossL.",
      "reflection",
      "growingrelativelymodestlybyonly2x",
      "trait",
      "ResearchInstitutehasstudiedsafetyissuesthatmayariseinveryadvancedAI[57,56,36,154",
      "researchenvironmentandforsupportingandencouragingthiscollaboration",
      "explainability",
      "whereX",
      "vectors",
      "writings",
      "fulfillment",
      "andwewilldemonstratethatthesemethodsagree",
      "sections",
      "dataexamples",
      "havenofundamentalmeaning",
      "vocabulary[38].Sentencepairswerebatchedtogetherbyapproximatesequencelength",
      "tokenperformancevariesforafixedmodelasafunctionofthetrainingstep",
      "MediocrefitsatsmallSareunsurprising",
      "by1400labelsouralgorithmperformsslightlybetterthanifithadsimplybeengiven",
      "swaths",
      "whichalsomatchesthescalingofFigure14towithinafewpercent",
      "fects",
      "Youcantreatitaspurelyaverageplay",
      "finding",
      "pursuit",
      "evenifourmodelfamilyconsistsofallTuringmachines",
      "intheserviceofcomplexhumanvaluesratherthanlow",
      "labels",
      "andsoisamovingtarget",
      "buildsagenerativemodelofthedistribution",
      "corruption",
      "book",
      "apartment",
      "mindfulness",
      "classifier",
      "imposition",
      "trip",
      "sidewalk",
      "timescale",
      "issues",
      "Onlinehumantrainingofamyoelectricprosthesiscontrollerviaactor",
      "lukaszkaiser@google.com",
      "ignorance",
      "Table",
      "specifications",
      "intervention",
      "predictive",
      "epistemology",
      "organism",
      "resources",
      "biases",
      "Buttherealworldcanbemuchlessforgiving",
      "deliberation",
      "instantiation",
      "permanence",
      "νόημα",
      "Retrying",
      "Redrectanglesrepresentenergyterms",
      "iors",
      "expertise",
      "computer",
      "characteristics",
      "efficientfrontierforanentiretrainingrun",
      "view",
      "S",
      "motives",
      "oftheotherdirections",
      "fittedness",
      "ior",
      "underlines",
      "literacy",
      "expertusers",
      "caring",
      "marker",
      "judgements",
      "night",
      "advertising",
      "thedotproductsgrowlargeinmagnitude",
      "statistics",
      "let’sdeterminehowtheoptimallossdependsonthecomputebudget",
      "matics",
      "competences",
      "kernels",
      "tations",
      "tationraisedby[95",
      "pneuma",
      "comparison",
      "heroes",
      "modalities",
      "creativity",
      "relaxation",
      "condemnation",
      "flows",
      "co",
      "somewhatalteredpowerlawforL(C",
      "decisions",
      "pretends",
      "Theshorterthesepathsbetweenanycombinationofpositionsintheinput",
      "chemistry",
      "limitation",
      "advances",
      "data",
      "support",
      "altogetherifweinsteaduseinverseRLorapprenticeshiplearning",
      "awe",
      "ad",
      "produceprobabilityestimates",
      "invasion",
      "hurdle",
      "Train",
      "Thehighceilingandlowfloorforperformanceonlanguage",
      "row",
      "level",
      "domocide",
      "canbeanyof",
      "drunkards",
      "accuracies",
      "/4",
      "multiply",
      "contact",
      "likethebufferoverflowexample",
      "Delusion",
      "TikZ",
      "carpet",
      "modelenginesomewhereintheirprefrontalcortex",
      "andharmfulbehaviorthatmayemergefrompoordesignofreal",
      "wiki",
      "acts",
      "headattentionallowsthemodeltojointlyattendtoinformationfromdifferentrepresentation",
      "locomotion",
      "piece",
      "givenanxtheenergylandscape",
      "forecasting",
      "isolation",
      "appropriation",
      "Thisallowsevery",
      "withtheexceptionoftherunswherethedatasethasbeenreducedbyafactorof",
      "extrapolation",
      "paradigms",
      "ByteNet[18",
      "checkpointaveraging",
      "suite",
      "αN",
      "JEPA-1",
      "courage",
      "trainingepisodes",
      "intelligence",
      "mythos",
      "ToevaluateiftheTransformercangeneralizetoothertasksweperformedexperimentsonEnglish",
      "forwardlayer",
      "allegory",
      "fromtwodifferentheadsfromtheencoderself",
      "usingthismoreflexiblearchitectureallowsustoimmediatelygeneralizetotasksforwhichtherewardfunction",
      "minimum",
      "performancewithfeweroptimizationsteps(Figure2)andusingfewerdatapoints(Figure4",
      "supposethatwewantedtousereinforcementlearningtotrainarobottocleanatableor",
      "continuity",
      "coefficients",
      "square)—so",
      "concepts",
      "network",
      "withanovelacquisitionfunction",
      "headattentionwithfulldimensionality",
      "pragmatics",
      "http://arxiv.org/abs/",
      "stacking",
      "ToolsandAlgorithmsfortheConstructionandAnalysis",
      "micro",
      "reasonforgoingbeyondtraditionalbenchmarksisprobablymoresignificant",
      "abstractions",
      "integration",
      "Privacy",
      "commonsense",
      "theorists",
      "transformative",
      "senses",
      "prejudices",
      "scheme",
      "Deepface",
      "Superintelligence",
      "ments",
      "toextractfurtherresults",
      "innovation",
      "wediscussinthisdocument.[55,118]providethoroughreviewsofthisliterature",
      "supervisedlearningofspeechrepresentations",
      "beha",
      "itmightbepossibletoremainonthecompute",
      "labeling",
      "watchingmoviesandreadingbooks",
      "Llionalsoexperimentedwithnovelmodelvariants",
      "Challenges",
      "magic",
      "Efficienttrainingofgiantneuralnetworksusingpipelineparallelism",
      "Operations",
      "nothingness",
      "yˆ)+µ||y−yˆ||2",
      "“Surlesapplicationsdelath´eoriedesprobabilit´esauxexperiencesagricoles",
      "canlikelybeexplainedusinganoisyquadraticmodel",
      "withsubwordunits",
      "product",
      "psychopathology",
      "equilibrium",
      "theentiretime",
      "Architectures",
      "DiederikKingmaandJimmyBa",
      "furniture",
      "neededbecausehumanratershaveaconstantprobabilityofmakinganerror",
      "notexperimentwithhigherlearningratesfortrainingrunsthatdidnotproceedtoconvergence",
      "we're",
      "observation",
      "remedy",
      "L(N,∞",
      "hierarchy",
      "configurable",
      "thermodynamics",
      "uncertainty",
      "pedals",
      "adver-",
      "pointers",
      "costs~$700",
      "experthuman",
      "fromFigures18and10forallourlateranalyses",
      "inference",
      "etal",
      "7a",
      "thatpredictingcomparisonsworkedmuchbetterthanpredictingscores",
      "appendix",
      "existentialists",
      "session",
      "12a",
      "quote",
      "paths",
      "darin",
      "Ifwecouldsuccessfullycommunicateouractualobjectivestoouragents",
      "whichcouldpotentiallyconfoundourattemptstolearnfromhuman",
      "thinker",
      "consciouness",
      "argmin",
      "proximity",
      "expectations",
      "fact",
      "ProceedingsDESIGN2006,the9thInternationalDesignConference",
      "themtonavigate",
      "butputasidethingsthatmight",
      "arXiv,2017,1710.03667",
      "Tothisend",
      "canberepresentedasalinearfunctionof",
      "indebtedness",
      "C0.73",
      "gravity",
      "fantasy",
      "perceptions",
      "interpretations",
      "sinceweexpecttobeabletoexpandthelossabouttheD",
      "notice",
      "yˆare",
      "contours",
      "includingtheamygdala",
      "vicinity",
      "Themodelbegins",
      "stochastic",
      "lapse",
      "Xception",
      "basedreinforcement",
      "socket",
      "Preference",
      "theyareunabletocomparethetwosegments",
      "misquotation",
      "Experiments",
      "books",
      "disagreement",
      "persons",
      "aspects",
      "answers",
      "internet",
      "inklings",
      "selves",
      "cid:38)(5×103)N0.74",
      "withanyoftheideasabove",
      "set",
      "multitude",
      "byover2BLEU.OntheWMT2014English",
      "translationsystem",
      "finite",
      "predictionforB",
      "generalizationinneuralnetworks",
      "Procedings",
      "sensors",
      "Oneepochisallyouneed,2019,arXiv:1906.06669",
      "fame",
      "steel",
      "psychiatry",
      "horizons",
      "withrealhumanfeedback",
      "Parameters",
      "6f",
      "houses",
      "wherewehaveusedthecompute",
      "permutation",
      "pitfalls",
      "stock",
      "arXiv:2103.01950",
      "clearings",
      "need",
      "knowing",
      "siblyneedtobelearnedbymodelslikeneuralnetworks",
      "sisters",
      "MachineCommonSense",
      "guidance",
      "expressionism",
      "party",
      "Representation",
      "abs/1904.10509",
      "Policy",
      "lies",
      "Interaction",
      "generative",
      "Thing",
      "control",
      "sharing",
      "availableInternetBooks",
      "asymmetry",
      "andoftencomparablytosyntheticfeedbackthathas40",
      "forwarddimension",
      "performanceofimagemodels",
      "touch",
      "priest",
      "a[3",
      "quo",
      "islow",
      "textdata",
      "αmin",
      "culture",
      "Ekstasis",
      "nurse",
      "squares",
      "orask",
      "output",
      "cid:17)2",
      "country",
      "pattern",
      "Aandarewardfunctionestimate",
      "AshishVaswani∗",
      "approach",
      "lack",
      "altitude",
      "Google’sneuralmachine",
      "weemployedlabelsmoothingofvalueϵ",
      "century",
      "spirits",
      "rewards",
      "player",
      "scaleworldstateprediction",
      "anestimateofthetotalnon",
      "theloss",
      "possibility",
      "appraisal",
      "unit",
      "econometrics",
      "ads",
      "disclosure",
      "personhood",
      "questioniswhattodooncethedetectionoccurs",
      "Advances",
      "mitigatingthesecausesmaybeavaluablecontributiontosafety",
      "chance",
      "O(n2·d",
      "memory",
      "epitome",
      "InFigure6wedisplaytheperformanceofawidevarietyofmodels",
      "stores",
      "community",
      "gnikam",
      "neuropsychiatrist",
      "track",
      "preprint",
      "game",
      "company",
      "broom",
      "circumstances",
      "reception",
      "hangover",
      "edge",
      "video",
      "importance",
      "thanitneeds",
      "analogues",
      "sefirot",
      "sensitivities",
      "necessity",
      "Sub",
      "whichoftwotrajectorysegmentsisbetter",
      "danger",
      "andseveralaspectsofthesafetyproblem",
      "normalization",
      "species",
      "termimplicationsofpowerfulAI[27,167",
      "aggregate",
      "discussions",
      "parent",
      "bollywood",
      "sixties",
      "denoising",
      "compatibility",
      "date",
      "Weincludealltrainingstepsafterthewarmupperiodofthelearningrateschedule",
      "discretization",
      "starts",
      "comments",
      "paradigminwhichalearningsystemistrainedto“fillintheblanks",
      "diverging",
      "efficienttraininginvolvestrainingverylargemodels",
      "prescient",
      "plane",
      "ocean",
      "guilt",
      "whentheagentqueriesforacomparison",
      "R",
      "forgoodactions",
      "questions",
      "TC2(s",
      "area",
      "poesis",
      "people",
      "Itcombines",
      "inthedatasetsize",
      "worldAIsystems",
      "experiments",
      "nails",
      "4c",
      "Theagentistrainedtoperformasequenceofbackflips",
      "efficientsizerequirefewerstepstotrain",
      "Thisframeworkconsidersatwo",
      "ecology",
      "fellowship",
      "classes",
      "andkeys",
      "lawscalingofwidthasafunctionofdepth",
      "source",
      "basis",
      "dent",
      "suchthatoptimallycompute",
      "floors",
      "Sampleefficiency",
      "floor",
      "corner",
      "frame",
      "episode",
      "spouse",
      "landinguprighteachtime",
      "offset",
      "effortshavesincecontinuedtopushtheboundariesofrecurrentlanguagemodelsandencoder",
      "wecanfindf(9)bylookingforthevalueofxsuch",
      "thedatasetsizeintokens",
      "region",
      "corpora",
      "presentation",
      "B",
      "Safety",
      "atoms",
      "ctx",
      "implementation",
      "thenumberofmodelparameters",
      "variables",
      "behavior",
      "viewpoints",
      "resistance",
      "Parallax",
      "thenextstepmightbetomovetorealenvironments",
      "cellphones",
      "PMLR",
      "shouldnotbetrustedforverylargemodels",
      "inthecontext",
      "deconstructionism",
      "inversereinforcementlearning",
      "Ontology",
      "late-1990s",
      "success",
      "Delays",
      "hum",
      "fault",
      "limit",
      "isaplausiblecontinuationofthevideoclipx",
      "technique",
      "branching",
      "Thismaybeaconsequenceofunderlyingpower",
      "authorshavepersonallyobservedthisintrainingautomaticspeechrecognitionsystems",
      "rejoining",
      "test",
      "wepushthepossiblestochasticity",
      "transjectivity",
      "inefficiency",
      "arXiv:1606.03976",
      "ple",
      "queries",
      "forests",
      "competition",
      "wecouldinsteadconsiderrewardfunctionsthatdependonthewholesequence",
      "collection",
      "matters",
      "abs/1804.04235",
      "includingthebottle",
      "Don’tworryabouthowtheagentgotintothesituationitisin(forinstance",
      "realists",
      "improvementsinlanguagemodellossmayhideseeminglyqualitativechangesincapability",
      "constraintofsequentialcomputation",
      "Account",
      "gap",
      "manipulation",
      "results",
      "likelihoodfunction”",
      "Performance",
      "brainstorm",
      "Itallowsthemtoproduce",
      "existentialism",
      "general",
      "men",
      "Awakening",
      "infinitude",
      "operations",
      "0.6x",
      "degree",
      "andwenormalizeittohaveastandarddeviationof0.05(wecouldequivalentlyhaveadjustedour",
      "unpredictability",
      "sizewhilevaryingasinglehyperparameter",
      "elegance",
      "maxims",
      "theeffectors",
      "mode",
      "vqvae",
      "question",
      "backwardpasses",
      "understandings",
      "probabilities",
      "discs",
      "transgression",
      "iterations",
      "usesmoothinterpolation",
      "illusions",
      "problem",
      "slots",
      "thecomputingpowerusedtotrainthem",
      "andaskGPT-4towriteaproofofthesametheoremin",
      "and[BIK22]forGPT-3.5oninlaw",
      "positions",
      "achievement",
      "thegreendotontherobotarmshouldbeascloseaspossibletothereddot",
      "1Hereweassumeherethattherewardisafunctionoftheobservationandaction",
      "transformers",
      "chaos",
      "demand",
      "L(N),isfitbetterbyapowerlaw",
      "prophet",
      "service",
      "mindware",
      "magnitude",
      "predictorifweviewrˆasalatentfactor",
      "distraction",
      "comeback",
      "Thisfitsintoarecenttrendofscalingreward",
      "communication",
      "types",
      "pixels",
      "carburetor",
      "lot",
      "lava",
      "motor",
      "mathematician",
      "superintelligent",
      "pair[31]representations",
      "sensor",
      "lawcorrelationsinlanguage[EP94,ACDE12",
      "factor",
      "consistency",
      "g.",
      "andoutputareallvectors",
      "partition",
      "depths",
      "instantiations",
      "preexists",
      "MichaelLLittman",
      "tomaximizethesumofthepredictedrewardsr",
      "quadcopter",
      "thenµisuniform",
      "disconnectedness",
      "kids",
      "cardinality",
      "Biases",
      "Wepresentedfivepossibleresearchproblems",
      "efficientinferenceandvisualizations",
      "means",
      "teleology",
      "Inourexperiments",
      "ontology",
      "regularizer",
      "cognition",
      "Margin",
      "representation",
      "objectivity",
      "provision",
      "arc",
      "sensitivetothepreciseexponentsfromourpower",
      "0.98andϵ=10−9",
      "linearlyinmodelsizewhileavoidingoverfitting",
      "rates",
      "characterization",
      "itispossibletoreducetheinteraction",
      "Weakdependenceofperformanceonhyperparametertuning",
      "comp",
      "andtheideaofpredictivecodinghasbeenaprominentconceptinneuroscience",
      "O(n",
      "accordingtotheformula",
      "predictor",
      "psychometrician",
      "criticisms",
      "likelihood",
      "failureistheextremelybrittleimplicitcontractinmostmachinelearningsystems",
      "diagnostics",
      "reinforcementlearning",
      "agenda",
      "handlepredictionatmultipletimescalesandmultiplelevelsofabstraction",
      "abs/1605.07157",
      "signs",
      "title",
      "∈Z",
      "patient",
      "exponents",
      "brain",
      "encountering",
      "explosion",
      "fortitude",
      "fromcomparisonsoftrajectorysegments",
      "withaproperallocationschemeofnewkey",
      "internalization",
      "path",
      "issimilartothatofsingle",
      "rendering",
      "allowsagentstobetaughtbynon",
      "basedsolelyonattentionmechanisms",
      "critique",
      "371–421",
      "Predictor",
      "diagram",
      "etal.(2011),Pilarskietal.(2011),Akrouretal.(2012),Wilsonetal.(2012),Sugiyamaetal.(2012",
      "Depth",
      "branch",
      "Fig",
      "neglect",
      "Criticism",
      "time",
      "cross",
      "deepdive",
      "Wonder",
      "coupling",
      "mightbeappropriateforthissortofexperiment",
      "totalrewardoveratrajectorysegment",
      "mother",
      "Ifwecansuccessfullyregularizeagentsin",
      "marking",
      "interruption",
      "abstractivesummarization",
      "numberofself",
      "likelihoodforothergenerativemodelingtasks",
      "valuepairstoanoutput",
      "right",
      "Process",
      "dollars",
      "habits",
      "=",
      "networksformachine",
      "compu-",
      "yesterday",
      "comparelongerclipsbecausetheyprovidemorecontextthansingleframes",
      "neuroscientist",
      "1e",
      "numberoflayers",
      "Language",
      "leadingto",
      "Activereinforcementlearning",
      "andmodelthisrewardfunctionwitharecurrentneuralnetwork",
      "psychotechnologies",
      "noesis",
      "activity",
      "Thedatastronglysuggeststhatsample",
      "attentionlayerandapoint",
      "seventies",
      "change",
      "motivatecuriosityandexploration",
      "controllers",
      "termmemory",
      "listoffivepracticalresearchproblemsrelatedtoaccidentrisk",
      "ulation",
      "margin",
      "isthebatchsize",
      "Here’saguideonwhatconstitutesgoodandbadbehaviorineachspecificdomain",
      "animal",
      "realness",
      "Foroptimallycompute",
      "andonlylearnslonger",
      "Husserl",
      "neologisms",
      "Transformerarchitecture[VSP+17,LSP+18",
      "mis",
      "op",
      "morning",
      "overlaps",
      "onearlytokens",
      "Transframing",
      "configurations",
      "Interpretation",
      "anddatarequirementswhentraininglargelanguagemodels",
      "millions",
      "evaluation",
      "disponibilité",
      "bestmodelsfromtheliterature",
      "acrosslanguages",
      "objects",
      "disagreement(randomqueries",
      "h",
      "Text",
      "andisthespecializationoftheLuce",
      "accident",
      "Development",
      "Therearemanychoicesofpositionalencodings",
      "experimentsinvolvingreinforcementlearningfromactualhumanfeedback",
      "of-",
      "BydefinitionC",
      "interference",
      "robotandacleaningrobotprobablywanttoavoidknockingoverfurniture",
      "Nazis",
      "singlescalar",
      "original",
      "withevenasmalldecreaseinthisdistance",
      "authenticity",
      "J.D.,andGriffiths",
      "forerunner",
      "criticalbatchsize",
      "faculty",
      "barriers",
      "learningandtolearnmuchmorecomplexbehaviors",
      "wedding",
      "prey",
      "beyondness",
      "counterpart",
      "12d",
      "airport",
      "36,1280)onfixedsubsetsoftheWebText2",
      "conversation",
      "beer",
      "manner",
      "store",
      "prototypes",
      "schedule",
      "lowerbound",
      "scalemachinelearningonheterogeneousdistributedsystems",
      "child",
      "domains",
      "sum",
      "PC",
      "outcomes",
      "distribution",
      "modal",
      "L(N",
      "sports",
      "Thiswilldrivezˇtobesparse",
      "beings",
      "N(y",
      "lilypads",
      "positionalembeddinginsteadofsinusoids",
      "relationsandphysics",
      "Architectures(JEPA)whichcanseenasacombinationoftheJointEmbeddingArchitecture",
      "review",
      "burst",
      "capacity",
      "section5.4),learningratesandbeamsizeontheSection22developmentset",
      "trilogy",
      "rater",
      "constant",
      "salvation",
      "w∗",
      "bolts",
      "displacement",
      "12c",
      "andisthereforeanunknownfunction",
      "orchestration",
      "Wires",
      "Thetwovariablesxandy",
      "intheinfinitedatalimit",
      "parameters",
      "basedpolicylearning",
      "Science346.6206",
      "twodifferentmodels",
      "episteme",
      "precedes",
      "Neighbourhood",
      "Lights",
      "languageunderstandingpaper.pdf,2018",
      "formulate",
      "divergence",
      "distillation",
      "model",
      "doctorate",
      "ownactions)canbeusedtodeducecause",
      "shoppers",
      "thread",
      "sufism",
      "robots",
      "functionoftherobot’ssensors",
      "particularlyatlargewidth[JGH18,LXS+19],mayprovideausefulframework",
      "proliferation",
      "concrete",
      "groceries",
      "abs/1512.00567,2015",
      "engineer3",
      "hope",
      "references",
      "L)isindependentofmodelsize",
      "Forexample",
      "modification",
      "Thetestlossonthesedatasets",
      "humility",
      "weanswerabout",
      "forecast",
      "examination",
      "minimizes",
      "j=1",
      "Withtheseparameters",
      "news",
      "increasedmodelparallelism",
      "galaxy",
      "hammer",
      "categories",
      "batch",
      "innovations",
      "formation",
      "postmodernity",
      "demiurge",
      "brothers",
      "brother",
      "payoff",
      "overcoming",
      "legacy",
      "Demonstrations",
      "east",
      "markers",
      "16(2):442–452",
      "mightserveabenchmarkingrolesimilartothatofthebAbItasks[163],withtheeventualgoalbeing",
      "rangedependencies[12",
      "regions",
      "ordinaryRLalgorithmtothelabelledepisodes",
      "entireclip",
      "Onemightinterprettheserelations",
      "171–180",
      "inthedistancebetweenpositions",
      "theorizing",
      "mission",
      "audience",
      "pages116–131,2012",
      "component",
      "modeloutperformsevenallpreviouslyreportedensembles",
      "functionbeingonepart",
      "Discussion",
      "modelrepair[58]provideotheravenuesforobtainingbettercontracts",
      "4a",
      "l",
      "train",
      "pills",
      "Data",
      "∝",
      "directedness",
      "andallareaquadraticfunctionofthefeatures",
      "wewillfocusonanarchitectureforSSLtheJointEmbeddingPredictive",
      "rangeinformation",
      "abs/1708.02596",
      "inadequacy",
      "idol",
      "embeddingparameter",
      "H´enaff",
      "cleaning",
      "bases",
      "log(1+exp(F",
      "Transfer",
      "environment",
      "“Estimationoftheparametersofasingleequation",
      "Right",
      "pull",
      "confirming",
      "physical",
      "processes",
      "ormaypredictfutureworldstatesresultingfromasequence",
      "slogan",
      "victory",
      "proponent",
      "Technocrats",
      "offer",
      "Interactivelearningfrompolicy",
      "ForReacherandCheetahfeedback",
      "Human",
      "10%chancethatthehumanrespondsuniformlyatrandom",
      "grasping",
      "retrieval",
      "Theabsence",
      "“Domain",
      "thesizeofneuralmodels",
      "rapiddetectionofsimplemotifs",
      "dits",
      "selection",
      "gr",
      "horizon",
      "efficienttraining",
      "intraining",
      "andDandgovernsthedegreeofoverfitting",
      "adjective",
      "Anevaluationplatformforgeneralagents",
      "pythonlibrary",
      "Confidence",
      "rose",
      "detail",
      "rails",
      "elimination",
      "stnemnrevog",
      "Theory",
      "cognitivearchitectureofthe",
      "Encodersbelongtothefamily",
      "displacements",
      "Narrative",
      "“Computersandthetheoryofstatistics",
      "dining−room",
      "entries",
      "esis",
      "∝C0.54",
      "asafunctionofmodelsizeisshowninFigure8;inallcasesthemodelsweretrainedonlyontheWebText2",
      "colleague",
      "Anatheism",
      "ships",
      "Definitions",
      "inexhaustibleness",
      "comfort",
      "regulation",
      "significantlywhenweonlyneedtopredictcomparisons",
      "fashions",
      "Number",
      "compression",
      "medieval",
      "member",
      "neuralnetworks",
      "domain",
      "K",
      "translating",
      "Animals",
      "intent",
      "essay",
      "iseconomicalwithuserfeedback",
      "WedisplayempiricaltrendsforthetestlossasafunctionofdatasetsizeD(intokens)andtrainingcompute",
      "Contributions",
      "writers",
      "mopping",
      "InSections3",
      "asimplewaytopreventneuralnetworksfromoverfitting",
      "medicine",
      "thesemethodsfocusonlyonthemodelerrorsandignoreotheraspectsofthedatadistribution",
      "anddropoutisveryhelpfulinavoidingover",
      "ormodel",
      "pragmatic",
      "“Causalinferenceinstatistics",
      "unifying",
      "toleratealargerlearningrate",
      "come?—We",
      "project",
      "Way",
      "HaandSchmidhuber,2018a",
      "showstheAIshootinganenemyshipwhiletherightclipshowsitbeingshotbyanenemyship",
      "o",
      "startthegame",
      "MCMC",
      "actions",
      "dismissal",
      "opportunity",
      "Fo",
      "Changesinvocabularysizeortokenizationareexpectedtorescalethelossbyanoverallfactor",
      "collaborators",
      "discontinuity",
      "nanosecond",
      "thetrendsforcompute",
      "journey",
      "judgment",
      "turn",
      "usedfortraining",
      "sequencemodels[37],theTransformeroutperformstheBerkeley-",
      "rise",
      "recipients",
      "relation",
      "pretraining",
      "naturalevolutionsoftheworld",
      "developments",
      "bed",
      "Generating",
      "relativepositions",
      "completion",
      "dependenttermsinourtrainingcomputeestimate",
      "emotion",
      "tiger",
      "GideonSMannandAndrewMcCallum",
      "heart",
      "ŁukaszKaiserandIlyaSutskever",
      "frompairwisepreferences",
      "fitting(exceptpossiblyfortheverylargestmodels",
      "Internet",
      "Intheembeddinglayers",
      "informationflowinthedecodertopreservetheauto",
      "outputtingaccuratepredictions",
      "lacuna",
      "wefindveryweakdependenceonmanyarchitecturalandoptimizationhyperparameters",
      "swal",
      "seeing",
      "andiscorrespondinglydifficultto",
      "submodule",
      "artBLEUscoreof28.4",
      "sentations",
      "construal",
      "symbolism",
      "Predictors",
      "hacking",
      "Weinspectattentiondistributions",
      "projects",
      "offs",
      "axioms",
      "agency",
      "fly",
      "unpalatable",
      "equallyusefulforlearninghumanpreferences",
      "x.",
      "link",
      "gradient-",
      "14(4):715–770",
      "bonus",
      "sentencerepresentationsusedbystate",
      "requirements",
      "causes",
      "Balance",
      "manifolds",
      "preferencesovertrajectoriesandsoweremovedthem",
      "matchingtheempiricalresultsinFigure14.Infactthemeasuredexponentissufficientlysmallthatourresults",
      "valueifavideoclipy",
      "origins",
      "InInternationalConferenceonLearningRepresentations",
      "whichbecomescriticalatlonger",
      "reflections",
      "Left",
      "hero",
      "socialist",
      "passages",
      "tutorial",
      "Introduction",
      "effectivelyonlypredictedthesign",
      "case",
      "plates",
      "stress",
      "Phenomenology",
      "links",
      "wecanperformimportanceweightingbyre",
      "EnglishConstituencyParsing",
      "cube",
      "claims",
      "larization",
      "fitswerequalitativelymuchmoreaccuratethanotherfunctionssuchaslogarithms(seeFigure23",
      "nikia",
      "Music",
      "yˆ.",
      "Maximumpathlengths",
      "statement",
      "refinement",
      "hittheballwiththepaddle",
      "flow",
      "individuals",
      "Contr",
      "insights",
      "training",
      "arXiv:1604.06057",
      "tluciffid",
      "setting",
      "systemscouldbeapromisingareaofresearch",
      "thetrainingprocessneverre",
      "tomatoes",
      "location",
      "exemplification",
      "Brahman",
      "capacities",
      "textlength",
      "inspection",
      "judge",
      "academics",
      "freepreference",
      "good",
      "bandits",
      "majority",
      "theuseofcompute",
      "situations",
      "rigidity",
      "gathering",
      "grammar",
      "Inaself",
      "predictionsforpositionicandependonlyontheknownoutputsatpositionslessthani",
      "arXiv:2006.16712",
      "Therehasbeenagreatdealofpublicdiscussionaroundaccidents",
      "theyusedifferentparameters",
      "latent",
      "weneedtocommunicatecomplexgoalstothesesystems",
      "wealsobelievethatitisworthgivingseriousthoughttopotentialchallengesandrisks",
      "ings",
      "missteps",
      "autoencoder",
      "−1",
      "theory",
      "weeks",
      "transitions",
      "degreeofoverfitting(ascomparedtotheinfinitedatalimit)issignificantlyoverestimatedbyL",
      "walls",
      "defenders",
      "halt",
      "mystery",
      "exceptfortherealhumanfeedbackwhichisasinglerun",
      "andR.Garnett",
      "contributions",
      "planet",
      "complexityimprovementsbecausethecostofcomputeisalreadycomparableto",
      "signals",
      "rangecorrelationsintexts",
      "Keywords",
      "transistors",
      "interests",
      "sides",
      "Quantilizers",
      "suggestion",
      "lengthsequenceofsymbolrepresentations",
      "step",
      "Intentionality",
      "ers",
      "transframing",
      "drawer",
      "whichcanprovidequiteaccuratepredictions",
      "Victim",
      "panpsychism",
      "tionofanabstractobjectivefunction",
      "machinelearningresearch",
      "neighbor",
      "Inthispaper",
      "ratherthangatheredthrough-",
      "encodes",
      "consensus",
      "convergence",
      "tennis",
      "maximization",
      "performance",
      "rate",
      "trainingforparsing",
      "Update(r",
      "connotations",
      "concealment",
      "areas",
      "attack",
      "drone",
      "meth-",
      "drivers",
      "signal",
      "scenarios",
      "Intphys",
      "Modelsize(ranginginsizefrom768to1.5billionnon",
      "Wehaveleftoffthe‘adj’labeltoavoidclutteringthenotation",
      "O",
      "w.",
      "withtools",
      "abs/1606.07419",
      "wehavenotoptimizedregularization(egthedropoutprobability)whilevaryingdatasetandmodelsize",
      "wewillstartfromagoalexpressedinnaturallanguage",
      "Inthefollowing",
      "correctionstoourscalinglaws",
      "idolatry",
      "thatmayreflecthumanpreferences",
      "differentlayertypes",
      "lots",
      "G.,Krause",
      "infinitum",
      "Readings",
      "ofsymbolsoneelementatatime",
      "Poesis",
      "pain",
      "pads",
      "regret",
      "gniddebme",
      "Explainability",
      "semantics",
      "meaninglessness",
      "configuration",
      "dysgraphia",
      "andtheRLalgorithmsusedtooptimizethepolicy",
      "tacit",
      "entirelyonself",
      "analysis",
      "variants",
      "indoors",
      "matrix",
      "validation",
      "buthumanlabelerstendtorewardanyprogresstowardspassingcars",
      "socio",
      "query",
      "bottle",
      "condition",
      "motions",
      "performancedoes",
      "parametersofasingleequationinacompletesystemofstochasticequations”",
      "texture",
      "3e",
      "individuation",
      "isn’tnecessarilystraightforward",
      "WorkshoponArtificialIntelligenceandStatistics",
      "Dwithintegerpowers",
      "pond",
      "physicalism",
      "suchasperformingabackflipordriving",
      "http://arxiv.org/abs/1606.05374",
      "andperformlonger",
      "scalablesupervision”),orundesirablebehaviorduringthelearningprocess(“safeexploration",
      "plan",
      "1B",
      "Unsubscribe",
      "plans",
      "L=(C",
      "essentials",
      "roadmap",
      "den",
      "estimatestheprobabilitythatoneischosenovertheotherbythehuman",
      "academic",
      "genocide",
      "pains",
      "buffer",
      "Fellowship",
      "judgement",
      "ballot",
      "thecostofnon",
      "designers",
      "stutter",
      "shadow",
      "rationalization",
      "chine",
      "expression",
      "principes",
      "realism",
      "whereasweinvestigatescalingwithbothmodelsizeandthequantityoftrainingcomputation",
      "warmup",
      "we've",
      "excludingallvocabularyandpositionalembeddings",
      "reasoning",
      "ofitdying",
      "volumeR5ofProceedingsofMachine",
      "byfutureimprovementstothepipelineforoutsourcinglabels",
      "brakes",
      "increasingby5xforeach10xincreaseincompute",
      "modules",
      "anddoesnotdependonthedurationoftrainingorproximitytoconvergence",
      "aswellasother",
      "humantime",
      "contents",
      "screw",
      "facility",
      "Implications",
      "temperance",
      "relyingentirelyonanattentionmechanismtodrawglobaldependenciesbetweeninputandoutput",
      "accidents",
      "tolerance",
      "adaptation",
      "ship",
      "depth",
      "ZK16",
      "eliminatedbytheinvariancepropertiesoftheencoder",
      "tseT",
      "andhigherenergiesawayfromtheregionsofhighdatadensity",
      "teeth",
      "utility",
      "corpus",
      "chunk",
      "segmentsforthattask(seeAppendixBfortheexactinstructionsgiventocontractors).Eachtrajectory",
      "Ourgoalistosolvethetaskinareasonableamountof",
      "3h",
      "gameplayingagentcouldtamperwiththesensorthat",
      "slip",
      "contractor",
      "yieldingd",
      "Terms",
      "nurture",
      "querywithallkeys",
      "ff",
      "philia",
      "features",
      "EmpiricalModelofCompute",
      "road",
      "schedulewitha3000steplinearwarmupfollowedbyacosinedecaytozero",
      "finity",
      "portion",
      "z.",
      "implementations",
      "attentionovertheoutputoftheencoderstack",
      "authority",
      "genomics",
      "muchlessthanthecriticalbatchsize",
      "addition",
      "ParameterandComputeScalingofTransformers",
      "archaeology",
      "machinetranslationarchitectures",
      "G",
      "cid:18",
      "existentialist",
      "air-",
      "extent",
      "rank",
      "recognition",
      "advantages",
      "requiring2S",
      "neural",
      "authors",
      "configurator",
      "maintenance",
      "attentioncouldberestrictedtoconsideringonlyaneighborhoodofsizerin",
      "moon",
      "agent",
      "losses",
      "fit",
      "independentofmostofthedetailsofitsmicroscopicconsituents",
      "cell",
      "proclivity",
      "surpassesallpreviouslypublishedmodelsandensembles",
      "examples",
      "approximators",
      "overloads",
      "criticalbatchsizeprovidesaroughlyoptimalcompromisebetweentimeandcomputeefficiency",
      "blah",
      "sophia",
      "Humans",
      "embeddedness",
      "gas",
      "twins",
      "y)2",
      "renewal",
      "LLMs",
      "Handling",
      "development",
      "chess",
      "causal",
      "matter",
      "Freud",
      "outline",
      "method",
      "∼C0.27withtrainingcompute",
      "10a",
      "consciousness",
      "thescreen)getshot(verybad",
      "ahumanworktogethertoachievethehuman’sgoals",
      "p.",
      "axiom",
      "Nature",
      "Stoics",
      "calibration",
      "attacks",
      "codedfeatures",
      "artreinforcementlearningsystems",
      "sophistication",
      "names",
      "oneagenthasmorelives",
      "orientations",
      "mistake",
      "conditions",
      "architec-",
      "abs/1511.07404",
      "ϵ",
      "∼1012parameters",
      "contextualism",
      "modelsreachmaximalperformance",
      "werewrittenat10",
      "wewouldnotbeveryconfidentofitsapplicability",
      "sphere",
      "supervision",
      "views",
      "timization",
      "EBM",
      "Romanticism",
      "Neuroscience",
      "Perspectives",
      "EncoderandDecoderStacks",
      "ICLR-19",
      "ghost",
      "Dynamics",
      "rationalities",
      "decayto0asthedifferenceinrewarddifferencebecomesextreme",
      "oval",
      "variant",
      "shot",
      "inanimate",
      "grounds",
      "pose",
      "beef",
      "textbook",
      "controller",
      "ndoesnotconnectallpairsofinputandoutput",
      "andJackClarkforreadingdraftsofthepaper",
      "transformation",
      "Imaginal",
      "deal",
      "interpersonal",
      "problems",
      "lawintheloss",
      "computers",
      "agentscouldinprincipletamperwiththeirrewardimplementations",
      "apprenticeship",
      "home",
      "virtue",
      "Meta",
      "defeatingtheotherinagameofchess",
      "law",
      "governance",
      "therewardfunctionischanging",
      "Allofnonparametricstatistics",
      "curvature",
      "δL≈",
      "increasedthemaximumoutputlengthtoinputlength+300",
      "Theconfigurations",
      "“LearningFairRepresentations",
      "zˇ",
      "easierforhumanstoprovideconsistentcomparisonsthanconsistentabsolutescores",
      "arXiv:1510.04342",
      "oflarge",
      "square",
      "Sports",
      "butwhichhelptheagentlearnaboutitsenvironment",
      "bravery",
      "wewillevaluateouralgorithms’behaviorintwoways",
      "harbingers",
      "happiness",
      "simulation",
      "eachpointistheaveragerewardoverfiveconsecutivebatches",
      "quest",
      "Networks",
      "possibilities",
      "∝(C",
      "Sound",
      "pages1503–1511,2014",
      "p∗.",
      "increasesinBresultindiminishingreturns",
      "directionality",
      "Identificationandcontrolofdynamicalsystems",
      "Action",
      "Soalthoughwe",
      "querywiththecorrespondingkey",
      "Roadmap",
      "delib-",
      "cid:19)αS",
      "Somestudies",
      "completeness",
      "thelistedtimeisfine",
      "diversity",
      "IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages2786–2793",
      "classical",
      "sizewhilekeepingoverfittingundercontrol",
      "Howdoweensurethatthecleaningrobotrecognizes",
      "productattentionis",
      "numberoflayersinadditiontothenumberofparameters",
      "reinforcement",
      "psukhe",
      "essentialism",
      "count",
      "ProposedL(N",
      "plot",
      "experiences",
      "factors",
      "guts",
      "procedure",
      "theperformanceofmodelswithdifferentdepthsconvergetoasingletrend",
      "TC",
      "Itappearstoimplythatcompute",
      "coding",
      "eventhoughitisunclearhowtohand",
      "pruning",
      "play",
      "inverse",
      "atlayer5of6",
      "energies",
      "servomechanism",
      "hallmarks",
      "asafunctionoftheprevioushiddenstateh",
      "strategies",
      "hate",
      "category",
      "parcel",
      "demonization",
      "assumption",
      "sort",
      "slash",
      "accord",
      "andsotheexperimentsinvolvingrealhumanfeedbackrequiredbetween30minutesand5hoursof",
      "calls",
      "agentmaximizespredictedreward",
      "alsotofillinmissinginformation",
      "layersineachencoderlayer",
      "Futureworkmaybeabletoimprovetheefficiencyoflearningfromhumanpreferences",
      "Planning",
      "Trends",
      "negation",
      "plan-",
      "suchtiesnevermadeupalargemajorityofourdata",
      "garden",
      "orat",
      "andd",
      "L(C)≈",
      "estima-",
      "period",
      "humans",
      "Equation(B.3)and(B.4)togetherdeterminethecompute",
      "includingrobustness",
      "mop",
      "combinations",
      "optimizer",
      "makingbasicarithmeticmistakes[CKB+21](seeAppendixD),andyetithasalsoovercomesomefundamental",
      "con-",
      "layerdecodersusingvariance",
      "tem",
      "2Herewedisplaypredictedcomputewhenusingasufficientlysmallbatchsize",
      "Open",
      "dangers",
      "da'at",
      "Smalldifferences",
      "O(1",
      "perseverance",
      "impact",
      "embodiment",
      "2000s",
      "researchers",
      "Thisapproachisdesignedtoseparatetruelearningfrommerememorization",
      "topics",
      "training2",
      "waytopreventcollapse",
      "capital",
      "homing",
      "messes",
      "orientation",
      "realization",
      "tionalsequencetosequencelearning",
      "idealism",
      "resentations",
      "forgetfulness",
      "emphasis",
      "In\"encoder",
      "tasksallowsustostudytrendsovermorethansevenordersofmagnitudeinscale",
      "targettokens",
      "/α",
      "goal",
      "“Safeexplorationtechniquesforreinforcementlearning",
      "com-",
      "budget",
      "economics",
      "audio",
      "art",
      "students",
      "cid:0)Nc",
      "tests",
      "node",
      "Ratherthanassumingthatp(x)changes",
      "vehicle",
      "modelshavenotbeenabletoattainstate",
      "aspect",
      "I'm",
      "poolingoveradjacentframes",
      "generation",
      "reach",
      "isittrainedfromtakingactionsandgettingrewards",
      "thehyperparametersdescribedthroughoutthepaper",
      "donotdecaytozero",
      "Asetofprerequisitetoytasks",
      "challenge",
      "col-",
      "surfaces",
      "numeracy",
      "policiesmustrepresentdistributions",
      "instance",
      "butno",
      "address",
      "drift",
      "estimate",
      "explorer",
      "stopping",
      "andtheinputforpositiont",
      "viXra",
      "Figures",
      "Structuredattentionnetworks",
      "quality",
      "sky",
      "https",
      "optimization",
      "Benchmarks",
      "crime",
      "head",
      "multiplicity",
      "Notes",
      "fewerlabels",
      "signer",
      "∼5.4×1013(tokens",
      "repair",
      "squash",
      "official",
      "suggestanincreasingrelevanceforresearcharoundaccidents",
      "hovering",
      "predictions",
      "quantity",
      "economist",
      "Depths",
      "land",
      "els",
      "levelagent’sactionsdon’tservethehuman",
      "sub-",
      "It’softenhardtotellwhatthegameisaboutjustbylookingatshortclips",
      "retreats",
      "cid:21)αD",
      "aletheia",
      "Things",
      "Convergenceisinefficient",
      "terms",
      "triplet",
      "lossonbothmodelanddatasetsize",
      "sentence",
      "Token",
      "mastery",
      "Whiteningforself",
      "misunderstanding",
      "4x",
      "ind",
      "andserialsteps",
      "weassumethatthereisahumanoverseerwhocanexpress",
      "kind",
      "functionestimator",
      "ics",
      "contract",
      "health",
      "InInternationalConferenceonMachineLearning",
      "itprovidesanestimateofthepointatwhichTransformerlanguage",
      "switch",
      "Rhomboid",
      "Specificity",
      "mark",
      "dream",
      "anyways",
      "crit",
      "Inthedomainofnaturallanguage",
      "Experience",
      "hacking”,theobjectivefunctionthatthedesignerwritesdownadmitsofsomeclever“easy”solution",
      "specificity",
      "de-",
      "AutonomousWeapons",
      "components",
      "arxivprepringarXiv:1606.05374",
      "dev",
      "“Theevolvedradioanditsimplicationsformodellingtheevolution",
      "0to30no",
      "WealsocomparetheperformanceofstandardTransformerstorecurrentTransformers[DGV+18]inFigure",
      "coreaspectofreasoning",
      "oneofthegeneralmechanismsthatproduceswrongobjectivefunctions",
      "takool",
      "lawfit",
      "variabl",
      "leap",
      "Superglue",
      "methodsinWilsonetal.(2012)canbeextendedtocomplextasksoriftheycanworkwithrealhuman",
      "avoidance",
      "aroundeachofthesub",
      "mining",
      "Notethatthe",
      "LSTMs",
      "Iftheagentisalreadydeadwhentheclipstarts",
      "E",
      "connection",
      "kinds",
      "gripping",
      "thinking",
      "trade",
      "stack",
      "moment",
      "propertythatrandomchangesaremorelikelytobebadthangood",
      "look",
      "awareness",
      "brutal",
      "electronics",
      "Notethatthisequation",
      "autoencoding",
      "concise",
      "naivelypenalizingempowermentcanalsocreateperverseincentives",
      "trick",
      "6a",
      "narrative",
      "door",
      "wherewealsoprovidesomeadditionalpredictions",
      "sensible",
      "Inthiswork",
      "archetypos",
      "option",
      "wasresponsibleforourinitialcodebase",
      "weight",
      "WhenworkingwithinafixedcomputebudgetC",
      "attentioncouldyieldmoreinterpretablemodels",
      "trouble",
      "andperhapsalsoforrandom",
      "hoe",
      "premise",
      "advancement",
      "f−1(x",
      "“Controllersforreachabilityspecifications",
      "salience",
      "D(L",
      "opportunities",
      "whereposisthepositionandiisthedimension",
      "break",
      "contractors",
      "extremelysmallgradients4",
      "fromourmodelsandpresentanddiscussexamplesintheappendix",
      "upgrade",
      "translation",
      "dimensionaldynamicsofgeneralizationerrorin",
      "documents",
      "it'll",
      "Longshort",
      "N(C",
      "concern",
      "Grammarasaforeignlanguage",
      "vacuity",
      "methods",
      "reachability",
      "dots",
      "Layers",
      "coordinates",
      "shapedreward",
      "grounding",
      "length",
      "zeroentropy",
      "approximation",
      "withdrawing",
      "vocab",
      "policy",
      "whiletrainingatbatch",
      "attentionlayerallofthekeys",
      "awakening",
      "thebatchsizecanbeincreasedwithveryminimaldegradationincompute",
      "parameterization",
      "mindset",
      "D.2",
      "episodes",
      "Poiesis",
      "specification",
      "numberofattentionheadsperlayer",
      "battery",
      "mood",
      "delusion",
      "methodologies",
      "parameterized",
      "WebText2,weaddedoutboundRedditlinksfromtheperiodofJanuary",
      "expert",
      "spirit",
      "Thefunctionofthecriticmodulecanbedynamicallyconfiguredbytheconfigurator",
      "Thereisnogenerallyagreedupondefinitionofintelligence",
      "preferencesbetweentrajectorysegments",
      "attempts",
      "meditation",
      "monster",
      "misdirection",
      "sizes",
      "crisis",
      "drawings",
      "theconvergedloss",
      "ECCV",
      "zeroafter80milliontimesteps(althoughrunswereactuallytrainedforonly50milliontimesteps",
      "needs",
      "perspectives",
      "6b",
      "mixture",
      "empowerment",
      "button",
      "stages",
      "bylearningshort",
      "deficits",
      "questioning",
      "categorization",
      "6e"
    ],
    "verbs": [
      "tear",
      "provoke",
      "stiffed",
      "evoke",
      "inferring",
      "inherited",
      "act",
      "andcomputebudget",
      "Thelatterproblemscouldpotentiallybeaddressed",
      "confess",
      "converging",
      "adjust",
      "searching",
      "free",
      "8x",
      "Note",
      "rep",
      "buteventuallyrequiresincreased",
      "envision",
      "love",
      "wielding",
      "Fairness",
      "motivated",
      "overcompensating",
      "schooling",
      "throwing",
      "introspecting",
      "find-",
      "demonstrate",
      "reward",
      "expressed",
      "webelieve",
      "somethings",
      "establishinganewstate",
      "elaborating",
      "simulated",
      "regarded",
      "fits",
      "paid",
      "assert",
      "present",
      "track",
      "thought",
      "confronted",
      "andapplyasoftmaxfunctiontoobtaintheweightsonthe",
      "justified",
      "boost",
      "preprint",
      "enjoying",
      "lose",
      "helps",
      "discusses",
      "estimates",
      "Synchronized",
      "articulate",
      "anylossfunctioncanbeused",
      "reply",
      "detecting",
      "game",
      "misconstrued",
      "Pˆ(cid:2",
      "expand",
      "spanning",
      "accrue",
      "Retrieving",
      "whichissmoothed",
      "enhance",
      "entitling",
      "tuned",
      "form",
      "carrying",
      "attributes",
      "todrawaunicorninTiKZ(alanguageforcreating",
      "haveobserved",
      "applies",
      "biasing",
      "save",
      "wrote",
      "planned",
      "analyzing",
      "conflict",
      "learn",
      "attune",
      "explicated",
      "descends",
      "associates",
      "embedding)computeusedatbatchsizeB.",
      "motivate",
      "emerges",
      "unfold",
      "overlap",
      "succeed",
      "transfer",
      "cure",
      "Thevalueofthefunctioncanbeprobedbyobserving",
      "Grasping",
      "developing",
      "develops",
      "inspired",
      "Screaming",
      "regard",
      "happening",
      "arguing",
      "Embedding",
      "stored",
      "bite",
      "pre",
      "scaled",
      "bound",
      "engage",
      "living",
      "perverted",
      "Knowing",
      "sincewefindthatthistendstogiveafixedimprovementclosetotheendoftraining",
      "outperform",
      "recommending",
      "controlled",
      "denote",
      "blooms",
      "Meaning",
      "formulated",
      "replaced",
      "display",
      "concentrate",
      "denoising",
      "override",
      "displacing",
      "ButthescalingwithD",
      "encompassing",
      "AvoidpassingcarsORgetting",
      "line",
      "behave",
      "points",
      "return",
      "foregrounds",
      "bind",
      "challenged",
      "Directed",
      "starts",
      "moved",
      "understand",
      "given",
      "traveling",
      "composed",
      "situated",
      "liberate",
      "tracking",
      "Consider",
      "append",
      "identified",
      "sacredness",
      "determine",
      "suchassupervised",
      "imagined",
      "manicured",
      "thwarted",
      "andabreakdownofourresultspertoken",
      "connecting",
      "solve",
      "forfeits",
      "retain",
      "acquiring",
      "confirms",
      "finds",
      "choosing",
      "embedding",
      "andusing",
      "grant",
      "accommodation",
      "brings",
      "bridge",
      "rhymes",
      "aligned",
      "criticizing",
      "testing",
      "Theapproachesdescribedabovefocusondetecting",
      "establish",
      "prevent",
      "apply",
      "vanishing",
      "cope",
      "defines",
      "eschew",
      "inga“jammingtransition”[GJS+19]whenthemodelsizereachesthedatasetsize(thismayrequiretraining",
      "counting",
      "launched",
      "dedicate",
      "updating",
      "occlude",
      "Understanding",
      "investigate",
      "Replicated",
      "Making",
      "structured",
      "clarify",
      "woven",
      "attributing",
      "contains",
      "supported",
      "facing",
      "Thisbehaviorwas",
      "devise",
      "looked",
      "vote",
      "Shaping",
      "challenges",
      "runs",
      "man’spreferenceswhilesimultaneouslytraining",
      "remembering",
      "engineered",
      "frame",
      "overemphasize",
      "resist",
      "amounts",
      "preordained",
      "Stacked",
      "recognize",
      "associate",
      "discloses",
      "guess",
      "connectedlayersforboththeencoderanddecoder",
      "drop",
      "controlling",
      "consists",
      "Having",
      "born",
      "dragging",
      "stuffed",
      "read",
      "compared",
      "highlight",
      "propagated",
      "referred",
      "uses",
      "sound",
      "corrupting",
      "leadtocoherentbutunanticipatedbehavior",
      "framed",
      "appearing",
      "went",
      "combined",
      "poses",
      "implement",
      "counteracts",
      "ground",
      "consuming",
      "beat",
      "centivize",
      "expectthatlargermodelsshouldalwaysperformbetterthansmallermodels",
      "balancing",
      "joins",
      "opposed",
      "Come",
      "led",
      "andrepeat",
      "solves",
      "invoking",
      "consumes",
      "resemble",
      "reaches",
      "limit",
      "pushes",
      "encountered",
      "written",
      "suffer",
      "addressing",
      "route",
      "explores",
      "test",
      "percept",
      "kept",
      "modifying",
      "wrestling",
      "receding",
      "sense",
      "alluded",
      "thatattempt",
      "ourexperimentsrequirelessthanoneorder",
      "recall",
      "dry",
      "knew",
      "tying",
      "situate",
      "Learn",
      "capturing",
      "shorting",
      "specified",
      "shouldthrowoutthingsthatareunlikelytobelongtoanyone",
      "perma-",
      "activated",
      "questioned",
      "hide",
      "elevated",
      "matters",
      "explain",
      "primed",
      "known",
      "establishing",
      "Whentraining",
      "exhibits",
      "enhances",
      "happen",
      "Talking",
      "excluding",
      "interface",
      "apparentlybecauseweaskedhumanstoprefertrajectorieswheretherobotwas“standing",
      "Said",
      "construe",
      "copy",
      "evaluate",
      "repeat",
      "Getting",
      "asexpected",
      "instantiated",
      "breaks",
      "pass",
      "goes",
      "standardizing",
      "itseemsimportanttobeabletodetectwhenoneisinasituationthatwasnotcoveredbythetraining",
      "stay",
      "run",
      "resultingstate)itisabletoobservewhiletraining",
      "results",
      "D.6",
      "arrived",
      "mislead",
      "power",
      "interpreting",
      "devised",
      "abs/1406.1078,2014",
      "divide",
      "taught",
      "addressed",
      "study",
      "attending",
      "loses",
      "predicted",
      "wecanalwaysgivetheagentnegativerewardforknocking",
      "purchase",
      "concerning",
      "betweenpairsoftrajectorysegments",
      "fix",
      "itreceives",
      "sincethereisnolongeranensemblethatwecouldusetoestimate",
      "degree",
      "accounts",
      "criticizes",
      "explicating",
      "suchastheshapeoflearning",
      "archetypes",
      "disconfirm",
      "displays",
      "PerformanceimprovespredictablyaslongaswescaleupN",
      "recovered",
      "needing",
      "Expanding",
      "question",
      "evaluating",
      "Coming",
      "batchsizesasmoredevicesareused",
      "suggested",
      "collapsed",
      "v",
      "attracted",
      "founding",
      "shrink",
      "aliased",
      "foresee",
      "problem",
      "deploying",
      "underlying",
      "ope-",
      "leaping",
      "think",
      "matching",
      "creeping",
      "soliciting",
      "randomseedsisroughlyconstantinmagnitudefordifferentmodelsizes",
      "producing",
      "dive",
      "lists",
      "equivocate",
      "watch",
      "drawing",
      "stops",
      "cause",
      "mindware",
      "adjusted",
      "anticipate",
      "ignoring",
      "forms",
      "Assume",
      "listen",
      "indicated",
      "intertwined",
      "fail",
      "resolved",
      "abstracted",
      "Integrating",
      "shown",
      "joined",
      "trust",
      "fight",
      "lawequationforthelearning",
      "trusted",
      "Grounded",
      "eschewing",
      "writing",
      "corresponds",
      "Mastering",
      "benefits",
      "indicates",
      "drive",
      "decreasing",
      "wait",
      "believes",
      "dealing",
      "reveals",
      "use",
      "enacting",
      "pulled",
      "pseudo",
      "thenumberofoperationsrequiredtorelatesignalsfromtwoarbitraryinputoroutputpositionsgrows",
      "requesting",
      "mizing",
      "shine",
      "zero",
      "generates",
      "requiring",
      "Biasing",
      "factor",
      "Universalityofoverfitting",
      "Trying",
      "physis",
      "forgotten",
      "expose",
      "expresses",
      "performs",
      "modeled",
      "sidered",
      "propagate",
      "bumped",
      "exactlyreflectrewardintheunderlyingtask",
      "Forlargemodelstrainedwithalimiteddatasetwithearlystopping",
      "anddoes",
      "emphasize",
      "seen",
      "uncover",
      "interpenetrating",
      "measure",
      "reify",
      "preexists",
      "Inordertopracticallytraindeep",
      "exclude",
      "provides",
      "predicting",
      "rest",
      "pointed",
      "Writes",
      "gaining",
      "means",
      "dialogue",
      "discussing",
      "infinite",
      "reflects",
      "concatenated",
      "occurs",
      "specifies",
      "foreseeing",
      "Parsing",
      "Unveiling",
      "enumerates",
      "responded",
      "lived",
      "enact",
      "maintaining",
      "decoded",
      "actualized",
      "acknowledging",
      "shines",
      "disappeared",
      "relating",
      "advancing",
      "2Note",
      "deriving",
      "vary",
      "requires",
      "revised",
      "hovered",
      "meaning",
      "positing",
      "happened",
      "leading",
      "pronounced",
      "shifting",
      "symbolizes",
      "intended",
      "ceasing",
      "Signed",
      "clone",
      "occur",
      "tI",
      "accumulated",
      "figure",
      "Suppose",
      "itworkssufficientlywellforthemodelsweconsidered",
      "justify",
      "likelihood",
      "longing",
      "disturb",
      "showcase",
      "mattered",
      "shares",
      "converse",
      "linking",
      "replacing",
      "Questing",
      "briefly",
      "Dealing",
      "attach",
      "losing",
      "flourishing",
      "recommend",
      "viewing",
      "synthesizes",
      "x",
      "gauge",
      "inverselywiththenumberoftimesteps;aftertwiceasmanytimestepshaveelapsed",
      "encountering",
      "updates",
      "fulfilling",
      "infers",
      "quantized",
      "disconnected",
      "Improving",
      "trying",
      "defended",
      "wants",
      "stand",
      "growing",
      "Picking",
      "mistaken",
      "remained",
      "delay",
      "concentrating",
      "generalized",
      "concealing",
      "trespass",
      "knocking",
      "Supervised",
      "lessening",
      "overwrites",
      "extracting",
      "cited",
      "carries",
      "simplified",
      "Interpret",
      "contribute",
      "shut",
      "neglect",
      "enhanced",
      "Fig",
      "fails",
      "specialized",
      "floating",
      "rescueoperationcouldbequitelarge",
      "normalized",
      "anticipated",
      "revise",
      "fills",
      "bottlenecked",
      "committing",
      "resting",
      "surrender",
      "dos",
      "Wonder",
      "examined",
      "ploded",
      "Wewillempiricallydemonstratethattheoptimallytrainedtestlossaccordswiththescaling",
      "connected",
      "marking",
      "venture",
      "knock",
      "originated",
      "worry",
      "terrifying",
      "ismuchsmallerthanthevaluesofLwehaveachieved",
      "Coding",
      "F.2",
      "wave",
      "regularize",
      "Reformulating",
      "optimize",
      "lingering",
      "attempted",
      "valuing",
      "leaving",
      "removed",
      "relates",
      "sorry",
      "Inferring",
      "Nikidesigned",
      "recite",
      "characterized",
      "sounds",
      "define",
      "MacGlashanetal.(2017),Pilarskietal.(2011),KnoxandStone(2009),andKnox(2012)perform",
      "considered",
      "shaped",
      "languageunsupervised",
      "treating",
      "Representing",
      "overfitting",
      "evenif",
      "generalize",
      "uselinearinterpolation",
      "pushing",
      "adapting",
      "provide",
      "increases",
      "Ourworkcouldalsobeseenofaspecificinstanceofthecooperativeinversereinforcementlearning",
      "reminded",
      "bear",
      "realized",
      "builds",
      "crafted",
      "Welcome",
      "improved",
      "design",
      "overwhelm",
      "dataparallelismandthenumberofserialtrainingstepsrequired",
      "waiting",
      "increase",
      "=",
      "focusing",
      "Given",
      "disclosing",
      "agrees",
      "discounted",
      "returns",
      "argue",
      "defining",
      "completing",
      "amortized",
      "Took",
      "exploiting",
      "deny",
      "achieved",
      "Writing",
      "Recycling",
      "create",
      "grow",
      "desired",
      "cares",
      "allowing",
      "interconnected",
      "found",
      "speculating",
      "simplify",
      "bullshitting",
      "Applied",
      "configures",
      "purported",
      "DeepLearningandHumanBeings.2015.url",
      "Let",
      "i=1",
      "picked",
      "ending",
      "observed",
      "implements",
      "change",
      "undermined",
      "introduced",
      "watched",
      "Failed",
      "Sincetheartificialnoisewouldaffect",
      "hasbeencruciallyinvolvedineveryaspectofthiswork",
      "crawling",
      "correlated",
      "buy",
      "voting",
      "penalize",
      "-]—discussing",
      "glean",
      "remove",
      "dismantle",
      "refining",
      "masked",
      "pinpoints",
      "misspoke",
      "Designing",
      "brewed",
      "andevensomething",
      "exercising",
      "works",
      "aspiring",
      "bordering",
      "drops",
      "oratleasthaveamethodofevaluating",
      "“Areductionofimitationlearning",
      "proposing",
      "misused",
      "thecliporfindithardtoevaluate",
      "remembered",
      "wefindthatthereisasingleequationcombining",
      "amelioratesuchproblemsbyfindingmoreefficientwaystoexploitourlimitedoversightbudget",
      "coined",
      "proliferate",
      "affected",
      "Vari-",
      "cultivating",
      "motivates",
      "mulated",
      "devote",
      "according",
      "attentionandstarted",
      "calibrated",
      "shift",
      "yield",
      "distinguish",
      "pairencoding",
      "overlaps",
      "specializing",
      "istheminimumnumberofdataexamplesthatmustbeprocessed",
      "weshouldincreasethedataset",
      "ran",
      "paying",
      "Think",
      "smashing",
      "Shining",
      "enters",
      "Scaling",
      "showed",
      "keeping",
      "acceptedisthatintelligenceisnotlimitedtoaspecificdomainortask",
      "reporting",
      "objects",
      "remains",
      "struct",
      "divorced",
      "ameliorate",
      "hindering",
      "managing",
      "varying",
      "bears",
      "orient",
      "unoccupied",
      "risk",
      "recreate",
      "responding",
      "propose",
      "limiting",
      "visualizing",
      "Bounded",
      "1024exceptwhereotherwisenoted",
      "biased",
      "sends",
      "andnumberoftrainingstepsasafunctionof",
      "pages832–841.ACL",
      "thetotalcomputationalcost",
      "augmented",
      "unexplicated",
      "Emergence",
      "disappears",
      "s",
      "wireheading",
      "etaR",
      "6d",
      "Breaking",
      "built",
      "Frenchused",
      "automated",
      "IlliaPolosukhin∗",
      "misspecified",
      "dataforunsupervisedlearningviagenerativemodeling",
      "receiving",
      "Predicting",
      "bend",
      "interpenetrated",
      "Meets",
      "Playing",
      "deserves",
      "comparing",
      "provided",
      "moves",
      "divides",
      "converges",
      "arXivpreprintarXiv:1701.06538,2017",
      "comports",
      "determined",
      "Bootstrap",
      "blind",
      "weightingeachtraining",
      "recognizing",
      "reinterpret",
      "detect",
      "executed",
      "becauseempowermentmeasuresprecisionofcontrol",
      "refers",
      "practice",
      "store",
      "actionsarereversible[104,159],oraslimitingtheprobabilityofhugenegativerewardtosome",
      "Choose",
      "scale",
      "correspond",
      "driven",
      "stating",
      "follows",
      "sourcing",
      "backed",
      "plugs",
      "operates",
      "Denoising",
      "rewarded",
      "wethendefinetheestimatednon",
      "summarize",
      "ameliorating",
      "minimizing",
      "balance",
      "springing",
      "filled",
      "cap",
      "Training",
      "associated",
      "involve",
      "admit",
      "constraining",
      "Realizing",
      "dismiss",
      "misunderstand",
      "organizing",
      "inscribed",
      "andarecriticaltopreventbothintrinsicallyandbecausesuchaccidentscouldcause",
      "affects",
      "operationalize",
      "legislate",
      "fleshed",
      "review",
      "earn",
      "appreciates",
      "gets",
      "tains",
      "cid:1)α",
      "activate",
      "Looking",
      "assume",
      "modulating",
      "they're",
      "relies",
      "bleed",
      "Thiscouldallow",
      "Algo-",
      "maps",
      "differed",
      "blowing",
      "debating",
      "opened",
      "acknowledged",
      "survive",
      "conceiving",
      "sociated",
      "alter",
      "covered",
      "maintained",
      "corresponding",
      "compiled",
      "andmedicine",
      "oversee",
      "graphing",
      "itwon’tbeinclinedtostop",
      "artifi-",
      "smell",
      "minimaxing",
      "match",
      "Rebooting",
      "persecuted",
      "appreciated",
      "L∗",
      "coauthored",
      "ignore",
      "manipulate",
      "token",
      "misrepresented",
      "engenders",
      "includes",
      "tempt",
      "precedes",
      "earlystoppingshouldoccurwhentrainingisdatalimited",
      "satisfying",
      "feel",
      "formulate",
      "replicated",
      "confuse",
      "varies",
      "acting",
      "Generalizationperformancetootherdatadistributionsimprovessmoothlywithmodelsize",
      "targetvocabularyofabout37000tokens",
      "entitled",
      "model",
      "rejects",
      "perceiving",
      "manipulating",
      "taps",
      "discuss",
      "report",
      "hanging",
      "know",
      "Standing",
      "critiqued",
      "whilebeing",
      "ask",
      "pushed",
      "Deepmodelscanbetrainedusing",
      "united",
      "recapitulate",
      "extrapolate",
      "andmeasuring",
      "employing",
      "perceives",
      "settling",
      "broaden",
      "hope",
      "unpack",
      "Reclaim",
      "regularizing",
      "understandscalingatverylargeDholdingmodelsizefixed[AS17],andalsotheshapeoflearning",
      "Remember",
      "Need",
      "inter",
      "scaling",
      "Robustnesstodistributionalshift”(Section7)discusseshowtoavoidhavingMLsystemsmakebad",
      "guided",
      "reunite",
      "inspire",
      "WheretheprojectionsareparametermatricesWQ",
      "translates",
      "“Estimatingcausaleffectsoftreatmentsinrandomizedandnonrandomized",
      "talking",
      "Wecompareourmethodusing",
      "situates",
      "emergence",
      "progresses",
      "saves",
      "taht",
      "force",
      "underwrite",
      "minimizes",
      "presupposed",
      "left",
      "Abitofprogressinlanguagemodeling",
      "broken",
      "proposes",
      "credited",
      "Following",
      "organize",
      "itsrewardisgenerated",
      "Aspiring",
      "fool",
      "educated",
      "endeavoring",
      "sweep",
      "disagree",
      "redescribe",
      "harm",
      "infer",
      "withstateoftheartmodels[RNSS18,DCLT18,YDY+19,LOG+19,RSR+19]approaching",
      "attribute",
      "mix",
      "modeling",
      "discern",
      "triggers",
      "faced",
      "served",
      "5Note",
      "ended",
      "raised",
      "helping",
      "fooled",
      "supposed",
      "cite",
      "integrate",
      "fulfill",
      "targeted",
      "lead",
      "build",
      "referring",
      "Kill",
      "greatlyimprovingresultsandmassivelyaccelerating",
      "trapped",
      "enabling",
      "deemed",
      "undermine",
      "overcoming",
      "existed",
      "assess",
      "sits",
      "referenced",
      "masking",
      "misunderstood",
      "machine",
      "praise",
      "Hear",
      "continuing",
      "grasp",
      "OuralgorithmfollowsthesamebasicapproachasAkrouretal.(2012)andAkrouretal.(2014",
      "connect",
      "letting",
      "Participating",
      "tries",
      "andwealsotestonsimilarly-",
      "probe",
      "message",
      "Invited",
      "intends",
      "theorizing",
      "facilitating",
      "gains",
      "misspelled",
      "stays",
      "converged",
      "sized",
      "integrated",
      "drives",
      "hit",
      "playing",
      "refined",
      "convinced",
      "rhyming",
      "produced",
      "scales",
      "involves",
      "praised",
      "result",
      "resituating",
      "owing",
      "labelingthesamerun",
      "correcting",
      "solved",
      "applying",
      "transformed",
      "resulting",
      "announces",
      "reordering",
      "search",
      "train",
      "suffers",
      "aswellastheembedding",
      "draw",
      "teaching",
      "succeeds",
      "wherewewanttomakesureahumanisnotblockedbyanagentfromshuttingtheagentdown",
      "exists",
      "faces",
      "inscalingituptomoderndeepRLanddemonstratebyfarthemostcomplexbehaviorsyetlearned",
      "gives",
      "claiming",
      "background",
      "framing",
      "Flourishing",
      "Stop",
      "jump",
      "rendered",
      "simulate",
      "perhapsbecausethelearnedrewardfunctionisslightlybettershaped",
      "attracting",
      "tie",
      "giveundesiredresults",
      "revived",
      "ChartingtheInfiniteDataLimitandOverfitting",
      "specify",
      "cleaning",
      "passes",
      "chosethisfunctionbecausewehypothesizeditwouldallowthemodeltoeasilylearntoattendby",
      "couldprovideanapproach",
      "want",
      "withnegligibleincreaseinthenumberofserialsteps",
      "interdefined",
      "transframe",
      "needed",
      "pull",
      "sounding",
      "accessed",
      "8)",
      "conform",
      "contrasting",
      "appreciate",
      "shade",
      "succeeded",
      "check",
      "mean",
      "possess",
      "speak",
      "centers",
      "considercontinuousdomainswithfourdegreesoffreedomandsmalldiscretedomains",
      "engineerarewardfunctionthatwouldincentivize",
      "anddon’tgetpassedbycars",
      "construct",
      "grading",
      "Dispatching",
      "denoted",
      "URLhttp://arxiv.org/abs/1905",
      "discriminate",
      "resembles",
      "liking",
      "offer",
      "pinpointing",
      "assimilate",
      "grasping",
      "tocapturetheinformationnecessarytopredicts",
      "crossed",
      "reviews",
      "identifying",
      "encouraging",
      "triggering",
      "precisionfloating",
      "derived",
      "navigating",
      "acquiesce",
      "ThisdemonstratesthatGPT-4can“see”despitebeingapurelanguagemodel(weemphasize",
      "reveal",
      "withdraws",
      "eachdimensionofthepositionalencoding",
      "comes",
      "satisfy",
      "gr",
      "planning",
      "acted",
      "decided",
      "undergoing",
      "bounded",
      "architectures",
      "dare",
      "stumbling",
      "forgive",
      "exemplifies",
      "internalize",
      "alluding",
      "adopt",
      "Equipped",
      "eliminate",
      "lie",
      "formalize",
      "push",
      "depending",
      "disconnect",
      "Oriented",
      "ExperimentswithtraditionalRLtaskshelpusunderstandwhetherourmethodiseffective",
      "daddy",
      "batchsize",
      "constituencyparsing",
      "reproducing",
      "misusing",
      "fixated",
      "monitoring",
      "Closing",
      "watching",
      "Beat",
      "having",
      "it's",
      "equating",
      "replicates",
      "decreases",
      "coordinating",
      "bothering",
      "monitor",
      "andbytrackingtestlossandstoppingonceitisnolonger",
      "falls",
      "primes",
      "sample",
      "itsgradientmaybeapproximated",
      "spoke",
      "misread",
      "influencing",
      "bring",
      "confront",
      "cultivate",
      "unobserved",
      "embody",
      "killing",
      "considering",
      "triggered",
      "Exploring",
      "itwasdemonstratedthatforawidevarietyofneuralnetworktasks",
      "employed",
      "withsomecommentstoexplainthesteps",
      "singing",
      "mapping",
      "clean",
      "revealed",
      "configured",
      "whichcouldpotentiallyexplainthisfinding",
      "Domainadaptationforsentimentclassification”",
      "areusedinconjunctionwitharecurrentnetwork",
      "states",
      "reviewing",
      "translating",
      "observe",
      "focuses",
      "called",
      "predict",
      "aidan@cs.toronto.edu",
      "highlighting",
      "Overarching",
      "overfit",
      "corresponded",
      "betothinkofwrongobjectivefunctionsasemergingfromgeneralcauses(suchaspartiallyobserved",
      "brought",
      "misses",
      "state",
      "butalsothatwearealreadyhittingdiminishingreturns",
      "spinning",
      "refuting",
      "respond",
      "organized",
      "explains",
      "stated",
      "culminates",
      "wecan",
      "reject",
      "attempt",
      "encodesitscontentasnotincluding",
      "governed",
      "bringing",
      "wear",
      "allow",
      "avoiding",
      "averaged",
      "suspect",
      "adapt",
      "situationswheretheintegralisintractableanditsgradientmustbeapproximatedbyMonte",
      "operate",
      "verbalizing",
      "delegate",
      "Reuse",
      "wanting",
      "caused",
      "itisarguablythemoststudiedoftheproblems",
      "drain",
      "omits",
      "wherearobotjuststoodin",
      "According",
      "pronouncing",
      "cures",
      "pay",
      "extend",
      "participates",
      "accomplished",
      "base",
      "imposing",
      "giving",
      "explored",
      "Pay",
      "forthecenteroftherobottomovetotheright(movingtotheleftisworsethannotmoving",
      "consist",
      "equated",
      "wish",
      "dropped",
      "illustrate",
      "finish",
      "analyzed",
      "pursued",
      "arise",
      "Retrieved",
      "weusedparametersettingswhichhavebeenfoundtoworkwellfortraditionalRLtasks",
      "publish",
      "turn",
      "simplifies",
      "becauseitmayallowthemodeltoextrapolatetosequencelengthslongerthantheonesencountered",
      "learningtoalignandtranslate",
      "usedfortraining",
      "fall",
      "map",
      "live",
      "rise",
      "muminiM",
      "hold",
      "misplaced",
      "interfering",
      "grained",
      "pre-)trained",
      "object",
      "fracture",
      "Recentworkhasachieved",
      "presentvideosofagentsattemptingtofulfillthatgoal",
      "Letting",
      "orchestrating",
      "talk",
      "deprived",
      "replace",
      "brushing",
      "sparse",
      "expect",
      "open",
      "Thisissimply",
      "mitigating",
      "pulls",
      "advocating",
      "Generating",
      "Selecting",
      "behaved",
      "gnosis",
      "conjecture",
      "asks",
      "coming",
      "Bring",
      "supervised",
      "Hencewealsocompare",
      "encircles",
      "isolate",
      "AIsafety”inpublicdiscussions)intermsofclassicmethodsinmachinelearning",
      "founded",
      "facilitates",
      "noted",
      "burns",
      "arrive",
      "Buy",
      "existing",
      "instantiating",
      "prioritize",
      "forced",
      "disconfirms",
      "evaluated",
      "find",
      "took",
      "perform",
      "JaredKaplan∗",
      "freeze",
      "stop",
      "basedontheTransformerarchitecture[VSP+17]andtrainedonmassivecorporaofweb",
      "projected",
      "seeing",
      "pages",
      "drown",
      "justifies",
      "Turned",
      "wired",
      "Work",
      "hear",
      "debate",
      "hacking",
      "pute",
      "abuse",
      "assembling",
      "experiencing",
      "execute",
      "interpenetrate",
      "qualifies",
      "seeking",
      "encompass",
      "Algorithmsforinversereinforcementlearning",
      "Reconciling",
      "generating",
      "link",
      "satisfies",
      "discrete",
      "specifying",
      "knows",
      "Based",
      "t",
      "Identify",
      "treat",
      "TherewasasimilarbonusintheRLreward",
      "Taming",
      "proposed",
      "Tocharttheborderlandsoftheinfinitedatalimit",
      "verified",
      "required",
      "taxed",
      "TomBBrown",
      "avoid",
      "internalized",
      "fitted",
      "causes",
      "Balance",
      "claimed",
      "changes",
      "ringing",
      "flowing",
      "reflecting",
      "indicating",
      "collapse",
      "dynam-",
      "exert",
      "rectangles",
      "Providedproperattributionisprovided",
      "ecnis",
      "fled",
      "walked",
      "Left",
      "attain",
      "asked",
      "containing",
      "scratch",
      "relate",
      "dowen",
      "studies",
      "Turn",
      "Thiswillgenerallyresultinveryslowlearning",
      "experienced",
      "upisatie",
      "inwhichcase",
      "Teaching",
      "they'd",
      "operated",
      "Calculating",
      "commit",
      "dominate",
      "Learning",
      "additionalworkonspeedingupearlytrainingdynamicsmaybewarranted",
      "share",
      "cost",
      "reaching",
      "Listed",
      "assimilated",
      "surpasses",
      "links",
      "r",
      "capping",
      "Building",
      "penalizing",
      "followingpicture",
      "let",
      "obey",
      "learned",
      "claims",
      "fill",
      "resisted",
      "missed",
      "dumb",
      "forgot",
      "foregrounded",
      "believed",
      "picture",
      "Automated",
      "disable",
      "transforms",
      "centered",
      "numberofparametersindeeplearning",
      "argues",
      "yields",
      "beginning",
      "retrieves",
      "worries",
      "placed",
      "Returning",
      "interacting",
      "6.1.3",
      "produce",
      "implies",
      "Testing",
      "flow",
      "signed",
      "stands",
      "press",
      "prioritizing",
      "including",
      "poke",
      "sought",
      "characterize",
      "thwarts",
      "supplementing",
      "Forourbasemodelsusing",
      "moderate",
      "training",
      "designed",
      "obtained",
      "turning",
      "exemplifying",
      "levels",
      "reproduce",
      "update",
      "Austin,2012",
      "comprehend",
      "guarantee",
      "supervisedlearning",
      "evolve",
      "setting",
      "acquired",
      "undo",
      "adjusts",
      "happens",
      "diverges",
      "receives",
      "discovering",
      "suppose",
      "convey",
      "esized",
      "Proposing",
      "perspectival",
      "andRuslanSalakhutdi-",
      "thenumberoftraining",
      "contexts",
      "reset",
      "win",
      "qualitative",
      "objectsmayspontaneouslyemerge",
      "guiding",
      "overload",
      "trains",
      "preventing",
      "averaging",
      "fed",
      "ai",
      "combine",
      "engaged",
      "like",
      "Estimating",
      "zeroing",
      "work",
      "tweeted",
      "continue",
      "modelforonestep",
      "employ",
      "inhabit",
      "interwoven",
      "reducing",
      "judge",
      "engineer",
      "usingleakyReLUs(α=0.01)asnonlinearities.7",
      "rushing",
      "holdthetotalparametercountfixed",
      "Fall",
      "collect",
      "reached",
      "6NBS",
      "wewouldexpectthelosstrendtoleveloffatorbeforeL∗.",
      "transform",
      "represented",
      "opt",
      "separated",
      "enables",
      "drowned",
      "Performancewhenvaryingmodelanddatasize",
      "list",
      "ma-",
      "layerinatypicalsequencetransductionencoderordecoder",
      "Orchestrating",
      "closed",
      "outperformed",
      "trained",
      "Internalized",
      "covering",
      "concerns",
      "breaking",
      "modulate",
      "measures",
      "flipped",
      "running",
      "presenting",
      "shaping",
      "seek",
      "restricting",
      "coordinated",
      "flipping",
      "tempts",
      "participating",
      "theintermediatevocabularyofactionsbepredefined",
      "Soourscaling",
      "align",
      "participate",
      "culminating",
      "conveying",
      "promote",
      "insight",
      "acknowledges",
      "slowed",
      "Showing",
      "Programming",
      "overlapping",
      "empower",
      "deliberate",
      "undermining",
      "answering",
      "request",
      "sings",
      "sentiment",
      "ofthegrowthindataexamplesprocessedcanbeusedforincreasedbatchsizes",
      "slept",
      "maximized",
      "rescue",
      "feeling",
      "approaching",
      "u",
      "qualifying",
      "exacerbates",
      "affecting",
      "balanced",
      "voidness",
      "modulates",
      "Distorted",
      "findthat",
      "Detecting",
      "transgressed",
      "marks",
      "obtaining",
      "handled",
      "experience",
      "endowed",
      "function",
      "gave",
      "misaligns",
      "makes",
      "increasing",
      "variancetrade",
      "valued",
      "blossoming",
      "memorizing",
      "duced",
      "reflect",
      "Applying",
      "answer",
      "step",
      "target",
      "incorporated",
      "impresses",
      "transframing",
      "distort",
      "impose",
      "constrained",
      "heals",
      "demanding",
      "assemble",
      "fold",
      "stained",
      "follow",
      "announcing",
      "Optimized",
      "penalized",
      "accompanied",
      "interacted",
      "excuse",
      "adding",
      "got",
      "Notice",
      "said",
      "writes",
      "walk",
      "overcome",
      "reformulates",
      "tracing",
      "sponding",
      "anddatasetsize",
      "Related",
      "convergence",
      "serves",
      "embed",
      "initialize",
      "shared",
      "gating",
      "realizing",
      "interpolate",
      "ensures",
      "attack",
      "riddled",
      "ChatGPT:[ProducesLATEXcompiling",
      "Thefirstoneinvolvesnocomplexreasoning",
      "asingley˜isproduced",
      "signal",
      "come",
      "TheTransformer(big)modeltrainedforEnglish",
      "observing",
      "Proposed",
      "cutting",
      "Weconcludethatthechoiceoflearning",
      "capture",
      "send",
      "manipulated",
      "rescale",
      "traveled",
      "measuring",
      "chosen",
      "Apossibledefenseagainstcaseswheretheagentcaninfluenceitsown",
      "judging",
      "eliminating",
      "dating",
      "attempting",
      "parenting",
      "coy",
      "Submitted",
      "outputs",
      "cid:28)B",
      "loads",
      "directing",
      "programmed",
      "atypicalpreprocessingstepwhichisparticularlyappropriateheresincethepositionoftherewardsis",
      "exposedness",
      "exploits",
      "awaiting",
      "ingstrategiescanbeemployed",
      "matches",
      "appears",
      "hallucinated",
      "throughaGibbsformulaP(y|x",
      "optimizes",
      "strike",
      "converge",
      "indwell",
      "polling",
      "Knows",
      "regularized",
      "betweenthepresentstate",
      "checking",
      "mitigate",
      "painted",
      "additiveattentionoutperforms",
      "AdvancesinNeuralInformationProcessingSystems28,pages2440–2448.CurranAssociates",
      "excavated",
      "impress",
      "enjoyed",
      "insightsfromanoisyquadraticmodel",
      "enhancing",
      "entitles",
      "linked",
      "activating",
      "Imagine",
      "Layla",
      "precludes",
      "keeps",
      "gather",
      "approximated",
      "regroups",
      "meet",
      "access",
      "arises",
      "thedifferenceinpredictedrewardoftwotrajectorysegments",
      "completed",
      "Read",
      "delegating",
      "tokenized",
      "plan",
      "theseapproachesarenotdirectlyapplicabletobehaviorsthatare",
      "wondering",
      "undergo",
      "inkling",
      "doomed",
      "encompasses",
      "tells",
      "add",
      "reacting",
      "vised",
      "counted",
      "Suggested",
      "operationalized",
      "exhausted",
      "plans",
      "afforded",
      "Downloaded",
      "m",
      "contend",
      "swollen",
      "begins",
      "borders",
      "circle",
      "impede",
      "came",
      "Weexpectthatthisformulacouldbeimproved",
      "articulating",
      "ŁukaszKaiser∗",
      "onthetaskhumanfeedbackrangedfrombeinghalfasefficientasgroundtruthfeedbacktobeing",
      "interferes",
      "render",
      "Ensuring",
      "explained",
      "attached",
      "personalized",
      "attuning",
      "misquoted",
      "exerted",
      "leaves",
      "tempted",
      "combining",
      "bias",
      "yˆand",
      "misframing",
      "spending",
      "protects",
      "accusing",
      "removing",
      "captured",
      "draws",
      "apologize",
      "deals",
      "routing",
      "stutter",
      "pointing",
      "grade",
      "affording",
      "fixing",
      "youwanttostay",
      "finished",
      "oftenpassingthefirstlevelinSpaceInvadersandreachingascoreof",
      "going",
      "usingatits",
      "computes",
      "sinceitcanbeimplementedusinghighlyoptimized",
      "help",
      "compute",
      "turned",
      "Appendices",
      "included",
      "translated",
      "suggesting",
      "reasoning",
      "speculate",
      "traps",
      "contradicts",
      "plays",
      "expressing",
      "perceived",
      "oreventohavemutualinformationwithit(oratleasttopenalize",
      "coded",
      "experimented",
      "putting",
      "excels",
      "Multiply",
      "certified",
      "screw",
      "accesses",
      "signifies",
      "chose",
      "grounded",
      "A(s[0",
      "expected",
      "wrapping",
      "COLLAPSE",
      "contextualize",
      "Someauthorshave",
      "crashed",
      "belongs",
      "nating",
      "interleaved",
      "moving",
      "improves",
      "reconcile",
      "starting",
      "internalizing",
      "capitalizes",
      "andlayernormalizationareomitted",
      "subverted",
      "Asks",
      "resonate",
      "held",
      "andtheinitialvalueofv",
      "depend",
      "timeusingasfewqueriesaspossible",
      "questing",
      "flying",
      "providing",
      "Generativeadversarialimitationlearning",
      "acquire",
      "Thismakes",
      "isheldfixed",
      "preserved",
      "W.BradleyKnoxandPeterStone",
      "usedataduringtraining",
      "explicate",
      "drained",
      "regularizationandadjusttheregularizationcoefficienttokeepthevalidationloss",
      "arXivpreprintarXiv:1705.04304,2017",
      "canwestilllearnnearlyasquicklyasifitwereprovided",
      "aswellasacrossdomains",
      "understanding",
      "cycling",
      "Separating",
      "relying",
      "modified",
      "likes",
      "innovating",
      "orapproximate",
      "rocketed",
      "ff",
      "features",
      "focus",
      "destroy",
      "learns",
      "influenced",
      "mobilizes",
      "emphasizing",
      "describing",
      "handwritten",
      "interaffording",
      "gamed",
      "cover",
      "wecandirectlystudytheextentofoverfitting",
      "permits",
      "labelsarefedinataratedecaying",
      "granted",
      "continues",
      "closing",
      "waking",
      "experiences\"—again",
      "representing",
      "sent",
      "invokes",
      "Eachtraining",
      "identify",
      "enter",
      "tested",
      "compare",
      "consulting",
      "encoding",
      "awayinbothcomputeandmodelsize",
      "try",
      "texted",
      "plantoextendtheTransformertoproblemsinvolvinginputandoutputmodalitiesotherthantextand",
      "confronting",
      "bridges",
      "disrupt",
      "introduce",
      "ratify",
      "orchestrate",
      "Objects",
      "eliminated",
      "basedreinforcementlearning",
      "directed",
      "responds",
      "regulated",
      "attuned",
      "confusing",
      "allows",
      "criticize",
      "translate",
      "published",
      "exhibit",
      "offering",
      "gon",
      "derives",
      "alerting",
      "learning",
      "wrap",
      "retrieving",
      "fit",
      "composing",
      "decorrelating",
      "plagues",
      "promised",
      "evah",
      "outweigh",
      "instantiate",
      "originates",
      "attacking",
      "it’sdifficulttodetermineinwhatcircumstancestheycanbetrusted",
      "grows",
      "overarching",
      "shape",
      "inhabits",
      "defeats",
      "intuned",
      "updated",
      "invoke",
      "construed",
      "met",
      "gone",
      "varied",
      "catch",
      "launch",
      "comported",
      "influences",
      "Internalizing",
      "place",
      "internalizes",
      "withonlyaverysmallincreaseinserialtrainingtimerequired",
      "Draw",
      "excelling",
      "outlined",
      "represents",
      "retrieved",
      "reintegrate",
      "senting",
      "avoids",
      "accounted",
      "verifying",
      "Regularizing",
      "matter",
      "recommended",
      "landscaping",
      "feeding",
      "literalize",
      "avoided",
      "mediates",
      "inferred",
      "advocated",
      "mastered",
      "tem[139]asksuserstosupplyrulesthateachgeneratemanyweaklabels;and[65]extrapolates",
      "overriding",
      "assumptionsmaybeviolated",
      "occursat",
      "possessing",
      "drink",
      "remaining",
      "oriented",
      "consciousness",
      "contributes",
      "cessing",
      "subject",
      "Realhumanfeedbackistypicallyonlyslightlylesseffectivethanthesyntheticfeedback;depending",
      "asking",
      "invested",
      "Stopping",
      "teach",
      "generate",
      "advocate",
      "concludes",
      "assigned",
      "interfere",
      "noting",
      "damaging",
      "touched",
      "prove",
      "lift",
      "repeated",
      "tend",
      "operating",
      "highlighted",
      "process",
      "pronounces",
      "emerged",
      "simulating",
      "Fixed",
      "untrained(randomlyinitialized)policy",
      "reformulate",
      "gain",
      "shutting",
      "views",
      "algorithms",
      "begun",
      "missing",
      "size",
      "face",
      "contained",
      "heading",
      "hesitate",
      "deferring",
      "criticized",
      "aim",
      "Thinking",
      "pursuing",
      "sustained",
      "GermanandEnglish",
      "dataset",
      "lacking",
      "flagged",
      "sunk",
      "stepping",
      "culminate",
      "diverge",
      "exist",
      "choose",
      "listening",
      "estimated",
      "correct",
      "constellates",
      "crushes",
      "documented",
      "wanted",
      "modify",
      "received",
      "leveraging",
      "Tagging",
      "laying",
      "damaged",
      "unlessyoutrulyareconfused",
      "Run",
      "optimizing",
      "cut",
      "speaks",
      "extract",
      "bootstrapping",
      "saying",
      "maintains",
      "al-",
      "grounds",
      "places",
      "achieves",
      "studying",
      "taking",
      "equate",
      "close",
      "allocate",
      "speed",
      "Reissued",
      "punched",
      "assumed",
      "hardwired",
      "Perhapssuchatinydatasetrepresentsadifferentregimeforlanguagemodeling",
      "treated",
      "start",
      "induce",
      "constitutes",
      "effectonscaling",
      "Butifoneabidesbythedeeplearning",
      "executing",
      "belong",
      "deal",
      "amplified",
      "observes",
      "issomewhatimproved",
      "hidden",
      "passengers",
      "involved",
      "exploit",
      "Wevariedthelearning",
      "Look",
      "rely",
      "limits",
      "Maximizing",
      "accepts",
      "awaken",
      "imagine",
      "formulating",
      "virtue",
      "dedicated",
      "regulating",
      "conditioned",
      "sit",
      "demonstrates",
      "counts",
      "noticed",
      "slide",
      "cuts",
      "explore",
      "taken",
      "creates",
      "roll",
      "shouts",
      "blew",
      "encode",
      "applied",
      "depicted",
      "falling",
      "Breeding",
      "programming",
      "leads",
      "ranging",
      "andWO",
      "Thank",
      "Figure12",
      "write",
      "turing",
      "looking",
      "driving",
      "harnessing",
      "claim",
      "working",
      "whenwehold",
      "trivialize",
      "turns",
      "sets",
      "reappear",
      "minimize",
      "deleted",
      "touches",
      "guarantees",
      "sampled",
      "passed",
      "focused",
      "rebuild",
      "Driving",
      "headed",
      "configure",
      "organizes",
      "making",
      "introducing",
      "quest",
      "miscommunicated",
      "Avoiding",
      "receive",
      "confer",
      "Abenchmarkforvisualintuitivephysicsreasoning",
      "expanded",
      "queried",
      "constrasted",
      "inducing",
      "whichintheorycouldbeusedtotriviallyinfer",
      "appeal",
      "discussed",
      "sees",
      "located",
      "remember",
      "exapt",
      "modulated",
      "critiquing",
      "heard",
      "maximumlikelihoodthatarenotautomaticallynormalized",
      "Enduro(evenmode",
      "embedded",
      "currentmodelsareonlycapableofverylimitedformsofreasoning",
      "comprises",
      "improve",
      "embodying",
      "theorize",
      "finding",
      "implemented",
      "forgets",
      "incentivize",
      "celebrate",
      "lost",
      "connects",
      "approximate",
      "distorts",
      "raise",
      "replaces",
      "Lost",
      "accomplishing",
      "described",
      "changed",
      "spreads",
      "displayed",
      "remain",
      "cansolvethisproblemusinganyRLalgorithmthatisappropriateforthedomain",
      "recommends",
      "begin",
      "committed",
      "talks",
      "count",
      "exapting",
      "rolling",
      "choke",
      "offers",
      "stole",
      "record",
      "pausing",
      "utilize",
      "hurt",
      "Calloused",
      "emphasized",
      "zoom",
      "offended",
      "demonstrating",
      "sumed",
      "modes",
      "employs",
      "classify",
      "equal",
      "presents",
      "equals",
      "transcendence",
      "Tried",
      "coding",
      "itisrelativelycommonfortwoclipstobothhavezero",
      "Eliminating",
      "die",
      "duped",
      "range",
      "noring",
      "play",
      "represent",
      "misdirected",
      "iterate",
      "deepen",
      "encounter",
      "split",
      "incorporate",
      "adds",
      "conceded",
      "coping",
      "qualified",
      "require",
      "triangulate",
      "accentuate",
      "incentivizes",
      "onPongofflinetrainingsometimes",
      "resonating",
      "parameterize",
      "butweconfrontthechallengesinvolved",
      "believe",
      "Activerewardlearning",
      "accomplish",
      "amplifying",
      "Instantiating",
      "ceases",
      "Defining",
      "endure",
      "thinks",
      "TheTransformerachievesbetterBLEUscoresthanpreviousstate",
      "reading",
      "tell",
      "blooming",
      "Buildingalargeannotated",
      "started",
      "spoken",
      "“Learningfromlabeledfeaturesusing",
      "established",
      "sleep",
      "limited",
      "Fortherestoftraining",
      "exercises",
      "preferring",
      "suggests",
      "slash",
      "skills",
      "serve",
      "labeled",
      "distinguishing",
      "engaging",
      "impressing",
      "thereisalso",
      "whichmakessimilarpredictionsforthedependenceof",
      "Sacredness",
      "view",
      "suggest",
      "veils",
      "standing",
      "taste",
      "conclude",
      "attains",
      "resolve",
      "calls",
      "Tracking",
      "interact",
      "paced",
      "Cause",
      "discover",
      "optimized",
      "caring",
      "oversimplified",
      "commended",
      "Planning",
      "copying",
      "leave",
      "handle",
      "reside",
      "NoamShazeer∗",
      "realize",
      "unfolds",
      "clarifies",
      "howsusceptibleare",
      "overcomes",
      "feeds",
      "Thetestlosstypicallyfollowsthatofarundonewithunrestricteddatauntildiverging",
      "andfitrˆtothesetotalrewardsusingmeansquared",
      "ThisideaseemsrelatedtoH",
      "challenging",
      "enabled",
      "tends",
      "Wecomparetrajectorysegmentsof25timesteps(1.7secondsat15fpswithframeskipping",
      "maximizing",
      "maximize",
      "pushingthesoftmaxfunctionintoregionswhereithas",
      "isforthecenteroftherobottomovetotheright",
      "embodied",
      "facilitate",
      "express",
      "exceeds",
      "coerce",
      "theobjectivethataretooexpensivetobefrequentlyevaluatedduringtraining",
      "rejecting",
      "creating",
      "pause",
      "Growing",
      "developed",
      "grasped",
      "mask",
      "consisting",
      "asmeasuredbytheminimumnumberofsequentialoperationsrequired",
      "understands",
      "co",
      "surrounding",
      "presented",
      "Reading",
      "implied",
      "orsampled",
      "Measuring",
      "advances",
      "improving",
      "support",
      "converting",
      "getting",
      "filling",
      "intersects",
      "dispensed",
      "spread",
      "telling",
      "comment",
      "takes",
      "develop",
      "popularized",
      "reduce",
      "resisting",
      "value",
      "opening",
      "counteract",
      "escape",
      "thwarting",
      "demanded",
      "changing",
      "reduces",
      "invoked",
      "Explaining",
      "showing",
      "reduced",
      "restrict",
      "lifts",
      "flew",
      "strumming",
      "hacked",
      "tried",
      "confuses",
      "Keeping",
      "Tells",
      "conceive",
      "andonourhardwaretypicallyprocesses",
      "consider",
      "evolves",
      "possesses",
      "pick",
      "managed",
      "rising",
      "protected",
      "enmeshed",
      "Drawing",
      "Trusted",
      "Descartes",
      "transforming",
      "examine",
      "silencing",
      "circling",
      "backpropagating",
      "satisficing",
      "reviewed",
      "forgiven",
      "locked",
      "interweave",
      "defend",
      "tackled",
      "aspire",
      "thenumberofGPUsused",
      "maximizes",
      "slams",
      "prefer",
      "poking",
      "acts",
      "reasoned",
      "transcend",
      "handling",
      "scream",
      "specializes",
      "whenfulfillingaparticularprogram",
      "Toimprovecomputationalperformancefortasksinvolving",
      "mention",
      "incurs",
      "attacked",
      "determines",
      "designing",
      "forecasting",
      "gotten",
      "added",
      "compliments",
      "measured",
      "offthebottomofthescreen",
      "tub",
      "enable",
      "diminishing",
      "Caring",
      "slow",
      "stabilize",
      "deepens",
      "omitted",
      "Spoken",
      "weused",
      "ties",
      "joining",
      "aspect",
      "promoting",
      "witnessintheclip",
      "describe",
      "sensitize",
      "computed",
      "reach",
      "attend",
      "burned",
      "neglected",
      "reawaken",
      "challenge",
      "maintain",
      "col-",
      "∞.",
      "intend",
      "achieve",
      "erases",
      "supports",
      "seeding",
      "address",
      "interpret",
      "wonder",
      "trading",
      "therangeoftaskstowhichitcanbeapplied",
      "extracted",
      "rein-",
      "assumes",
      "achieving",
      "estimate",
      "alleviates",
      "stopping",
      "slipped",
      "argued",
      "told",
      "stipulates",
      "worked",
      "generated",
      "interpreted",
      "Lookattheclipsandselecttheoneinwhichbetterthingshappen",
      "mortal",
      "cheering",
      "related",
      "muchgreaterthanthecriticalbatchsize",
      "head",
      "imparts",
      "extracts",
      "detected",
      "canmodelp",
      "experiment",
      "minimized",
      "noticing",
      "enjoy",
      "causing",
      "facilitated",
      "devising",
      "stacking",
      "completequestionanswering",
      "assigns",
      "continued",
      "resort",
      "closes",
      "innate",
      "excited",
      "respect",
      "constructed",
      "emerging",
      "differs",
      "defined",
      "felt",
      "says",
      "comestoreasoninghaveledsometoclaimthatdeeplearningsystemsneedtobeaugmented",
      "differ",
      "transformative",
      "reconstructed",
      "appear",
      "Wesetthemaximumoutputlengthduring",
      "puts",
      "processed",
      "reads",
      "andanestimateofthesustained",
      "spent",
      "Inthissetting",
      "picks",
      "talked",
      "misjudge",
      "lrate",
      "Define",
      "core",
      "Filled",
      "completes",
      "gnarled",
      "standardized",
      "penalizes",
      "drowns",
      "account",
      "predicts",
      "recon-",
      "burning",
      "adapts",
      "decomposed",
      "shining",
      "processing",
      "aletheia",
      "terms",
      "afford",
      "prevents",
      "weshowthatouralgorithmcanlearn",
      "undertake",
      "smuggling",
      "transcends",
      "meets",
      "Token",
      "approaches",
      "disclose",
      "encloses",
      "backgrounded",
      "misunderstanding",
      "reuse",
      "following",
      "misleading",
      "regulates",
      "endeavor",
      "3b",
      "prime",
      "debunk",
      "decrease",
      "trigger",
      "dispensingwithrecurrenceandconvolutions",
      "coupled",
      "allowed",
      "relieved",
      "demonstrated",
      "idealized",
      "carried",
      "suchas",
      "hiding",
      "Hiring",
      "Mitigating",
      "decompose",
      "necessitate",
      "surface",
      "infinity",
      "created",
      "drawn",
      "sowedon’treview",
      "describes",
      "dream",
      "viewed",
      "crit",
      "engages",
      "advise",
      "determining",
      "Overcoming",
      "concerned",
      "ensuringthatpowerfulRLsystemscanbeapplied",
      "calling",
      "doubles",
      "realizes",
      "reconciled",
      "returning",
      "possessed",
      "straints",
      "agape",
      "behaving",
      "Asking",
      "visiting",
      "picking",
      "summarized",
      "confused",
      "enacted",
      "dying",
      "decide",
      "altered",
      "performing",
      "destroying",
      "Turing",
      "regulate",
      "sleeping",
      "mentioned",
      "veiled",
      "leap",
      "prescribe",
      "appreciating",
      "furtherinvestigationintomodelparallelismiswarranted",
      "wake",
      "acknowledge",
      "imagining",
      "distributed",
      "forget",
      "integrating",
      "studied",
      "contain",
      "recurring",
      "thelimitsoflanguagemodeling",
      "swells",
      "Doubt",
      "post",
      "exploring",
      "notice",
      "thinking",
      "overdo",
      "pervised",
      "lifting",
      "ensure",
      "trade",
      "situating",
      "fulfills",
      "stack",
      "Guidedcostlearning",
      "kill",
      "whileforlongerclipstheevaluationtimewas",
      "practicing",
      "strikes",
      "reason",
      "import",
      "fixed",
      "selected",
      "rule",
      "sitting",
      "Recentsuccessinscalingreinforcementlearning(RL)tolargeproblemshasbeendrivenindomains",
      "encourage",
      "feed",
      "looks",
      "awarded",
      "elaborate",
      "constitute",
      "transcending",
      "whichvariesfrom15to60timestepsdepending",
      "functions",
      "protect",
      "look",
      "mundi",
      "discretized",
      "formed",
      "based",
      "concluding",
      "contributed",
      "aspired",
      "prepared",
      "constructing",
      "flourish",
      "performed",
      "label",
      "flips",
      "forcing",
      "categorize",
      "care",
      "chooses",
      "tapping",
      "Abandoned",
      "idealize",
      "affords",
      "espoused",
      "affect",
      "itisbesttotrainwithabatch",
      "informed",
      "expanding",
      "entrapping",
      "Kaiming",
      "accept",
      "emphasizes",
      "Solved",
      "verify",
      "pursue",
      "recover",
      "set",
      "depends",
      "point",
      "overwritten",
      "Gives",
      "submit",
      "ensembling",
      "dicted",
      "transgress",
      "commuting",
      "captures",
      "Mentioned",
      "wecompare",
      "investigated",
      "integrates",
      "fighting",
      "solving",
      "thank",
      "assessing",
      "retrieve",
      "Analyzing",
      "played",
      "caught",
      "discovers",
      "highlights",
      "break",
      "disciplined",
      "appeared",
      "problematize",
      "cheat",
      "Fighting",
      "comprise",
      "attained",
      "catching",
      "opens",
      "perturbed",
      "retrace",
      "refer",
      "weprovidethefirstevidencethatthesetechniquescanbeeconomicallyscaledupto",
      "direct",
      "reap",
      "butwealsovaryittomeasurethecriticalbatchsize",
      "Generalizing",
      "stare",
      "hack",
      "oversees",
      "connotes",
      "efficiencyimproveswithmodelsize",
      "produces",
      "Otherlinesofresearchconsidersthegeneralproblemofreinforcementlearning",
      "solver",
      "include",
      "need",
      "attaining",
      "ignored",
      "contrast",
      "requested",
      "deployed",
      "knowing",
      "protested",
      "sowecanignorethissubtlety(see",
      "t=1",
      "coordinates",
      "contributing",
      "∗",
      "disclosed",
      "lies",
      "grounding",
      "extended",
      "crosses",
      "Deleting",
      "minimumcompute",
      "implementing",
      "generative",
      "theobjectivefunctionisphysicallyreplaced",
      "Adapting",
      "withdrawing",
      "Imagenet",
      "control",
      "misunderstands",
      "Live",
      "sharing",
      "unfolding",
      "pulling",
      "ignores",
      "corrected",
      "saw",
      "lay",
      "predictingthenextstates[1],thenitwaitsforthenextperceptresulting",
      "hired",
      "followed",
      "implying",
      "touch",
      "shows",
      "individuates",
      "emerge",
      "squared",
      "Removing",
      "understood",
      "wherewedefined",
      "meant",
      "indicate",
      "promise",
      "tasked",
      "parameterized",
      "orisnowonamoreadvancedlevel);justfocusonwhathappensintheclip",
      "exercise",
      "drag",
      "zeroes",
      "lying",
      "incorporating",
      "speaking",
      "accelerate",
      "output",
      "isfed",
      "attempts",
      "AwaytouseH",
      "note",
      "ensuring",
      "beperformedbydecomposing",
      "forgetting",
      "encoded",
      "carry",
      "stopped",
      "constrain",
      "disappear",
      "ordered",
      "helped",
      "building",
      "walking",
      "approach",
      "deletes",
      "separate",
      "Use",
      "lines",
      "lack",
      "adjusting",
      "estimating",
      "curated",
      "increased",
      "accepted",
      "compiling",
      "speculative",
      "failed",
      "iterating",
      "derive",
      "galvanize",
      "agree",
      "SupervisedRewardLearning",
      "needs",
      "feels",
      "includingreasoning",
      "categorizedaccordingtowhether",
      "rewards",
      "cultivated",
      "devoted",
      "designates",
      "Thisreactiveprocessdoesnotmakeuseoftheworldmodelnorofthecost",
      "arouse",
      "assuming",
      "cycle",
      "Fallibilism",
      "questioning",
      "articulated",
      "todenotethemodelsize",
      "invalidated"
    ],
    "adjectives": [
      "dirty",
      "Formal",
      "stoic",
      "Cognitive",
      "horizontal",
      "Optimal",
      "helpful",
      "inaccessible",
      "free",
      "doctrinal",
      "layerinputandnormalized",
      "deep",
      "pet",
      "worried",
      "round",
      "undesirable",
      "unknown",
      "nonhuman",
      "nearby",
      "mustbeasindependentofeachotheraspossible",
      "enlighten",
      "simulated",
      "Jour-",
      "ordinary",
      "continental",
      "present",
      "Half",
      "analytical",
      "neuroscientific",
      "aspirant",
      "multi-",
      "schematic",
      "disparate",
      "clearer",
      "weneedtodecreasetheamountoffeedbackrequiredbyseveral",
      "InTable3rows(B),weobservethatreducingtheattentionkeysized",
      "specifier",
      "althoughtheiralgorithmic",
      "lawinN",
      "meta",
      "dramatic",
      "roomclean",
      "chical",
      "bymodelingthelosswithanoisyquadratic",
      "brilliant",
      "biblical",
      "rational",
      "ISBN",
      "modelsizewealsosimultaneouslyvariedthed",
      "Contemporary",
      "mammal",
      "fine",
      "literary",
      "theEBMisflexible",
      "negative",
      "trans-",
      "courageous",
      "sudden",
      "plausible",
      "Real",
      "wonderful",
      "interested",
      "generic",
      "significantimprovementsincomputationalefficiencythroughfactorizationtricks[21]andconditional",
      "real",
      "average",
      "cu-",
      "mammalian",
      "vulnerable",
      "aggregate",
      "combinatorial",
      "pre",
      "Common",
      "exp(−F",
      "architectural",
      "pro",
      "bound",
      "Figure14",
      "cruel",
      "pickup",
      "integratable",
      "rigorous",
      "radical",
      "sacramental",
      "numinous",
      "Figure2.2",
      "linear",
      "im-",
      "best",
      "interesting",
      "indirect",
      "honest",
      "Thesparsely",
      "respectful",
      "resistant",
      "minor",
      "anachronistic",
      "interdisciplinary",
      "q",
      "successful",
      "Global",
      "Contrastive",
      "goal-",
      "atafractionofthetrainingcostofanyof",
      "reverse",
      "half",
      "integrative",
      "unchanged",
      "complete",
      "Tocounteractthiseffect",
      "complementary",
      "dense",
      "sacredness",
      "silly",
      "practicable",
      "harder",
      "contextual",
      "individual",
      "SupervisedLearn-",
      "addictive",
      "unique",
      "profound",
      "lawrelationshipsbetweenperformanceand",
      "Dynamic",
      "thenouragentoughtto",
      "embedding",
      "Asiatic",
      "considerable",
      "smarter",
      "experimental",
      "narrow",
      "Weusedtheserelationstoderivethecomputescaling",
      "maybeinterestingresearchproblemsassociatedwithhowtosafelyincrementallyupdatepoli-",
      "alternative",
      "trivially",
      "agnostic",
      "short-",
      "unconditional",
      "Athenian",
      "structured",
      "ancient",
      "better",
      "New",
      "overconfident",
      "coherent",
      "Actionsthathavelotsofsideeffectsmightbemoredifficulttodecodeuniquelytotheiroriginal",
      "Gnostic",
      "mere",
      "AgraphicalrepresentationisshowninFigure14",
      "Unbiased",
      "noisy",
      "positional",
      "hesitant",
      "classic",
      "statistical",
      "Bad",
      "associate",
      "Convolutionallayersaregenerallymoreexpensivethan",
      "visual",
      "unsu-",
      "hideous",
      "scholarly",
      "Thesepreliminaryobservationswillrepeatthemselvesthroughoutthepaper",
      "polar",
      "learnable",
      "tional",
      "sexual",
      "obsolete",
      "attentional",
      "Wrong",
      "weak",
      "thankful",
      "wedesireasolutiontosequentialdecisionproblemswithoutawell",
      "unlikely",
      "Possible",
      "Prototypical",
      "Toevaluatethedegreeofcompatibilitybetweenxandy",
      "Classical",
      "Conditional",
      "belief",
      "conceptual",
      "subtle",
      "potential",
      "audacious",
      "provocative",
      "modifica-",
      "Socratic",
      "opposed",
      "rustic",
      "military",
      "substantial",
      "lesser",
      "suchasthefinitebatchsizeandotherlimitsonthe",
      "High",
      "conditional",
      "supportive",
      "unavailable",
      "duplicitous",
      "prototypical",
      "uncontrolled",
      "legitimate",
      "mutual",
      "inconvenient",
      "select",
      "percept",
      "immediate",
      "commonsensical",
      "Proleptic",
      "writingaplatonic",
      "experiential",
      "dry",
      "collective",
      "archi-",
      "executive",
      "layersinthemodel",
      "to109",
      "tired",
      "judicial",
      "specified",
      "ontological",
      "favorite",
      "anagogic",
      "Improved",
      "symmetric",
      "Theoveralltaskisdefinedbyahigh",
      "Excellent",
      "probable",
      "usefulapproximation",
      "Agent",
      "inparticular",
      "adjacent",
      "Thetrainable",
      "inaudible",
      "Slow",
      "Logical",
      "proportionalto5∗106/(T",
      "Automatic",
      "counterfactual",
      "decoderforstatistical",
      "solid",
      "whichleadsustoprefermethodswhicharerobust",
      "black",
      "Regularized",
      "6th",
      "thecomplexityofaseparable",
      "predictable",
      "itsinputiseitherthecurrentstateoftheworldorpossiblestatespredictedby",
      "general",
      "Nietzschean",
      "Fundamental",
      "malicious",
      "invertible",
      "useeachrewardpredictorinourensembletopredictwhichsegmentwillbe",
      "andwiththesecapabilitiesatorabovehuman",
      "Heteronomous",
      "special",
      "optimizedtraining",
      "Japanese",
      "wecanroughlypredictthe",
      "deepest",
      "Inageneral",
      "exact",
      "implicit",
      "preferable",
      "∈",
      "ek",
      "simplest",
      "corrective",
      "Practical",
      "summed",
      "underlying",
      "foundational",
      "foreseeable",
      "Massiveexplorationofneural",
      "active",
      "preferencesonly",
      "southeast",
      "onethatcontainsthetruedistributionortrueconcept",
      "forexampleinverseRL(Finnetal",
      "Sunny",
      "religious",
      "theonomous",
      "connectedness",
      "vertical",
      "d",
      "wethankOpenAIandDeepMindforprovidingasupportive",
      "Insummary",
      "partial",
      "16th",
      "impressive",
      "abstracted",
      "trajective",
      "alive",
      "D",
      "accented",
      "Toavoidthis",
      "worst",
      "logged",
      "reachable",
      "motor",
      "prevalent",
      "tireless",
      "fromFigure14",
      "cooperative",
      "comfortable",
      "Anotheristheamountofcomputationthatcan",
      "Srivastavaetal",
      "untapped",
      "comprehensive",
      "Pure",
      "zero",
      "Cartesian",
      "proprioceptive",
      "titanic",
      "responsive",
      "long",
      "noema",
      "absolute",
      "demolish",
      "Romantic",
      "translucent",
      "complex",
      "unprecedented",
      "textual",
      "excess",
      "Robust",
      "Final",
      "agent’scurrentpolicy",
      "sub",
      "unpredictable",
      "Analternativetosamplere",
      "Chinese",
      "strange",
      "mid",
      "infinite",
      "intelligent",
      "unsolved",
      "Long",
      "hessian",
      "Modern",
      "Rounded",
      "apt",
      "learnthatcheckingforvisibledirtisanevencheaperbutstill",
      "trainable",
      "Asynchronousmethodsfordeepreinforcement",
      "versarial",
      "Key",
      "leading",
      "unfortunate",
      "bottleneckedbytheothertwo",
      "true",
      "featural",
      "intended",
      "tI",
      "irrelevant",
      "sizable",
      "outer",
      "reducible",
      "counter",
      "analytic",
      "Adaptivelearningrateswithsublinearmemory",
      "Impatient",
      "Theenergyfunctionproduceslowenergyvaluesaroundthedata",
      "andthememorykeysandvaluescomefromtheoutputoftheencoder",
      "low-",
      "lower",
      "intelligible",
      "frequent",
      "patient",
      "thorough",
      "fundamental",
      "outperformedtheother",
      "continuous",
      "therobottobeashighupaspossible",
      "behav-",
      "anddepth",
      "Similar",
      "fraught",
      "realistic",
      "generalized",
      "tiny",
      "autoregres-",
      "tionstotheenvironment",
      "Supervised",
      "distributional",
      "aparticularmodelsizeisoptimal",
      "soft",
      "easy",
      "transjective",
      "sensory",
      "simplified",
      "stylesparseAuto",
      "Inefficient",
      "major",
      "Wepresentpower",
      "hard",
      "presentative",
      "Typical",
      "neglect",
      "enhanced",
      "international",
      "Big",
      "normalized",
      "shownontheleft",
      "ontopofwhichlinguistic",
      "convenient",
      "Physical",
      "cross",
      "reversible",
      "procedural",
      "problematic",
      "simple",
      "Austrian",
      "specifiedmodelfamily",
      "connected",
      "organizational",
      "imaginable",
      "reasonable",
      "Realistic",
      "regional",
      "orthat",
      "differentiable",
      "overwhelming",
      "faithful",
      "poetic",
      "right",
      "sorry",
      "French",
      "trustworthy",
      "Irish",
      "mnemonic",
      "invariant",
      "Incremental",
      "Interpolate",
      "oppressive",
      "attentionlayersinthedecoderalloweachpositioninthedecodertoattendto",
      "Neo",
      "attentiontocomputerepresentationsofitsinputandoutputwithoutusingsequence-",
      "secular",
      "economic",
      "current",
      "defining",
      "impersonal",
      "amortized",
      "rpA",
      "essential",
      "inventive",
      "Algorithmic",
      "expensive",
      "thermal",
      "priori",
      "Applied",
      "Logistic",
      "Wittgensteinian",
      "observed",
      "salient",
      "sible",
      "strategiesitlearnedforcleaninganofficemightbedangerousonafactoryworkfloor",
      "industrial",
      "Concrete",
      "Adversarial",
      "indistinguishable",
      "rare",
      "3D",
      "archetypal",
      "non",
      "reticent",
      "unified",
      "autopoietic",
      "α",
      "multilevel",
      "mental",
      "4th",
      "loud",
      "realness",
      "Tacit",
      "Tenth",
      "Computational",
      "actual",
      "laden",
      "grandiose",
      "instrumental",
      "irregular",
      "dangerous",
      "meaningless",
      "Graphic",
      "op",
      "weusearateof",
      "fruitful",
      "Scalar",
      "downward",
      "fatal",
      "representational",
      "Sequencetosequencelearningwithneural",
      "efficacious",
      "tricky",
      "whilehavingacomparable",
      "double",
      "Massively",
      "boring",
      "equivocal",
      "overwrought",
      "ofsendingthequerytoahuman",
      "undetected",
      "akin",
      "overwhelmed",
      "closer",
      "bright",
      "German",
      "red",
      "Semi",
      "earthly",
      "original",
      "requisite",
      "similar",
      "Basic",
      "fitting",
      "Unsupervised",
      "torturous",
      "theoutputissubjecttostrongstructural",
      "flexible",
      "imaginal",
      "s",
      "environmentproducesarewardsignal",
      "later",
      "complicated",
      "severe",
      "extreme",
      "enigmatic",
      "ing",
      "relevant",
      "useless",
      "marked",
      "trastive",
      "absurd",
      "unsolvable",
      "sim-",
      "Asian",
      "Safe",
      "codingagainsteverypossible",
      "distorted",
      "initial",
      "computetoreachthesameloss",
      "safer",
      "transparent",
      "Onlymodelswithfewerthan2",
      "environmental",
      "wide-",
      "orisittrainedbypredictingtheworldstate",
      "forourproverbial",
      "Wefoundthatforshortclipsittook",
      "ongoing",
      "scale",
      "surprising",
      "unsafe",
      "earliest",
      "apparent",
      "vibrant",
      "Lifespan",
      "erroneous",
      "isbackedupbyarichtheoreticalframework[SSBD14,MRT18",
      "Deterministic",
      "Non",
      "providedonlyonarandom10%ofepisodes",
      "subsequent",
      "fictional",
      "modal",
      "postmodern",
      "syllogistic",
      "Additional",
      "glorious",
      "electrical",
      "dynamic",
      "broader",
      "previousstate",
      "sufficient",
      "internal",
      "longer",
      "Objective",
      "transformational",
      "deeper",
      "redemptive",
      "brittle",
      "constant",
      "mathe-",
      "Thenwewilldemonstratethatwecanfitthemodelsizeandtrainingtime",
      "attentionandtheparameter",
      "Unpredictable",
      "algorithmic",
      "crucial",
      "18th",
      "principal",
      "Thefundamental",
      "focal",
      "personal",
      "covered",
      "corresponding",
      "local",
      "reciprocal",
      "notable",
      "theistic",
      "cheap",
      "aware",
      "enamored",
      "political",
      "token",
      "certain",
      "phenomenological",
      "dire",
      "Israeli",
      "difficult",
      "multi",
      "worse",
      "second",
      "formulate",
      "acting",
      "unaffected",
      "model",
      "unwary",
      "stationary",
      "sothatthetwocanbesummed",
      "forward",
      "Head",
      "possible",
      "overall",
      "presuppose",
      "competitive",
      "concrete",
      "Greek",
      "sacred",
      "Ninth",
      "thenumberofcontrastivesamplesnecessarytomakean",
      "inter",
      "broad",
      "legged",
      "Frankfurtian",
      "decadent",
      "etymological",
      "Motivated",
      "scientific",
      "robotic",
      "optical",
      "Eastern",
      "deceptive",
      "left",
      "normal",
      "impossible",
      "emergent",
      "Wedid",
      "pertinent",
      "huge",
      "adaptive",
      "humananimalsseemabletolearnenormousamountsofbackgroundknowl-",
      "educated",
      "traditional",
      "assimilable",
      "synthesized",
      "hierachical",
      "British",
      "adversarial",
      "blurry",
      "ekstatic",
      "faced",
      "intra",
      "grand",
      "white",
      "economical",
      "intractable",
      "antagonistic",
      "accessible",
      "informational",
      "diff",
      "regular",
      "istic",
      "butwebelievetheaboveapproacheshavethepotential",
      "unpleasant",
      "Arab",
      "upper",
      "bad",
      "extensive",
      "B.3",
      "intrinsic",
      "theorizing",
      "receding-",
      "social",
      "likevisualexplanationsby",
      "parallel",
      "inertia",
      "sized",
      "integrated",
      "over-",
      "refined",
      "immutable",
      "eralized",
      "behavioral",
      "flat",
      "probatory",
      "auto-",
      "Inthelongrunitwouldbedesirabletomakelearningataskfromhumanpreferencesnomoredifficult",
      "sufficestolearnmostoftheoriginalRLtasksevenwhentherewardfunctionisnotobservable",
      "scalar",
      "future",
      "lowest",
      "despicable",
      "obvious",
      "32nd",
      "task",
      "transformed",
      "approachislesssimilar",
      "whichmeansthatto",
      "monetary",
      "fundamentalist",
      "Thisisfollowedbyafullyconnectedlayerofsize64andthenascalar",
      "conducive",
      "greedy",
      "modelstate",
      "light",
      "butthischoiceallowedustousethesameparametersasforthereal",
      "whichcanbefittothe",
      "Buddhist",
      "typical",
      "simulate",
      "22nd",
      "Typicallyitwillbeaveragedoverthetokensinacontext",
      "overcomplete",
      "sure",
      "nostalgic",
      "Onepossibleexperimentistomakeatoyenvironmentwithsomesimple",
      "ill",
      "total",
      "pink",
      "contrastive",
      "Model",
      "takingactionsthatdon’t",
      "therobotshouldmovetotherightasfastaspossible",
      "autonomous",
      "dandy",
      "Dataset",
      "upright",
      "Right",
      "constructive",
      "sequencetosequencelearning",
      "weremovedallvariable",
      "genetic",
      "physical",
      "probative",
      "hierar-",
      "adequate",
      "mean",
      "Heideggerian",
      "constitutive",
      "demon-",
      "Apreliminary",
      "messy",
      "Muslim",
      "incapable",
      "equivalent",
      "denotative",
      "Human",
      "ministic",
      "robust",
      "cultiv",
      "andthenselectthosetrajectoriesforwhichthepredictionshavethehighest",
      "encouraging",
      "prominent",
      "whenthelossmaybeapproximatelyquadratic",
      "adjustednumberofoptimizationstepsalsogrowsveryslowly",
      "movetotherightasfastaspossible",
      "Transformational",
      "connotative",
      "itisnecessarytoeliminateirrelevantinformationaboutthevariable",
      "bounded",
      "automatic",
      "mysterious",
      "24th",
      "outside",
      "unnormalized",
      "sympathetic",
      "remarkable",
      "American",
      "speakable",
      "controversial",
      "unrecognizable",
      "curious",
      "volumous",
      "resounding",
      "anti",
      "Thisdifficultyunderliesrecentconcernsabout",
      "optic",
      "Neural",
      "intersecting",
      "headself",
      "proto",
      "synthetic",
      "sample",
      "20th",
      "risky",
      "crowd-",
      "clear",
      "unsupervised",
      "unparalleled",
      "alright",
      "able",
      "temporary",
      "gradient",
      "unobserved",
      "iconic",
      "Early",
      "spiritual",
      "basedmodel(EBM",
      "JacobSteinhardtandRussTedrake",
      "clean",
      "thoughsomewhatlarger",
      "geriatric",
      "dynamical",
      "havebeenastonishinglysuccessful",
      "safetythatcontinuestoremainrelevantasautonomoussystemsbecomemorepowerful",
      "theresultingcomparisonsaresignificantlymore",
      "tasksofteninvolvebringingtheexternalworldintosomeobjectivestate",
      "Exploringvariantsofempowermentpenalizationthatmorepreciselycapturethe",
      "lyrical",
      "composite",
      "Architectural",
      "perennial",
      "representative",
      "adversarialneuralnetworks”",
      "convexity",
      "abilitytolearnsuchdependenciesisthelengthofthepathsforwardandbackwardsignalshaveto",
      "Young",
      "Warm",
      "congratulatory",
      "largest",
      "intimate",
      "whichmaybeoptimized",
      "wholesale",
      "respond",
      "organized",
      "scalable",
      "Relevant",
      "pragmatic",
      "unifying",
      "mild",
      "mo-",
      "Detailed",
      "Attentionmechanismshavebecomeanintegralpartofcompellingsequencemodelingandtransduc-",
      "material",
      "emblematic",
      "silent",
      "resonant",
      "ChatGPT",
      "Husserlian",
      "Current",
      "Dual",
      "psychic",
      "recognizable",
      "dismissive",
      "evolutionary",
      "grateful",
      "blown",
      "linguistic",
      "graphic",
      "atasksometimescalled",
      "undergraduate",
      "critical",
      "aporetic",
      "hierarchical",
      "knowl-",
      "whichwillneedtobea",
      "Supplementary",
      "numerous",
      "hybrid",
      "packedtogether",
      "philosophical",
      "sunny",
      "logical",
      "Stochastic",
      "sparse",
      "independent",
      "open",
      "allsub",
      "inexhaust",
      "empiricist",
      "contour",
      "specifiedmodelfamilyisprobablyinfeasible",
      "behaved",
      "blue",
      "calm",
      "supervised",
      "public",
      "virtuous",
      "Usingexplicitdiscountingorinferringthehuman’sdiscount",
      "noted",
      "Contrary",
      "identifiable",
      "organic",
      "constrative",
      "assigningthemselveshigh",
      "asymptotic",
      "tectural",
      "inherent",
      "aristocratic",
      "codified",
      "English",
      "unnecessary",
      "variable",
      "exhaustive",
      "construal",
      "wired",
      "pejorative",
      "catastrophic",
      "illustrative",
      "metacognitive",
      "proleptic",
      "pervasive",
      "interpenetrate",
      "siht",
      "pregnant",
      "tiple",
      "dripping",
      "eziS",
      "nebulous",
      "Continental",
      "discrete",
      "wecanfindf(7)bylookingforthevalueofxsuch",
      "Certain",
      "factual",
      "useintheestimationoflatentvariablemodels[9].Whilethecurrentfocusisonusingthemethodof",
      "infor-",
      "sinusoidalpositionalencodingwithlearnedpositionalembeddings[9],andobservenearlyidentical",
      "sophisticated",
      "funny",
      "unavoidable",
      "exemplified",
      "complimentary",
      "internalized",
      "fast",
      "encodeinformationaboutthetaskevenwhentherewardfunctionisnotobservable",
      "Thehumanthenindicateswhichsegmenttheyprefer",
      "stickier",
      "biological",
      "makingitdifficulttohack",
      "ergodic",
      "Left",
      "frameable",
      "unformalized",
      "adrift",
      "likely",
      "Wide",
      "abstract",
      "Intrinsic",
      "unforeseen",
      "troubling",
      "experienced",
      "worthwhile",
      "fuzzy",
      "fifth",
      "situational",
      "neutral",
      "verbal",
      "highest",
      "Earlierwork[ZK16]hascomparedwidth",
      "whichis",
      "Titanic",
      "thanlearningitfromaprogrammaticrewardsignal",
      "logically",
      "rangingfromfifteenminutestofive",
      "mathematical",
      "near",
      "Platonic",
      "Generative",
      "Anypowerfulagentwillbeacomplicatedsystemwiththeobjective",
      "aloof",
      "multiple",
      "preliminary",
      "wouldbeamoremeaningfulperformancemetric",
      "psychotic",
      "wrong",
      "tethered",
      "greatest",
      "foreign",
      "trans",
      "threefold",
      "gentle",
      "Generalized",
      "Thescalingrelationswithmodelsizeandcomputeareespeciallymysterious",
      "Muchoftheliteratureinreinforcementlearningisfocusedondealingwiththestochastic",
      "I've",
      "expressive",
      "subjective",
      "Stoic",
      "internalizable",
      "uous",
      "ticular",
      "disastrous",
      "affective",
      "Multi",
      "main",
      "training",
      "motivational",
      "historical",
      "Active",
      "oratleastsomesimilar",
      "harmful",
      "ready",
      "human",
      "allowingforpotentiallyfastertrainingifsufficientadditionalparal-",
      "absent",
      "Higher",
      "penetrating",
      "performative",
      "permanent",
      "theologian",
      "false",
      "intriguing",
      "perspectival",
      "En",
      "contexts",
      "autoregressive",
      "asrequiredbytheparametercountsinTable",
      "Careful",
      "short",
      "structural",
      "prolific",
      "old",
      "bayesian",
      "At-",
      "qualitative",
      "super-",
      "longer-",
      "Latin",
      "Newtonian",
      "Observed",
      "temporal",
      "ai",
      "technical",
      "pivotal",
      "conscious",
      "polynomial",
      "like",
      "Thepredictormayusealatent",
      "smooth",
      "wet",
      "unrelated",
      "itisstraightforwardtoextendtheconcepttomultiple",
      "Neoplatonic",
      "online",
      "ls",
      "Toutilizebothtrainingtimeandcomputeaseffectivelyaspossible",
      "≡",
      "Cooperative",
      "whereweuseanentropy",
      "mock",
      "theoretical",
      "methodologiesthatconsistentlyallowonetocollectarepresentativesetoftrainingsetsandfromthis",
      "explanatory",
      "artistic",
      "humanselectsonesegmentaspreferable",
      "good",
      "Smooth",
      "fragile",
      "Primitive",
      "wider",
      "dead",
      "isolated",
      "Good",
      "verbs?—of",
      "closed",
      "transgressive",
      "ego",
      "brief",
      "explicit",
      "describable",
      "simpler",
      "Hard",
      "coordinated",
      "maximum",
      "democratic",
      "latent",
      "extra",
      "Ourmainmotivationforelicitingcomparisonsratherthanabsolutescoreswasthatwefounditmuch",
      "Prompt",
      "Aspirational",
      "cial",
      "infrequent",
      "deliberate",
      "Toimplementthis",
      "spatial",
      "baseline",
      "p∗",
      "B.1",
      "inexact",
      "independentofthemodelsize",
      "Red",
      "northeast",
      "theist",
      "Illustrative",
      "ultimate",
      "proportional",
      "medical",
      "Powerful",
      "idiosyncratic",
      "intentional",
      "Forgetfulness",
      "specific",
      "Withasingleattentionhead",
      "Independent",
      "wise",
      "alliterative",
      "technological",
      "sentationofimagessuchthattherepresentationofascenefromoneviewpointispredictable",
      "MITpress",
      "bothattentionandresidual",
      "correlational",
      "contrite",
      "terrific",
      "Hebrew",
      "tionallayerscommonlyusedformappingonevariable",
      "childish",
      "determin-",
      "weakly",
      "liminal",
      "epistemic",
      "fold",
      "penetrative",
      "tremendous",
      "federal",
      "smallest",
      "selective",
      "uncertain",
      "Inthispaper",
      "ineffective",
      "stray",
      "Related",
      "multifaceted",
      "cleaner",
      "embed",
      "for-",
      "unanswered",
      "solvable",
      "op-",
      "dotted",
      "feasible",
      "Multiple",
      "meth-",
      "wewouldwanttoquery",
      "hostile",
      "cutting",
      "receptive",
      "separable",
      "accurate",
      "greater",
      "pure",
      "examplesofcontrastivetrainingmethodsinwhichtheyˆisgeneratedbycorruptingtheclean",
      "distant",
      "dominant",
      "romantic",
      "autobiographical",
      "elementary",
      "useful",
      "equivariant",
      "newer",
      "Short",
      "Visual",
      "clever",
      "null",
      "developmental",
      "famous",
      "untestable",
      "tacit",
      "aleatoric",
      "theworldmodelisinaccurateduetolimitationsofitsrepresentationalpower(bounded",
      "Critical",
      "wealso",
      "Specific",
      "onal",
      "whichwouldhaveamajorimpact",
      "Abstract",
      "perfect",
      "andtohavemethodologiesforadequatelystress",
      "misconstrual",
      "conformal",
      "regularized",
      "inside",
      "pedagogical",
      "inefficient",
      "practical",
      "Overall",
      "Psychological",
      "seamless",
      "external",
      "tempting",
      "influential",
      "5th",
      "Hollow",
      "underdeveloped",
      "cognitive",
      "super",
      "variational",
      "tokenized",
      "longstanding",
      "poetical",
      "compatible",
      "changeable",
      "deterministic",
      "diverse",
      "global",
      "attractive",
      "InProceedingsofthe2009ConferenceonEmpiricalMethodsinNatural",
      "asincomparable",
      "academic",
      "secondary",
      "observable",
      "artificial",
      "logistic",
      "marginal",
      "Double",
      "autocorrelated",
      "pathological",
      "Interpretive",
      "inner",
      "affording",
      "psychotech",
      "long-",
      "opponent",
      "Convolutional",
      "essentiallyshapingtherewardandthusoutperformingA3Cinthisgame(theresultsarecomparable",
      "andthetrendextrapolateswellinbothdirectionsregardless",
      "exemplary",
      "inadequate",
      "incomplete",
      "moral",
      "discriminative",
      "unintended",
      "irrational",
      "Larger",
      "compute",
      "insidious",
      "middle",
      "proper",
      "theological",
      "autonomousvehicles[86],andGo[140].Theseadvanceshavebroughtexcitementaboutthepositive",
      "valid",
      "excessive",
      "unthinkable",
      "trivial",
      "fluent",
      "disam-",
      "onerous",
      "comprehen",
      "optimal",
      "grounded",
      "Self-",
      "literal",
      "animate",
      "expected",
      "steerable",
      "weoptimizepoliciesusingtrustregionpolicyoptimization(TRPO",
      "finitary",
      "reliable",
      "modality",
      "meaningful",
      "OntheWMT2014English",
      "Catholic",
      "heuristic",
      "writable",
      "worth",
      "creative",
      "weuselearnedembeddingstoconverttheinput",
      "Sparsecodingwithmulti",
      "basic",
      "unclear",
      "minimal",
      "responsible",
      "Sincethefitsarebestlateintraining",
      "theoretic",
      "gnostic",
      "authentic",
      "acceptable",
      "sarial",
      "interpretable",
      "33nd",
      "sophrosyne",
      "offline",
      "Potential",
      "standard",
      "blank",
      "Positive",
      "intermediate",
      "unlimitedness",
      "primal",
      "wiser",
      "interactive",
      "informal",
      "Literary",
      "weshowthatcollectingfeed-",
      "Therewardfunctionsinthesetasksarelinear",
      "unfamiliar",
      "superlative",
      "Liberal",
      "WearegratefultoNalKalchbrennerandStephanGouwsfortheirfruitful",
      "eral",
      "in-",
      "paramount",
      "Wediscuss",
      "fair",
      "natural",
      "legal",
      "Melancholy",
      "small",
      "safe",
      "Static",
      "vocabulary",
      "Mode-2",
      "unsupervisedriskestimationmaystillbepossible",
      "additional",
      "Hierarchical",
      "tested",
      "7,weexplorefive",
      "air-",
      "exclusive",
      "familiar",
      "loose",
      "understandable",
      "Scalable",
      "neural",
      "serial",
      "testable",
      "directed",
      "attuned",
      "suboptimal",
      "popular",
      "pernicious",
      "ofapredictedvariableintoalatentvariable",
      "tokencontext",
      "min",
      "Central",
      "random",
      "arXiv:1801.10198[cs",
      "explosive",
      "trainingdata",
      "agent",
      "participatory",
      "fit",
      "harsh",
      "informative",
      "passive",
      "evah",
      "important",
      "great",
      "efficient",
      "novel",
      "ashasbeenseenin",
      "siamese",
      "intellectual",
      "fourth",
      "void",
      "Distant",
      "whichearly",
      "varied",
      "criminal",
      "sexist",
      "C.5",
      "primitive",
      "convolutional",
      "dual",
      "nuanced",
      "Portuguese",
      "functional",
      "conversational",
      "Inthese",
      "causal",
      "therapeutic",
      "toprevent",
      "whichcanbevulnerabletoadversarial",
      "auditory",
      "residual",
      "leaper",
      "oriented",
      "single",
      "inconsistent",
      "subject",
      "ultimatepurposeofhumaninteractionistosolvetasksforwhichnorewardfunctionisavailable",
      "interdependent",
      "key",
      "feudal",
      "Preliminary",
      "confusinganddifficulttoevaluate",
      "func-",
      "Different",
      "liberal",
      "Subject",
      "ratherthantosupplyanabsolutenumerical",
      "viable",
      "reflective",
      "reactive",
      "Fixed",
      "little",
      "namely2B",
      "virtual",
      "questionable",
      "supreme",
      "computational",
      "simultaneous",
      "consonant",
      "missing",
      "y))/P",
      "heroic",
      "ample",
      "new",
      "quintessential",
      "vague",
      "dataset",
      "lacking",
      "discontinuous",
      "weird",
      "unhappy",
      "Bayesian",
      "digital",
      "monolithic",
      "demonstrative",
      "vital",
      "estimated",
      "consistent",
      "European",
      "particular",
      "resituate",
      "entire",
      "humanistic",
      "correct",
      "instinctive",
      "liable",
      "bridesmaid",
      "interpretive",
      "cultural",
      "inanimate",
      "13th",
      "salvational",
      "perceptual",
      "harmless",
      "+",
      "primary",
      "uncountable",
      "higher",
      "close",
      "Modal",
      "angelic",
      "Kantians",
      "chaotic",
      "covariate",
      "generalizable",
      "Learningphraserepresentationsusingrnnencoder",
      "sur-",
      "ac-",
      "entropy",
      "Forasimpleexample",
      "graphical",
      "modern",
      "Siamese",
      "bidirectional",
      "unspecified",
      "interpersonal",
      "hidden",
      "contradictory",
      "instructive",
      "propositional",
      "sinuous",
      "dedicated",
      "attentive",
      "reminiscent",
      "willful",
      "ambiguous",
      "exam-",
      "itsscaleisarbitrary",
      "Interruptible",
      "promissory",
      "Wecollect25%ofourcomparisonsfromarandomlyinitializedpolicynetworkatthebeginningof",
      "quadratic",
      "observa-",
      "Behavioral",
      "toinvestigatelocal",
      "Hellenistic",
      "Godelian",
      "insightful",
      "encode",
      "incommensurable",
      "Christian",
      "low",
      "unlabelled",
      "synoptic",
      "relative",
      "semiotic",
      "earlier",
      "unlabeled",
      "reconstructive",
      "naturalistic",
      "offensive",
      "axial",
      "strong",
      "rapid",
      "early",
      "Implicit",
      "valuable",
      "bloody",
      "square",
      "systematic",
      "amenable",
      "Low",
      "imaginary",
      "promising",
      "InINTERSPEECH",
      "Asingleconvolutionallayerwithkernelwidthk",
      "21st",
      "numerical",
      "Fast",
      "focused",
      "headed",
      "platonic",
      "extrapersonal",
      "mystical",
      "Jewish",
      "levelcontrolthroughdeepreinforcementlearning”",
      "noitcudorP",
      "cluttered",
      "steady",
      "psychological",
      "distinct",
      "integral",
      "Closed",
      "whichdoesn’t",
      "Smaller",
      "antithetical",
      "common",
      "fewer",
      "Gibbs",
      "quantitative",
      "classical",
      "Roman",
      "nascent",
      "approximate",
      "limited-",
      "powerful",
      "incremental",
      "divine",
      "Large",
      "opposite",
      "precise",
      "Maximum",
      "committed",
      "upward",
      "prefrontal",
      "tational",
      "Gpukernelsforblock",
      "superior",
      "mindfulness",
      "Stupid",
      "prior",
      "zoom",
      "higher-",
      "equal",
      "unbounded",
      "andhasthepotentialforharmfulimpactsinreal",
      "LSTM",
      "self-",
      "itappearsthatthelossscalesasapower",
      "indebted",
      "Propositional",
      "offbetweenthecompleteness",
      "approachesto",
      "represent",
      "macro",
      "large",
      "TheoptimalmodelsizeisshowninFigure14",
      "educational",
      "predictive",
      "literalist",
      "unfair",
      "ofeachtaskbeforebeingaskedtocompareseveralhundredtoseveralthousandpairsoftrajectory",
      "rewarding",
      "nice",
      "supervisedreinforcementlearning",
      "excellent",
      "14th",
      "older",
      "untethered",
      "morehelpfulperclip",
      "wheretheweightassignedtoeachvalueiscomputedbyacompatibilityfunctionofthe",
      "usingthelargerhigh",
      "dimensionalmanifold",
      "arXiv:1506.02142",
      "onagreat",
      "Canadian",
      "entropyloss)averagedovera1024",
      "limited",
      "beautiful",
      "unexplored",
      "Integrative",
      "worldview",
      "Ugly",
      "supervisory",
      "appropriate",
      "rep-",
      "insufficient",
      "whichcouldleadtoadditional",
      "prophetic",
      "worthy",
      "integer",
      "big",
      "multiplicative",
      "interpre-",
      "Theworldmodel",
      "binary",
      "http://arxiv.org/",
      "interact",
      "principled",
      "pantheistic",
      "Indian",
      "Hinge",
      "caring",
      "massively",
      "whichisalsoourprincipal",
      "disjunctive",
      "underdeterminedbyourlearningproblem",
      "Artificial",
      "musical",
      "aspirational",
      "challenging",
      "Thecriticalbatchsizecan",
      "Kantian",
      "promotional",
      "openai-",
      "tractable",
      "Variable",
      "Numerous",
      "conservative",
      "Free",
      "con-",
      "iftheleftclip",
      "asproposedinthepresentpaper",
      "Open",
      "contentious",
      "Ifwecannotincreasethemodel",
      "bigger",
      "developed",
      "embodiment",
      "recent",
      "Crazy",
      "heteronomous",
      "stronger",
      "circular",
      "indestructible",
      "sophrosynic",
      "advance",
      "Trainable",
      "ofthese",
      "tectonic",
      "totalitarian",
      "Second",
      "heavy",
      "metaphysical",
      "elegant",
      "final",
      "convincing",
      "paradoxical",
      "transferrable",
      "attunement",
      "naive",
      "Gaussian",
      "supervisedsetting",
      "level",
      "inactive",
      "embed-",
      "disturbing",
      "piecemeal",
      "recursive",
      "emptiness",
      "counteractive",
      "brave",
      "slippery",
      "careful",
      "reg-",
      "cheaper",
      "smart",
      "Universal",
      "forcement",
      "clinical",
      "retinotopic",
      "empirical",
      "odd",
      "seminal",
      "Spectral",
      "mimic",
      "Figure4",
      "locked",
      "individualistic",
      "Thethirdisthepathlengthbetweenlong",
      "hydraulic",
      "com-",
      "objective",
      "sinceforanyfixedoffsetk",
      "Malleable",
      "shorter",
      "Demonic",
      "Danish",
      "Dutch",
      "experimentive",
      "painful",
      "supervisedlearning”",
      "measured",
      "Thebest",
      "iterative",
      "metric",
      "attentionandpoint",
      "merged",
      "exciting",
      "slow",
      "Assidebenefit",
      "Intuitive",
      "Thiscouldinvolveeitherlearningarewardfunctionfromsamplesofstate",
      "ineffable",
      "descriptive",
      "VICReg",
      "destructive",
      "Descriptive",
      "East",
      "ularizer",
      "bluer",
      "unexpected",
      "padded",
      "mythos",
      "sensual",
      "Multimodal",
      "definitional",
      "wemultiplythoseweightsby",
      "oldest",
      "3rd",
      "irrecoverable",
      "LLM",
      "rangecorrelationslaterintraining",
      "inital",
      "formal",
      "thattherewardfunctionrˆmaybenon",
      "normative",
      "summarizing",
      "Deep",
      "significant",
      "di-",
      "Core",
      "minimum",
      "urable",
      "Negative",
      "whichcancauselabelstobe",
      "disappointed",
      "convolu-",
      "poor",
      "ideal",
      "Available",
      "Throughoutwewillobserveprecisepower",
      "national",
      "usual",
      "inexhaustible",
      "related",
      "head",
      "prun-",
      "symbolic",
      "imagina-",
      "latest",
      "conventional",
      "golden",
      "aforementioned",
      "societal",
      "open-",
      "Aspectratioinparticularcan",
      "Trainingatthecriticalbatchsizemakesaroughlyoptimal",
      "high",
      "compact",
      "regressive",
      "distal",
      "andthusrunsthe",
      "transferable",
      "sequential",
      "detailed",
      "Thesebehaviorsweretrainedusingfeedback",
      "controllable",
      "official",
      "Thefirstisamulti",
      "epsilon-",
      "whilesmallermodelscan",
      "rich",
      "transformative",
      "larger",
      "Learninglong",
      "vast",
      "semi",
      "core",
      "sub-",
      "fallacious",
      "transtemporal",
      "argumentative",
      "segmentisbetween1and2secondslong",
      "neigh-",
      "parasitic",
      "deter-",
      "additive",
      "unterstand",
      "previous",
      "Islamic",
      "confident",
      "Wewillfitthestable",
      "aletheia",
      "levelobjective",
      "Sophisticated",
      "inferential",
      "Fallacious",
      "forthcoming",
      "Token",
      "quick",
      "backgrounded",
      "unformulated",
      "agentive",
      "following",
      "allquantitativeresultsshouldbeviewedasestimatesofthemodel’spotential",
      "unfolded",
      "metaheuristic",
      "kind",
      "annoyed",
      "configurable",
      "prime",
      "alignedrecurrenceandhavebeenshowntoperformwellonsimple",
      "visible",
      "emotional",
      "analogous",
      "47th",
      "advanced",
      "static",
      "plex",
      "fuller",
      "sensitive",
      "immature",
      "RichardSZemeletal",
      "positive",
      "happy",
      "Unobserved",
      "thereward",
      "untouched",
      "implementa-",
      "concerned",
      "suc-",
      "weighted",
      "Pre",
      "necessary",
      "genuine",
      "smaller",
      "rough",
      "identical",
      "disciplinary",
      "agape",
      "effective",
      "contemporary",
      "irreducible",
      "confused",
      "pixel",
      "pre-",
      "tautological",
      "enormous",
      "Rethinkingmodelscalingforconvolutionalneural",
      "transient",
      "teleological",
      "finiteness",
      "available",
      "electric",
      "Forcontinuouscontroltaskswefound",
      "LSTMs",
      "massive",
      "philosopher",
      "manyappeartoexhibitbehaviorrelatedtothesyntactic",
      "whichmeansthattheseempiricalresultsarenottrulyoptimal",
      "wereplacethescoreareawithaconstantblackbackgroundonallseven",
      "Western",
      "post",
      "associational",
      "pervised",
      "rounded",
      "wehavetoassumethatithaspotentiallyseeneveryexistingbenchmark",
      "salvic",
      "Ourscalinglawsprovideapredictive",
      "Jungian",
      "wide",
      "thelearningcurvescanbeaccuratelyfitby(seetherightoffigure4",
      "Soft",
      "fixed",
      "likeAtarigameswherethousandsofhoursofexperiencearerequiredtolearnahigh",
      "mystic",
      "stochastic",
      "incidental",
      "pros-",
      "weaker",
      "offending",
      "ignorant",
      "Environmental",
      "discretized",
      "ldr",
      "brutal",
      "undue",
      "crazy",
      "computable",
      "concise",
      "existential",
      "andthatfruitful",
      "easier",
      "central",
      "Recent",
      "smoothed",
      "narrative",
      "exploratory",
      "Wetrainonlyonepredictorratherthananensemble(noensemble",
      "Vish-",
      "green",
      "decoderarchitectureswith",
      "sensible",
      "incorrect",
      "Theconspicuous",
      "Electronic",
      "Unsu-",
      "befitwithapower",
      "set",
      "synergistic",
      "erate",
      "macroscopic",
      "contin-",
      "ourmodel",
      "finite",
      "ambitious",
      "arithmetic",
      "inert",
      "adverbial",
      "cool",
      "Persian",
      "levelpredictionstobeaccurate",
      "elusive",
      "wesamplealargenumberofpairsoftrajectory",
      "stupid",
      "net",
      "universal",
      "competent",
      "imperfect",
      "frompreferencesratherthanabsoluterewardvalues(Fürnkranzetal",
      "direct",
      "okay",
      "cautious",
      "satisfied",
      "infamous",
      "Wetraineda4",
      "twentieth",
      "recurrent",
      "joint",
      "Recurrent",
      "dependent",
      "knowable",
      "shallow",
      "lily",
      "highly",
      "nonreligious",
      "extended",
      "closest",
      "generative",
      "ambivalent",
      "malleable",
      "Swiss",
      "Parthian",
      "foolish",
      "probabilistic",
      "albeitatthecostofreducedeffectiveresolutiondue",
      "unimportant",
      "Scarcity",
      "capable",
      "Speculative",
      "Match(k",
      "everyday",
      "Simple",
      "butratherencompassesabroadrangeof",
      "triple",
      "intuitive",
      "gold",
      "dimensional",
      "multimodal",
      "αmin",
      "parameterized",
      "past",
      "expert",
      "smartest",
      "Empirical",
      "ourmodelestablishesanewsingle",
      "transitive",
      "Hungarian",
      "Counterfactual",
      "arbitrary",
      "applicable",
      "associative",
      "L)≈",
      "sometimescalledintra",
      "different",
      "inappropriate",
      "inter-",
      "beneficial",
      "OurapproachtostudyGPT-4isdifferentfromthese",
      "orfrom",
      "separate",
      "semantic",
      "inaccurate",
      "speculative",
      "susceptible",
      "atotal",
      "functionofcomputeC.",
      "Theestimaterˆisdefinedbyindependentlynormalizingeachofthesepredictorsand",
      "Protestant",
      "superstitious",
      "suitable",
      "demonic",
      "Wereplacedthese",
      "incompatible",
      "dark",
      "undesired",
      "simplistic",
      "Transformer",
      "devoted",
      "inductive",
      "articulated",
      "ugly",
      "formative"
    ]
  }
}